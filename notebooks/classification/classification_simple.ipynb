{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import layers\n",
    "\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"../../data/data_with_features/data_train_with_features.csv\").drop([\"Unnamed: 0\"], axis=1)\n",
    "data_test = pd.read_csv(\"../../data/data_with_features/data_test_with_features.csv\").drop([\"Unnamed: 0\"], axis=1)\n",
    "\n",
    "data_train = data_train.sample(frac=1).reset_index(drop=True)\n",
    "data_test = data_test.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Scientific       1.00      0.98      0.99        83\n",
      "        news       0.92      1.00      0.96        93\n",
      "     reviews       0.96      0.97      0.97       118\n",
      "       story       1.00      0.92      0.96       106\n",
      "\n",
      "    accuracy                           0.97       400\n",
      "   macro avg       0.97      0.97      0.97       400\n",
      "weighted avg       0.97      0.97      0.97       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "features = vectorizer.fit_transform(data_train[\"text\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, data_train[\"classification\"], test_size=0.2, random_state=42)\n",
    "\n",
    "model = svm.SVC(probability=True)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.   Duis autem vel eum iriure dolor in hendrerit in vulputate velit esse molestie consequat, vel illum dolore eu feugiat nulla facilisis at vero eros et accumsan et iusto odio dignissim qui blandit praesent luptatum zzril delenit augue duis dolore te feugait nulla facilisi. Lorem ipsum dolor sit amet.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Klasse 0: ['0.00%', '0.00%', '0.00%', '100.00%']\n",
      "Vorhergesagte Klasse: ['story']\n"
     ]
    }
   ],
   "source": [
    "new_text = data_test[\"text\"][432]\n",
    "#new_text = t\n",
    "new_text_features = vectorizer.transform([new_text])\n",
    "probabilities = model.predict_proba(new_text_features)\n",
    "predicted_class = model.predict(new_text_features)\n",
    "\n",
    "# Wahrscheinlichkeiten und vorhergesagte Klasse ausgeben\n",
    "for i, probs in enumerate(probabilities):\n",
    "    class_probabilities = [\"{:.2f}%\".format(prob * 100) for prob in probs]\n",
    "    print(\"Klasse {}: {}\".format(i, class_probabilities))\n",
    "print(\"Vorhergesagte Klasse:\", predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Scientific'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test[\"classification\"][432]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"../../data/data_with_features/data_train_with_features.csv\").drop([\"Unnamed: 0\"], axis=1)\n",
    "data_test = pd.read_csv(\"../../data/data_with_features/data_test_with_features.csv\").drop([\"Unnamed: 0\"], axis=1)\n",
    "\n",
    "data_train = data_train.sample(frac=1).reset_index(drop=True)\n",
    "data_test = data_test.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "features = vectorizer.fit_transform(data_train[\"text\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, data_train[\"classification\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mSequential([\n\u001b[0;32m      2\u001b[0m     layers\u001b[39m.\u001b[39mDense(\u001b[39m64\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m, input_shape\u001b[39m=\u001b[39m(features\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m],)),\n\u001b[0;32m      3\u001b[0m     layers\u001b[39m.\u001b[39mDense(\u001b[39m64\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[0;32m      4\u001b[0m     layers\u001b[39m.\u001b[39mDense(\u001b[39m4\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m ])\n\u001b[0;32m      7\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m               loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msparse_categorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      9\u001b[0m               metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(features.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m label_to_int \u001b[39m=\u001b[39m {label: i \u001b[39mfor\u001b[39;00m i, label \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(np\u001b[39m.\u001b[39munique(data_train[\u001b[39m\"\u001b[39m\u001b[39mclassification\u001b[39m\u001b[39m\"\u001b[39m]))}\n\u001b[1;32m----> 2\u001b[0m y_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([label_to_int[label] \u001b[39mfor\u001b[39;00m label \u001b[39min\u001b[39;00m y_train])\n\u001b[0;32m      3\u001b[0m y_test \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([label_to_int[label] \u001b[39mfor\u001b[39;00m label \u001b[39min\u001b[39;00m y_test])\n",
      "Cell \u001b[1;32mIn[43], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m label_to_int \u001b[39m=\u001b[39m {label: i \u001b[39mfor\u001b[39;00m i, label \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(np\u001b[39m.\u001b[39munique(data_train[\u001b[39m\"\u001b[39m\u001b[39mclassification\u001b[39m\u001b[39m\"\u001b[39m]))}\n\u001b[1;32m----> 2\u001b[0m y_train \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([label_to_int[label] \u001b[39mfor\u001b[39;00m label \u001b[39min\u001b[39;00m y_train])\n\u001b[0;32m      3\u001b[0m y_test \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([label_to_int[label] \u001b[39mfor\u001b[39;00m label \u001b[39min\u001b[39;00m y_test])\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "label_to_int = {label: i for i, label in enumerate(np.unique(data_train[\"classification\"]))}\n",
    "y_train = np.array([label_to_int[label] for label in y_train])\n",
    "y_test = np.array([label_to_int[label] for label in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.7543 - accuracy: 0.9394\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 4s 43ms/step - loss: 0.0452 - accuracy: 0.9987\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 4s 44ms/step - loss: 0.0060 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 4s 45ms/step - loss: 8.0117e-04 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 4s 44ms/step - loss: 6.0352e-04 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 4s 44ms/step - loss: 4.7017e-04 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 4s 44ms/step - loss: 3.7657e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ea304f3670>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train.toarray(), y_train, epochs=10, batch_size=16, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 1s 30ms/step - loss: 0.0207 - accuracy: 0.9950\n",
      "Test Loss: 0.020698027685284615\n",
      "Test Accuracy: 0.9950000047683716\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test.toarray(), y_test, verbose=1)\n",
    "print('Test Loss:', loss)\n",
    "print('Test Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"\"\"I've been thinking e-everyday\n",
    "I've been thinking 'bout what you say\n",
    "But words just get in the way, yeah\n",
    "And I stress 'cause I don't wanna make a mess\n",
    "When it comes to you\n",
    "I'll give my best, yeah yeah\n",
    "I'm trying to impress\n",
    "\n",
    "Oh oh oh oh oh and everyday\n",
    "Is like I see you for the first time\n",
    "Oh oh oh oh oh and over and over I try\n",
    "But words won't come my way\n",
    "\n",
    "Baby no oh oh oh oh\n",
    "This ain't just a love song\n",
    "Another love song\n",
    "Just random words\n",
    "On the same sad chords\n",
    "It's true, my song is all about you\n",
    "\n",
    "All my friends say I try too much\n",
    "They say it's just a little crush\n",
    "But you took over my heart\n",
    "And I stress 'cause I always\n",
    "Tend to make a mess\n",
    "Even though I try to give my best\n",
    "Yeah yeah, I'm trying to impress\n",
    "Yeah yeah! \n",
    "\n",
    "Oh oh oh oh oh and everyday\n",
    "Is like I see you for the first time\n",
    "Oh oh oh oh oh and over and over I try\n",
    "But words won't come my way\n",
    "\n",
    "Baby no oh oh oh oh\n",
    "This ain't just a love song\n",
    "Another love song\n",
    "Just random words\n",
    "On the same sad chords\n",
    "It's true, my song is all about\n",
    "Yo-o-o-ou\n",
    "Yo-o-o-ou\n",
    "Just random words\n",
    "On the same sad chords\n",
    "It's true, my song is all about\n",
    "\n",
    "You, the one that I can't escape\n",
    "The one that can take my breath\n",
    "The only one that keeps me coming back\n",
    "And 'cause my words fall short\n",
    "I'm singing you this song\n",
    "This song\n",
    "\n",
    "Baby no oh oh oh oh\n",
    "This ain't just a love song\n",
    "Another love song\n",
    "Just random words\n",
    "On the same sad chords\n",
    "It's true, this song is all about youuuu\n",
    "\n",
    "Baby no oh oh oh oh\n",
    "This ain't just a love song\n",
    "Another love song\n",
    "Just random words\n",
    "On the same sad chords\n",
    "It's true, my song is all about youuuu\n",
    "Yo-o-o-ou\n",
    "Yo-o-o-ou\n",
    "Just random words\n",
    "On the same sad chords\n",
    "It's true, this song is all about you\"\"\"\n",
    "\n",
    "lorem_ipsum = \"Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.   Duis autem vel eum iriure dolor in hendrerit in vulputate velit esse molestie consequat, vel illum dolore eu feugiat nulla facilisis at vero eros et accumsan et iusto odio dignissim qui blandit praesent luptatum zzril delenit augue duis dolore te feugait nulla facilisi. Lorem ipsum dolor sit amet.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step\n",
      "Vorhergesagte Klasse: reviews, Wahrscheinlichkeit: 0.9960267543792725\n"
     ]
    }
   ],
   "source": [
    "new_text = data_test[\"text\"][0]\n",
    "new_text = t\n",
    "new_text_features = vectorizer.transform([new_text])\n",
    "predictions = model.predict(new_text_features.toarray())\n",
    "predicted_class = np.argmax(predictions, axis=1)\n",
    "predicted_probability = np.max(predictions, axis=1)\n",
    "\n",
    "int_to_label = {i: label for label, i in label_to_int.items()}\n",
    "\n",
    "predicted_labels = [int_to_label[prediction] for prediction in predicted_class]\n",
    "for label, probability in zip(predicted_labels, predicted_probability):\n",
    "    print(f\"Vorhergesagte Klasse: {label}, Wahrscheinlichkeit: {probability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mit Allem: 99% Scientific\n",
    "# ohne Anhang & Vezeichnisse: 99% Scientific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Scientific': 0, 'news': 1, 'reviews': 2, 'story': 3}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"\"\"What are graph databases and how can quality\n",
    "be verified for their data?\n",
    "Abstract\n",
    "Loosing customers, missing opportunities and coming to wrong decisions are symptoms of\n",
    "the lack of accurate data in many enterprises. Every company can achieve big advantages\n",
    "by processing data correctly and efficiently, and in order to work with a huge amount of\n",
    "data, the data has to be trustworthy and of high quality. Boehringer Ingelheim is a global,\n",
    "family-owned researching pharmaceutical company that focuses on researching, developing,\n",
    "producing and selling prescription drugs for humans and animals. Pharmaceutical companies like Boehringer Ingelheim invest billions of euros and years of work into researching\n",
    "and developing; and sometimes without even finding a satisfying result. The development of\n",
    "a drug can cost 1.0 to 1.6 billion US-Dollars and can last for over 13 years; only for one age\n",
    "class. Because of that, data quality and trustworthy data in general play an important role for\n",
    "the company.\n",
    "This project paper simply explains how data quality can be verified using automatic generation of test cases from graph databases and how a framework could look like that ensures high\n",
    "quality data. It defines necessary vocabulary and explains required concepts, languages and\n",
    "functionalities like RDF, OWL, SHACL, SPARQL, the Semantic Web, graph databases (like\n",
    "knowledge graphs), the Astrea-Tool and the RDFUnit Testing Suite. The final result of this\n",
    "project paper is a software concept, that links the Astrea-Tool and RDFUnit Testing Suite to\n",
    "enable automatic generation of data shapes, as well as test cases for those data shapes. This\n",
    "final software concept only needs data stored in RDF triples and the corresponding ontologies to automatically create an inspection report, which clearly depicts errors or irregularities\n",
    "in any dataset.\n",
    "III\n",
    "1 Introduction 1\n",
    "1 Introduction\n",
    "1.1 The Importance of Data Quality\n",
    "“Accurate data is a fundamental requirement of good information systems” (Olson, 2008, p.\n",
    "6) and the lack of it can have negative consequences for companies, like loosing customers,\n",
    "missing opportunities and coming to wrong decisions. In order to ensure data quality a company must put effort in form of time and money into data quality assurance programs. Nowadays, the worlds biggest internet companies and web services Microsoft, Apple, Facebook,\n",
    "Google and Amazon use the data of their users to generate huge profits. Every company can\n",
    "achieve big advantages by processing data correctly and efficiently. But in order to work\n",
    "with huge datasets, it has to be trustworthy and of high quality. Data quality experts estimate\n",
    "that some businesses, including governmental and educational organizations, lose 15-25% of\n",
    "their profit due to working with poor-quality data (cf. Olson, 2008, p. 9). Data quality issues\n",
    "lead to a huge waste of time, energy and money for people, companies and their staff. The\n",
    "problem is, that those issues seem to be accepted or ignored nowadays. They have become\n",
    "invisible since many people declare those costs as normal and routine (cf. Olson, 2008, p.\n",
    "14). This shows, that a rethinking is needed to improve the data quality verification processes and that it needs big investments to overcome the current standards. Nevertheless, the\n",
    "outcome could be very substantial for everyone.\n",
    "Boehringer Ingelheim and Data Quality in the Pharmaceutical Industry\n",
    "Boehringer Ingelheim Pharma GmbH & Co.KG (BI) is a global, family-owned researching\n",
    "pharmaceutical company whose headquarter is located in Ingelheim am Rhein, Germany. BI\n",
    "focuses on researching, developing, producing and selling prescription drugs for humans and\n",
    "animals. BI has developed several so-called “Blockbusters” like e.g. “Jardiance” to treat type\n",
    "2 diabetes or “Spiriva” for chronic obstructive pulmonary diseases. Those “Blockbusters”\n",
    "pulled in sales amounting to billions. The year 2020 was, despite Covid-19, a good year\n",
    "for BI. The generated turnover was 19.57 billion euros; 3% more than in the preceding\n",
    "year. The goal of Boehringer Ingelheim is to become the number one in animal health\n",
    "and biopharmaceutical contract manufacture. (cf. Boehringer Ingelheim Pharma GmbH &\n",
    "Co. KG, 2020). An ambitious goal, since medicine, pharmacy and health are critical and\n",
    "important areas of research, that can be very expensive, long-lasting and risky. Companies\n",
    "like Boehringer Ingelheim invest billions of euros and years of work into researching and\n",
    "developing; and sometimes without even finding a satisfying result. The goal of everyone\n",
    "working for BI is to improve the steps behind drug development. This includes searching\n",
    "for a possible active ingredient, synthesizing it, testing it on animals and people (includes\n",
    "1.2 Objective of this Paper and Project 2\n",
    "finding volunteers), adjusting the drug, finding the correct dosage, waiting for the clinical\n",
    "studies to finish, gaining approval by the authorities, patenting the drug, manufacturing and\n",
    "selling it. This process can last for over 13 years; only for one age class. On average, 1.0\n",
    "to 1.6 billion US-Dollars have to be invested into the development of one drug. (cf. Die\n",
    "forschenden Pharma-Unternehmen, 2018). Ensuring data quality at Boehringer Ingelheim\n",
    "or other pharmaceutical companies can help saving time and money in every area that was\n",
    "just mentioned. Wrong data leads to wrong information, which leads to wrong knowledge\n",
    "and to wrong conclusions, which lead to wrong results in research, to wrong decisions and\n",
    "a lot of wasted time, money and effort. And in addition, especially in pharmacy, wrong data\n",
    "can lead to dangerous situations for humans.\n",
    "1.2 Objective of this Paper and Project\n",
    "The objective of this project is to automate generation and execution of data quality checkings. How could a framework look like that checks high quality data and helps bringing\n",
    "buried data to the fore for BI or other pharmaceutical companies? A framework that allows\n",
    "evaluating the quality of already existing or completely new datasets? This paper documents\n",
    "the approach to this project. It defines necessary vocabulary and explains required concepts,\n",
    "languages and functionalities. It contains reviews of the singular software tools and an explanation of how they could be connected.\n",
    "1.3 Proposed Approach to the Project\n",
    "In order to understand the necessity of the final software concept and how it works, it is\n",
    "inevitable to clearly define technical terms to fully understand the basics that lie behind\n",
    "Knowledge Graphs (KGs) or the semantic web, and to get to know standardized formats\n",
    "and languages supporting those basics. This is why chapter 2 will focus on explaining the\n",
    "background information and context of the project. Chapter 3 contains the practical content,\n",
    "like the review of the Astrea-Tool and RDFUnit Testing Suite. The last chapter is about\n",
    "the final result and gives a glimpse of how this project will affect the future at Boehringer\n",
    "Ingelheim.\n",
    "2 Foundations 3\n",
    "2 Foundations\n",
    "2.1 Definition of Data Quality\n",
    "The six following dimensions are very commonly used to describe and rate data quality.\n",
    "(Sources: (Olson, 2008, p. 24f.) & (Herzog et al., 2007, p. 8f.) & (Fleckenstein and\n",
    "Fellows, 2018, p. 103f.))\n",
    "1. Accuracy: Is the information correct?\n",
    "2. Timeliness: Is the information up-to-date?\n",
    "3. Relevance: Does the information help to answer the important (relevant) questions?\n",
    "4. Completeness: Is information missing?\n",
    "5. Credibility: Are there multiple versions of the same information?\n",
    "6. Validity: Does the information conform to the definition?\n",
    "Additional often used data quality dimensions are Currency, Consistency, Flexibility, Precision, Format, Interpretability, Content, Efficiency, Importance, Sufficiency, Usableness,\n",
    "Usefulness, Clarity, Comparability, Conciseness, Freedom of bias, Informativeness, level of\n",
    "detail, Quantitativeness, Scope, Understandability. (cf. Haug et al., 2011, p. 4f.)\n",
    "2.2 Definition of Knowledge\n",
    "It is not easy to define the term “knowledge”, especially in the English language. “Knowledge” has basically two different meanings: On the one hand “having knowledge of something” and on the other hand “having knowledge about something”. (cf. Machlup, 1981, p.\n",
    "27ff.) In other languages, like German, there are two words to describe those two cases of\n",
    "knowledge (“wissen” & “kennen”). Due to the lack of such a distinction in English, it is\n",
    "important to correctly define what kind of knowledge is meant when someone talks about\n",
    "it, so that everyone has the same understanding of the term. The knowledge portrayed by\n",
    "Knowledge Graphs (or other graph databases) does not represent what is really “known”\n",
    "about something in the epistemological sense. It is not about having a skill or understanding\n",
    "something, but instead it represents the information about different entities, things or objects\n",
    "and how they relate to each other in form of data and metadata. By doing this, Knowledge\n",
    "Graphs become the perfect tool to transform implicit knowledge buried in huge datasets (cf.\n",
    "Figure 2.1) into explicit knowledge. (cf. Blumauer and Nagy, 2020, p. 91)\n",
    "Explicit Knowledge is knowledge which can be encoded with literals, strings, mathematical equations or more and is knowledge that can be stored or be processed. Explicit knowl-\n",
    "2.3 Basics about the Semantic Web 4\n",
    "edge can be understood in an objective way and as something that exists physically in a kind\n",
    "of static collection of statements, facts or ideas. (cf. Schilcher, 2006, p. 19)\n",
    "Implicit Knowledge is knowledge “that can only be understood by the author himself.”\n",
    "(Blumauer and Nagy, 2020, p. 35). It is knowledge that cannot really get noted down because\n",
    "it is based on someones personal experiences, memories and feelings. Implicit Knowledge\n",
    "is close to practical skills and everyone could interpret it in another way (like a mindmap).\n",
    "People know very much, but cannot tell everything they know because they are not aware of\n",
    "some aspects of their knowledge and it is hard to separate the knowledge used in daily life\n",
    "from emotions, feelings and instinct.\n",
    "Figure 2.1: A huge part of the knowledge someone has, no matter if private person or\n",
    "company, is implicit. (Blumauer and Nagy, 2020, p. 91)\n",
    "An Example: If someone wants to explain how to make a cake, the cook would write down\n",
    "the used ingredients and when and how long they put them into the oven. This is called a\n",
    "recipe and is the perfect example for explicit knowledge. The knowledge by the cook is noted\n",
    "down so that everyone can easily copy it.\n",
    "But the cook bakes very often and collected a lot of practical baking skills. They know when\n",
    "the dough is perfectly finished, know little tricks to improve the final result and know how\n",
    "to react to unforeseen situations because of their experience. The cook has a lot of implicit\n",
    "knowledge which they sometimes use without even realizing and therefore it cannot be noted\n",
    "down in a recipe. That is why food in the restaurant or cakes in the bakery often taste better\n",
    "than the self-made ones at home.\n",
    "2.3 Basics about the Semantic Web\n",
    "The Internet, or World Wide Web (WWW), as it is known today was founded in 1989 by Tim\n",
    "Berners-Lee as a project at the European Organization for Nuclear Research facility, also\n",
    "known as CERN. Berners-Lee’s goal was to create a “wide-area hypermedia information\n",
    "retrieval initiative aiming to give universal access to a large universe of documents.” (cf.\n",
    "2.3 Basics about the Semantic Web 5\n",
    "Frysyk, 1994). The basic WWW is about connecting documents that contain data in the\n",
    "form of text or pictures. But already in 2001 Tim Berners-Lee talked about the next big step,\n",
    "namely the “Semantic Web”, which is more about connecting the data itself than connecting\n",
    "documents. Due to the World Wide Web Consortium (W3C), an organization founded by\n",
    "Tim Berners-Lee to create standardized web formats, the Semantic Web is about two things:\n",
    "(cf. W3C et al., 2013)\n",
    "1. Common formats for data integration and combination from diverse sources.\n",
    "2. Recording how data relates to real world objects.\n",
    "Metadata is the key in the mentioned idea of the Semantic Web. “Metadata is ’data about\n",
    "data’.” (Riley, 2017, p. 1). It provides more information about the actual data, allowing us\n",
    "to derive knowledge out of it. It is not only used to describe the appearance of the data, but\n",
    "also to describe and denote its meaning and relation to other data. For Example: A weather\n",
    "station stores data about temperature, air moisture and wind strength; but it stores not only\n",
    "the raw values, but also timestamps and coordinates. This is metadata and it can be found\n",
    "everywhere (Riley, 2017, p. 1). Using this additional metadata, the weather stations can\n",
    "create statistics for specific time periods or locations, which they can use to trace diverse\n",
    "weather developments. Other Systems, when they can rely on metadata, are able to understand words and phrases that are equivalent. (Davies et al., 2007, 3) A common example:\n",
    "When searching Google for „Jaguar“ in the context of motor industry, the Google Search\n",
    "Engine „knows“ the user is not searching for the animal because in this context the semantic\n",
    "relations to the animal are very poor (Davies et al., 2007, 3). There are different types of\n",
    "metadata that should give a good overview and understanding of what metadata really is and\n",
    "why it can be found basically everywhere (cf. Riley, 2017, 6):\n",
    "• Descriptive metadata: Understand what the data is about or what the data means.\n",
    "• Administrative metadata:\n",
    "– Technical metadata: How to decode the data or how data has to be processed.\n",
    "– Preservation metadata: How to store files for a longer time.\n",
    "– Metadata about rights: Additional information about the intellectual property\n",
    "rights.\n",
    "• Structural Metadata: Relationships between data.\n",
    "• Markup languages: Integration of metadata for additional structural or semantic features (e.g. XML).\n",
    "This already shows that metadata is the first step to ensure high quality data. It provides\n",
    "information about the different data quality dimensions from chapter 2.1, e.g. about time\n",
    "2.3 Basics about the Semantic Web 6\n",
    "(Timeliness), format (Validity) and planned usage/description (Relevance) of the data. More\n",
    "metadata helps evaluating the quality of data.\n",
    "Standardization is also an important factor in semantic web development. In the last\n",
    "decades Tim Berners-Lee and the W3C published several standards and recommendations\n",
    "to facilitate the development of the Semantic Web and Semantic Web applications. In addition, those standardization simplifies the compatibility of different applications. The most\n",
    "important standards, that will also play a part in this paper, are (Blumauer and Nagy, 2020,\n",
    "25):\n",
    "1. The Resource Description Framework (RDF) as a recommended approach to describe\n",
    "and store metadata.\n",
    "2. The Resource Description Framework Schema (RDFS) enabled the representation of\n",
    "the data in the WWW.\n",
    "3. The Web Ontology Language (OWL) was developed to allow users defining and instantiating web ontologies.\n",
    "4. The Simple Protocol and RDF Query Language (SPARQL) was developed to retrieve\n",
    "and manipulate data stored in RDF.\n",
    "5. The Shapes Constraint Language (SHACL) is used to validate graph-based data against\n",
    "a set of conditions.\n",
    "Figure 2.2: The Semantic Web is structured in different layers of formalisms and recommendations. (cf. Kingsley Uyi Idehen, 13.07.2017)\n",
    "2.4 Structure of Knowledge Graphs 7\n",
    "Figure 2.2 shows the structure of the Semantic Web and the interaction of the mentioned\n",
    "recommendations. The next chapters will introduce everything needed to follow this project.\n",
    "2.4 Structure of Knowledge Graphs\n",
    "A knowledge graph is a database that integrates data using structures known from the geometric graph theory. “The knowledge graph represents a collection of interlinked descriptions of entities.” (cf. Ontotext.com, 2018). Just like graphs in mathematics, knowledge\n",
    "graphs consist of nodes and directed edges between those nodes. Those edges connect several nodes with specific properties. Imagine a relational database in form of a table (cf.\n",
    "Figure 2.3): In a graph database, the first node conforms to the row, the property to the\n",
    "column and the second node to the value in this row and column. (like in Figure 2.4).\n",
    "Figure 2.3: What a typical relational database looks like. (Created with Visual Paradigm\n",
    "Online)\n",
    "Figure 2.4: How the data of Figure 2.3 “looks like” when stored in a graphical database.\n",
    "(Created with Visual Paradigm Online)\n",
    "2.4.1 Resource Description Framework\n",
    "The Resource Description Framework (RDF) is a standard model for data interchange in\n",
    "the semantic web. Data in RDF is stored in so-called “triples”, which consist of a subject, a\n",
    "predicate and an object. When several triples are connected to each other, this is called “RDF\n",
    "graph” (cf. RDF Working Group (2014)). The advantage of RDF and the storing of data in\n",
    "those triples is, that it is easy to read by machines as well as humans. RDF statements can\n",
    "2.4 Structure of Knowledge Graphs 8\n",
    "be visualized using directed graphs (cf. Figure 2.5) (cf. Blumauer and Nagy, 2020, p. 96ff.).\n",
    "Subjects and objects are represented as nodes. The predicate is a directed edge connecting\n",
    "two nodes. There are three different kinds of nodes:\n",
    "1. Unique Resource Identifier (URI) or International Resource Identifier (IRI)\n",
    "2. literal nodes\n",
    "3. blank nodes\n",
    "Figure 2.5: RDF triple consisting of a subject, a predicate and an object. (Created with\n",
    "Visual Paradigm Online)\n",
    "URIs are “Unique Resource Identifiers” that are used to clearly identify a thing or object\n",
    "in the real world. A URI points to a namespace or vocabulary in which the subject, predicate\n",
    "or object of the data is unambiguously defined. For example the predicate “hasTitle”: Does\n",
    "it mean the title of a book or the jobtitle in a company?. Namespaces/Vocabularies were\n",
    "created to avoid those complications and to differ between words with multiple possible\n",
    "meanings. IRIs are basically URIs with a wider range of possible characters (e.g Chinese\n",
    "symbols) that can be used to address an ontology. This is what a typical URI looks like:\n",
    "\"http://xmlns.com/foaf/0.1/\". Its structure is very similar to URLs used to address documents\n",
    "on web servers, because URLs are just special URIs. (cf. DuCharme, 2013, Chapter 2:\n",
    "URLs, URIs, IRIs and Namespaces)\n",
    "Literal nodes instead denote a literal value like strings or other data types. In Figure A.1\n",
    "the unique subject “Person:1337” has three objects that are literal values (cf. Blumauer and\n",
    "Nagy, 2020, p. 96). The “Person:” in this example is a so-called “prefix” that points to a\n",
    "specific namespace in which the subject 1337 is uniquely defined and described.\n",
    "Blank nodes are used to group data. Figure A.2 shows the address of a person stored\n",
    "using triples without blank nodes (cf. DuCharme, 2013, Chapter 2: URLs, URIs, IRIs and\n",
    "Namespaces). Figure A.3 shows how a blank node is used to group the postal address of the\n",
    "person. This helps arranging the graph and improves readability.\n",
    "The Serialization of RDF triples in an RDF graph data base is necessary to make the data\n",
    "machine-readable. There are several different formats that can be used to serialize the triples.\n",
    "Some examples are: Turtle, JSON-LD, N3 or RDF/XML (cf. Blumauer and Nagy, 2020,\n",
    "2.4 Structure of Knowledge Graphs 9\n",
    "p. 97f.). Every format has its advantages and disadvantages, but since Turtle is specially\n",
    "designed for human-understanding, Turtle will be the format used for RDF statements in\n",
    "this paper. Listing 2.1 shows the serialization of Figure A.1. In the first two lines of the\n",
    "code the prefixes “eo” and “foaf” get defined. The URIs at the end of the line point to an\n",
    "vocabulary/namespace where several objects and relations are unambiguously defined. The\n",
    "storing of data starts in line 3. Whatever or whoever is uniquely defined as “Person1337” in\n",
    "the example ontology, that is called “eo”, has the relation \"has_firstName\", which is clearly\n",
    "defined by the “Friend-of-a-Friend” (foaf) vocabulary, pointing to a literal value of type\n",
    "string with the content of “James”. Same is with his lastname and his age. In addition, what\n",
    "is not part of Figure A.1, Subject “Person1337” knows the object that is clearly defined as\n",
    "“Person1338” in the ontology. A dot always signals the end of a statement. This example\n",
    "is a bit simplified but it shows how easy it is for humans to understand Turtle and how\n",
    "the knowledge is stored and connected. The data stored in Listing 2.1 can be easily read\n",
    "by every non-computer scientist: “Person1337” is called James Parker, is 21 years old and\n",
    "knows “Person1338”.\n",
    "1 @prefix eo : < http :// www . exampleOntology . de / exampleOntology #> .\n",
    "2 @prefix foaf : < http :// xmlns . com / foaf /0.1/ > .\n",
    "3\n",
    "4 eo : Person1337 foaf : has_firstName \" James \" .\n",
    "5 eo : Person1337 foaf : has_lastName \" Parker \" .\n",
    "6 eo : Person1337 foaf : has_age 21 .\n",
    "7 eo : Person1337 foaf : knows eo : Person1338 .\n",
    "Listing 2.1: Example for RDF triples in Turtle\n",
    "It is also possible to assign specific datatypes to the literal values. It is necessary to differ\n",
    "between a date and an IBAN, although both have to be denoted as a string. Using the XML\n",
    "Schema Definition (XSD) specification by the W3C, datatypes can get defined in the triples,\n",
    "like the birthdate of “Person1337” in Listing A.1. The last important feature of RDF statements is the connecting of multiple triples. Obviously it is not very efficient to note down\n",
    "every relation of a subject separately. RDF allows the definition of several relations for the\n",
    "same subject by separating the statements with semicolons. The statement of Listing A.1\n",
    "can get shortened like it’s done in A.2.\n",
    "2.4.2 Reasoning Data with OWL\n",
    "Taxonomies are concepts or structures, which are unintentionally used by humans to find\n",
    "and classify things in hierarchies. In order to make the world more explainable and understandable and to arrange knowledge, things get assigned to other things that belong together\n",
    "(cf. Blumauer and Nagy, 2020, p. 98ff.).\n",
    "2.4 Structure of Knowledge Graphs 10\n",
    "An Example: Scientists divide natural sciences into chemistry, biology, physics etc. Then\n",
    "those sciences get divided into even more specific sciences, e.g chemistry into organic and\n",
    "inorganic chemistry etc. Everything is in some way part of a taxonomy created to draw basic\n",
    "relations between things (cf. Figure 2.6). “A taxonomy is a controlled vocabulary consisting\n",
    "of preferred terms, all of which are connected in a hierarchy or polyhierarchy.” (ANSI/NISO,\n",
    "2010, p. 18)\n",
    "Figure 2.6: Example Taxonomy that shows (roughly) how the natural sciences are divided\n",
    "into different areas of studies (inspired by (ANSI/NISO, 2010, p. 18)) (Created with Visual\n",
    "Paradigm Online)\n",
    "Ontologies are the heart of Semantic Web applications and are used to make knowledge\n",
    "machine-readable (cf. Landhäußer, n.d., p. 22). “An ontology is a formal, explicit specification of a shared conceptualization” (Studer et al., 1998, p. 25).\n",
    "• “Formal” means machine-readable.\n",
    "• “Explicit specification” implies the usage of concepts, attributes and relations.\n",
    "• It is a “conceptualization” because an ontology is an abstract model of “real world”\n",
    "phenomenons.\n",
    "• “Shared” means that the knowledge is coincident and not for private individuals, but\n",
    "to be accepted by a group\n",
    "(Source: (Studer et al., 1998, p. 25)). Ontologies are used to give more dimensionality\n",
    "to a KG (cf. Blumauer and Nagy, 2020, p. 102) by extending its structure and providing\n",
    "supplementary semantic information for the taxonomies (cf. Hüttenegger, 2006, p. 183).\n",
    "When a person has a pet of type cat, this implies that the person has a pet of type mammal.\n",
    "In addition and in contrast to a taxonomy, the ontology delivers the semantic meaning of\n",
    "the terms “person”, “cat” and “mammal” (cf. Hüttenegger, 2006, p. 183). Figure 2.7\n",
    "shows another example. It shows an ontology consisting of four concepts/classes and three\n",
    "instances of those classes that are in different relations to each other. Leonardo DaVinci is\n",
    "2.4 Structure of Knowledge Graphs 11\n",
    "a “human” who created the painting of Mona Lisa, a “painting” with values for width and\n",
    "height, that shows Mona Lisa, who is also a “human”. Because DaVinci drew a “painting”,\n",
    "he is also an instance of class “painter” (indicated by the red arrow) (cf. Landhäußer, n.d.,\n",
    "p. 23).\n",
    "Figure 2.7: Example ontology: The concepts/classes in normal bold; Individuals/Instances\n",
    "in an italic bold. (inspired by Landhäußer, n.d., p. 23) (Created with Visual Paradigm Online)\n",
    "OWL stands for “Web Ontology Language” and is a standardized language by the W3C\n",
    "to describe knowledge about things or groups of things and to define inferences/relations\n",
    "in datasets. An OWL document is nothing else than an ontology (cf. OWL Working Group,\n",
    "2012). OWL is property oriented and builds on RDFS. It allows defining domains and ranges\n",
    "and creating classes and subclasses. In addition OWL supports existence and cardinality\n",
    "constraints, so that the user can e.g. say that every person in the dataset must have exactly\n",
    "one biological mother. As mentioned when explaining taxonomies, those relationships are\n",
    "necessary to express and understand knowledge. OWL also enables transitive, inverse or\n",
    "symmetrical relations. For Example: The user can define that the relation “isPartOf” is the\n",
    "opposite of the relation “hasPart” (inverse) and that the relation “touches” counts in both directions (symmetrical). To define the ontologies with OWL, Turtle can be used again. Listing\n",
    "2.2 shows the definition of a class “Musician”. Using the RDF and RDFS vocabularies, the\n",
    "“musician” gets assigned to the type of “class”. In addition a label and a comment/description of the class is defined. The class “MusicalInstrument” is created as well as the property\n",
    "“playsInstrument”. This property has a domain pointing to the Musician class and a range\n",
    "pointing to the MusicalInstrument class. Because of this connection, whenever a person has\n",
    "the property “playsInstrument”, this person will automatically become a musician (by running an inference procedure enabling this kind of reasoning) and the object this person uses\n",
    "to make music will automatically become a musical instrument. Note that “a” (like in line\n",
    "10) is just short for “rdf:type”.\n",
    "2.5 Using SPARQL to access a Knowledge Graph 12\n",
    "1 @prefix eo : < http :// www . exampleOntology . de / exampleOntology # >.\n",
    "2 @prefix rdf : < http :// www . w3 . org /1999/02/22 - rdf - syntax - ns # > .\n",
    "3 @prefix rdfs : < http :// www . w3 . org /2000/01/ rdf - schema #> .\n",
    "4\n",
    "5 eo : Musician\n",
    "6 rdf : type rdfs : Class ;\n",
    "7 rdfs : label \" Musician \" ;\n",
    "8 rdfs : comment \" Someone who plays a musical instrument \" .\n",
    "9 eo : MusicalInstrument\n",
    "10 a rdfs : Class ;\n",
    "11 rdfs : label \" Musical instrument \" .\n",
    "12 eo : playsInstrument\n",
    "13 rdf : type rdf : Property ;\n",
    "14 rdfs : comment \" Identifies the instrument that someone plays \" ;\n",
    "15 rdfs : label \" plays instrument \" ;\n",
    "16 rdfs : domain eo : Musician ;\n",
    "17 rdfs : range eo : MusicalInstrument .\n",
    "Listing 2.2: Creating classes and properties with OWL\n",
    "2.5 Using SPARQL to access a Knowledge Graph\n",
    "SPARQL stands for “Simple Protocol and RDF Query Language” and is, as the name implies, a language to create queries to select specific data out of RDF-based graphs. SPARQL\n",
    "allows the user to filter the database. Tim Berners-Lee, the inventor of HTML, founder of\n",
    "the WWW and director of the W3C said: “Trying to use the Semantic Web without SPARQL\n",
    "is like trying to use a relational database without SQL” (cf. W3C, 2008). If the user wants\n",
    "specific data that meets specific conditions, they can use SPARQL queries with different\n",
    "commands to filter out exactly the data they is searching for. In Listing 2.3 SPARQL, is used\n",
    "to find the name of everyone Subject “P1337” knows (cf. Listing 2.1). Everyone familiar\n",
    "with relational databases and “Structured Query Language (SQL)” will recognize the similarities between SPARQL and SQL. The “SELECT” statement defines the variables which\n",
    "will be part of the final output and the “WHERE” statement contains the triple patterns to\n",
    "match. A question mark always indicates a variable. The result of the “SELECT” query is\n",
    "a table. Each selected variable becomes a column and each matched pattern becomes a row.\n",
    "Listing A.3 shows the data to work with in following examples. Listing 2.3 shows a typical\n",
    "SPARQL query. The SELECT statement declares the variables that will contain the values,\n",
    "which the query will generate as the output. The output will be the first name and last name\n",
    "of every person who knows P1338.\n",
    "1 PREFIX eo : < http :// www . exampleOntology . de / exampleOntology # >\n",
    "2 PREFIX d: < http :// www . ownOntology . de / data # >\n",
    "3 PREFIX foaf : < http :// xmlns . com / foaf /0.1/ >\n",
    "4\n",
    "2.5 Using SPARQL to access a Knowledge Graph 13\n",
    "5 SELECT ? first ? last\n",
    "6 WHERE {\n",
    "7 ? person foaf : knows d: P1338 .\n",
    "8 ? person eo : has_firstName ? first .\n",
    "9 ? person eo : has_lastName ? last .\n",
    "10 }\n",
    "Listing 2.3: A simple SPARQL query to filter the first name and last name of everyone who\n",
    "knows P1338 in the dataset (Listing A.3)\n",
    "The final output of Listing 2.3 is:\n",
    "Table 2.1: Result of Listing 2.3\n",
    "first last\n",
    "\"James\" \"Parker\"\n",
    "\"Jim\" \"Hammilton\"\n",
    "SPARQL is a simple yet very powerful tool to query and filter huge graph data bases. Additional important SPARQL keywords to follow this project are:\n",
    "• FILTER - to implement supplementary conditions, e.g. to filter every Person born\n",
    "before a specific date.\n",
    "• CONCAT - to concatenate two or more variables together.\n",
    "• BIND - to give value(s) an alias using the keyword “AS”, e.g. to store the “firstName”\n",
    "and “lastName” AS “fullName” after concatenating them. “AS” can also be used without “BIND” to create a new variable after arithmetic operations, like adding different\n",
    "prices together and storing them AS “totalPrice”.\n",
    "• CONSTRUCT - is a query form (meaning an alternative for the SELECT keyword)\n",
    "that returns triples by pulling them out of a data source without changing them. The\n",
    "values in those datasets can be used to create new triples. That is why SPARQL can\n",
    "be used to copy, create and convert data stored in RDF triples, which is very important\n",
    "for the review of the Astrea-Tool in Section 3.2.\n",
    "• OPTIONAL - to return a value only if it exists, e.g. used to search for former incidents,\n",
    "but only in case there are any. If none are found for a subject, no error will occur.\n",
    "• VALUES - to directly write or add data into a pattern or query. It allows specifying\n",
    "multiple variables in a data block.\n",
    "• URI - converts a string into a URI.\n",
    "Sources: (DuCharme, 2013, p. 47-182 (Chapters 3-5)) and the official SPARQL documentation Harris et al. (2013). There are many more possible keywords to use in SPARQL queries,\n",
    "but in order to understand this project paper, the ones mentioned above are the only ones\n",
    "needed to know. In the appendix, there is a table where all these statements are summarized\n",
    "again (cf. Table A.1).\n",
    "2.6 Advantages of Knowledge Graphs as data bases 14\n",
    "2.6 Advantages of Knowledge Graphs as data bases\n",
    "Transforming explicit knowledge into implicit knowledge is only one big advantage of KGs.\n",
    "They are the perfect tool to link data in enterprise management systems and can be used in\n",
    "many different scenarios like (cf. Blumauer and Nagy, 2020, p. 21f.):\n",
    "• Searching the Web (Google, Bing, Maps)\n",
    "• Crawl for product information (Amazon or other retailers)\n",
    "• Smart Assistants (Siri, Echo, Cortana)\n",
    "• Science Applications:\n",
    "– data exploration, data searching\n",
    "– finding buried connections in data\n",
    "– Analysis\n",
    "– Machine Learning\n",
    "Since the SPARQL queries can provide different methods of converting heterogeneous data,\n",
    "KGs can facilitate data integration from multiple sources and domains. It’s not difficult to\n",
    "transform relational data from different sources into triples, which are then stored in a KG.\n",
    "Those new triples and the KGs can get merged by comparing the data and drawing new\n",
    "connections (cf. Blumauer and Nagy, 2020, p. 69f.). The performance of graph databases\n",
    "like KGs and its queries remains relatively constant (or rather proportional) as datasets get\n",
    "bigger, while queries of relational databases tend to perform slower. In addition, graph\n",
    "databases are additive and easy to extend without any interference (cf. Robinson et al., 2015,\n",
    "p. 8f.). Another advantage of KGs is the overcoming of so-called “data silos”. Data silos\n",
    "inhibit productivity in companies and cause wasted resources, because only a specific group\n",
    "of people can fully access a set of data. When using KGs, replacing and migrating data\n",
    "becomes unnecessary. Instead data integration and linking of data get focused. This is done\n",
    "by using already existing data models to build semantic knowledge models, like ontologies.\n",
    "Those semantic solution approaches combine the benefits of data lakes and data warehouses\n",
    "and exactly mirror the ideas and interests of the semantic web (cf. Blumauer and Nagy,\n",
    "2020, p. 33f.). It is the data that matters, not the databases. The connection of data creates a\n",
    "data-centric knowledge foundation.\n",
    "2.7 Already existing Knowledge Graphs\n",
    "World Knowledge Graphs do not focus on a single field of knowledge. Instead they try\n",
    "to gather and connect all knowledge of the whole world. Examples for this kind of KGs are\n",
    "the Google Knowledge Graph, Wikidata or DBpedia. A company or even a private person\n",
    "could use subsets of those graphs containing relevant information for their concerns. World\n",
    "2.7 Already existing Knowledge Graphs 15\n",
    "Knowledge Graphs often provide useful information about general topics, like geographic\n",
    "information, that can be included in someone’s own KGs. (cf. Blumauer and Nagy, 2020, p.\n",
    "106f.)\n",
    "Domain Knowledge Graphs are already existing KGs for specific domains like (cf. Blumauer and Nagy, 2020, p. 107f.):\n",
    "• Business & Finance\n",
    "• Pharmacy & Medicine\n",
    "• Cultural Heritage\n",
    "• Sustainable Development\n",
    "• Geographic Information\n",
    "The medical sector is a pioneer in knowledge graph development (cf. Blumauer and Nagy,\n",
    "2020, p. 109f.) and for a researching pharmacy company like BI this domain could be of\n",
    "special interest. Using the web page of the Ontology Lookup Service (OLS) it is possible to\n",
    "gain access to the latest ontologies of the Pharmacy & Medical domain like the:\n",
    "• Chemical Entities of Biological Interest Ontology (ChEBI)\n",
    "• SNOMED Clinical Terms (SNOMED CT)\n",
    "• Gene Ontology\n",
    "Commonly used Vocabularies & Namespaces\n",
    "In chapter 2.4 the so-called “prefixes” were introduced. Those prefixes point to a specific\n",
    "namespace or vocabulary (in form of an ontology) which support the uniqueness of labels\n",
    "and objects. Reminder: Words like “title” can have different meanings in a different context.\n",
    "Table A.2 contains the most used vocabularies and their (common) prefixes. In addition,\n",
    "the Linked Open Vocabularies (LOV), a huge online collection of the biggest and most frequent used vocabularies and namespaces in the web, is a good source to get to know more\n",
    "ontologies (https://lov.linkeddata.es/dataset/lov/).\n",
    "3 Realisation of the Proposed Concept 16\n",
    "3 Realisation of the Proposed Concept\n",
    "3.1 The Shapes Constraint Language (SHACL)\n",
    "SHACL stands for “Shapes Constraint Language” and is a standardized language by the\n",
    "W3C to validate datasets and its individuals by creating so-called “shapes”, using already\n",
    "existing ontologies. Those shapes are applied to a set of data to verify its quality. In this\n",
    "case, quality means that the data fits every criteria, form and aspect, the user wants the data\n",
    "to fulfill. Just because a set of data is validated by SHACL, it does not mean that the data\n",
    "is automatically “high quality”. The data is just in the correct, user-demanded shape. In\n",
    "contrast to OWL, SHACL is there for validating data instead of inferencing data.\n",
    "An Example: If the shape defines that every individual of type person has to have exactly\n",
    "one integer value for the property age and this constraint should be violated, an exception\n",
    "will occur.\n",
    "3.1.1 Shapes\n",
    "are conjunctions of constraints that the targets must satisfy. Shapes are distinguished in\n",
    "“Node Shapes” and “Property Shapes”. Node Shapes declare constraints directly on a node\n",
    "while Property Shapes declare them on the property connected to the node through a “path”.\n",
    "A path is a sequence of edges connecting properties to nodes. Listing 3.1 shows a typical\n",
    "node shape of a vegetarian pizza in a pizza shop. In Listing 3.2 a property shape is defined in\n",
    "lines 4-7. The property shape inside the “RealItalianPizzaShape” demands that the base of\n",
    "this pizza has to be of type “ThinAndCrispyBase”. In case there is a “RealItalianPizza” in the\n",
    "dataset which does not have the value “ThinAndCrispyBase” for the “hasBase” predicate, an\n",
    "error will occur, since the data does not fit the SHACL shape.\n",
    "1 pizza : VegetarianPizzaShape\n",
    "2 rdf : type sh : NodeShape ;\n",
    "3 sh : targetClass pizza : VegetarianPizza ;\n",
    "4 sh : nodeKind sh : IRI .\n",
    "Listing 3.1: A typical node shape\n",
    "1 pizza : RealItalianPizzaShape\n",
    "2 a sh : NodeShape ;\n",
    "3 sh : nodeKind sh : IRI ;\n",
    "4 sh : property [ a sh : PropertyShape ;\n",
    "5 sh : class pizza : ThinAndCrispyBase ;\n",
    "6 sh : path pizza : hasBase\n",
    "7 ] ;\n",
    "8 sh : targetClass pizza : RealItalianPizza .\n",
    "3.1 The Shapes Constraint Language (SHACL) 17\n",
    "Listing 3.2: A typical property shape which is inside of a node shape\n",
    "3.1.2 Targets\n",
    "are used to specifically select certain nodes which have to be validated. It is important to\n",
    "differentiate between the kinds of targets, in order to understand SHACL:\n",
    "1. Node targets - targets a specific node in the graph (sh:targetNode).\n",
    "For Example: Validates every triple that exactly targets the node: “SalamiPizza”. (cf.\n",
    "Listing A.4)\n",
    "2. Class targets - targets a specific class (sh:targetClass)\n",
    "For Example: Validates every triple that targets every node that is of class “Pizza”. (cf.\n",
    "Listing A.5)\n",
    "3. Subjects-of-targets - targets every subject of a specific property (sh:targetSubjectsOf)\n",
    "For Example: Validates every subject of a triple that has the specific predicate/property\n",
    "“has_ingredient” (cf. Listing A.6)\n",
    "4. Objects-of-targets - targets every object of a specific property (sh:targetObjectsOf)\n",
    "For Example: Validates every object of a triple that has the specific predicate/property\n",
    "“has_ingredient” (cf. Listing A.7)\n",
    "3.1.3 Patterns\n",
    "(sh:pattern) are used to apply shapes on triples that fulfill specific criteria. For Example:\n",
    "Listing 3.3 shows a set of triples, that assign names to subjects of an ontology. The SHACL\n",
    "shape in Listing 3.4 is applied to every triple, where “eo:has_firstName” is the predicate and\n",
    "where the object follows the pattern “J” (starting with the letter “J”), which are the statements\n",
    "in line 3 and 5 of Listing 3.3.\n",
    "1 PREFIX eo : < http :// www . exampleOntology . de / exampleOntology # >\n",
    "2\n",
    "3 eo : P1337 eo : has_firstName \" James \" .\n",
    "4 eo : P1338 eo : has_firstName \" Marry \" .\n",
    "5 eo : P1339 eo : has_firstName \" Jim \" .\n",
    "Listing 3.3: RDF statements that connect subjects to first names\n",
    "1 PREFIX eo : < http :// www . exampleOntology . de / exampleOntology # >\n",
    "2 PREFIX sh : < http :// www . w3 . org / ns / shacl # >\n",
    "3\n",
    "4 eo : NameWithJExample\n",
    "5 a sh : NodeShape ;\n",
    "3.1 The Shapes Constraint Language (SHACL) 18\n",
    "6 sh : targetNode eo : P1337 , eo : P1338 , eo : P1339 ;\n",
    "7 sh : property [\n",
    "8 sh : path eo : has_firstName ;\n",
    "9 sh : pattern \"^J\" ; # apply for every first name starting with J\n",
    "10 ] .\n",
    "Listing 3.4: A SHACL shape that is applied on every triple declaring a first name starting\n",
    "with J using “sh:pattern”\n",
    "3.1.4 Validation\n",
    "Only the lines 1 and 3 of Listing 3.5 are validated by the shape defined in Listing 3.1, because\n",
    "the node kind of the statement in line 5 is an integer, not an IRI (like the shape demands it).\n",
    "In line 3 a URI / IRI points to the node, in line 1 the Margarita is an “instance” of this URI /\n",
    "IRI. That’s why line 1 is also a valid statement. Remember that “a” stands for “rdf:type”.\n",
    "1 pizza : Margarita a pizza : VegetarianPizza .\n",
    "2\n",
    "3 < www . pizza . de / margarita > a pizza : VegetarianPizza .\n",
    "4\n",
    "5 :5 a pizza : VegetarianPizza .\n",
    "Listing 3.5: Only the RDF statements in line 1 and line 3 are valid according to Listing 3.1\n",
    "Using SHACL shapes, it is also possible to filter the data and to put special constraints on it.\n",
    "Cardinality constraints, like the “minCount” or “maxCount” constraint, for example. They\n",
    "can be put on classes or nodes to say that every pizza must have at least one topping and can\n",
    "have a maximum of four toppings.\n",
    "The basics that were just explained is everything needed to understand this paper. In case\n",
    "something new appears, it will be explained in the corresponding chapter.\n",
    "3.1.5 SHACL vs. ShEx\n",
    "Shape Expressions (ShEx) is a data modeling language developed to validate and describe\n",
    "Resource Description Frameworks, just like SHACL does. So why use SHACL instead of\n",
    "ShEx? Since they were developed to fulfill the same tasks, SHACL and ShEx have many\n",
    "similarities. Nevertheless, there are some differences. Although both languages work with\n",
    "shapes and constraints and both have many features and syntactic rules in common (cf. Labra\n",
    "Gayo et al., 2018, Chapter 7.1: ’Common Features’ & Chapter 7.2: ’Syntactic Differences’),\n",
    "they differ in their foundation and their way of how they work. While SHACL works with\n",
    "constraints to validate RDF graphs by checking if the constraints are satisfied, ShEx can\n",
    "rather be understood as a grammatic schema, that describes a RDF graph. This means that\n",
    "the validation results of ShEx look different than SHACL’s results. It shows which nodes\n",
    "3.2 Astrea-Tool: Generation of Shapes 19\n",
    "and shapes were matched (in form of an annotaded graph), instead of clearly depicting which\n",
    "constraint has been violated and why. SHACL’s error description is way more detailed and\n",
    "precise. It is easier to detect and fix issues of RDF graphs using SHACL (cf. Labra Gayo\n",
    "et al., 2018, Chapter 7.3: ’Foundation: Schema vs. Constraints’). This is exactly what BI\n",
    "wants to do with its knowledge graphs. That is why SHACL is preferred to ShEx.\n",
    "3.2 Astrea-Tool: Generation of Shapes\n",
    "“Astrea” is an open source tool that uses own KG mappings consisting of SPARQL construct queries to generate SHACL shapes automatically for a set of ontologies. Astrea uses\n",
    "the “Astrea-KG”, that contains 158 mappings, all relating a different ontology constraint\n",
    "pattern with an equivalent SHACL constraint pattern (cf. Andrea Cimmino et al., n.d., p.\n",
    "498). The implemented SPARQL queries consist of CONSTRUCT statements containing\n",
    "the SHACL construct patterns and WHERE statements containing the ontology construct\n",
    "patterns. The ontology patterns get recognized and translated into the equivalent SHACL\n",
    "patterns (cf. Andrea Cimmino et al., n.d., p. 500f) using the Astrea-KG and the mappings\n",
    "defined in the “Queries.csv” file located in the “material” directory of the Astrea-Tool. Every\n",
    "mapping in the CSV file (Comma-Separated-Values) consists of nine columns:\n",
    "• The IMPLEMENTED and the ORDER columns are not relevant to understand the tool,\n",
    "since the values of those are always the same. (just metadata for the mapping)\n",
    "• The TOPIC column classifies, what the mapping is about to do, like “Class definiton”\n",
    "or “Object Property definition”.\n",
    "• The OWL CONSTRUCT column contains one or more ontology construct patterns\n",
    "from OWL, RDFS and XSD specifications. The SHACL CONSTRUCT column contains the equivalent SHACL construct patterns. (cf. Figure 3.1)\n",
    "• The SHACL CONSTRUCT TYPE column (additional metadata for the mapping)\n",
    "• The GRAPH PATTERN SOURCE and GRAPH PATTERN TARGET columns contain\n",
    "the patterns, that will get recognized (either the ontology pattern with OWL/RDFS/XSD statements, or the SHACL pattern). The mapping works in both directions.\n",
    "(cf. Figure 3.1)\n",
    "• The QUERY column, that contains the SPARQL query executing the mapping/translation (cf. Figure 3.1)\n",
    "Figure A.4 in the appendix shows how the mapping by Astrea works in detail. Since it\n",
    "would be to much to review every line of code (over 4000 lines), chapter A.1 in the appendix\n",
    "contains some code examples of following queries.\n",
    "3.2 Astrea-Tool: Generation of Shapes 20\n",
    "Figure 3.1: Simple overview of how Astrea works (Andrea Cimmino et al., n.d., p. 503).\n",
    "3.2.1 Generating Shapes using Queries\n",
    "Listing 3.6 shows the query of a “Restriction Pattern”, the type of pattern that appears most\n",
    "frequently (77 times) in the code. Other important types will also be explained here, but their\n",
    "code examples can only be found in the appendix.\n",
    "1 PREFIX rdfs : < http :// www . w3 . org /2000/01/ rdf - schema #>\n",
    "2 PREFIX sh : < http :// www . w3 . org / ns / shacl #>\n",
    "3 PREFIX rdf : < http :// www . w3 . org /1999/02/22 - rdf - syntax - ns #>\n",
    "4 PREFIX xsd : < http :// www . w3 . org /2001/ XMLSchema # >\n",
    "5\n",
    "6 CONSTRUCT {\n",
    "7 ? shapeUrl a sh : PropertyShape ;\n",
    "8 sh : pattern ? restrictionPattern .\n",
    "9 } WHERE {\n",
    "10 ? property a ? propertyType .\n",
    "11 VALUES ? propertyType { owl : DatatypeProperty rdfs : Datatype }\n",
    "12 ? property owl : withRestrictions ? restrictionsList .\n",
    "13 ? restrictionsList rdf : rest */ rdf : first ? restrictionElement .\n",
    "14 OPTIONAL { ? restrictionElement xsd : pattern ? restrictionPattern .\n",
    "}\n",
    "15 FILTER (! isBlank (? property )) .\n",
    "16 BIND ( URI ( CONCAT (’https :// astrea . linkeddata .es/ shapes #’, MD5 ( STR (?\n",
    "property )))) AS ? shapeUrl ) .\n",
    "17 }\n",
    "Listing 3.6: A Query of type “Pattern Restriction”\n",
    "Reminder: The CONSTRUCT statement contains the SHACL pattern and the WHERE statement the ontology pattern. The query in Listing 3.6 creates a SHACL Property Shape with\n",
    "a “sh:pattern” restriction, which specifies a regular expression, that the value node needs to\n",
    "match. The “restriction pattern(s)” come from the ontology, as can be seen in the WHERE\n",
    "3.2 Astrea-Tool: Generation of Shapes 21\n",
    "statement. Table A.1 in the appendix compactly summarizes the most important SPARQL\n",
    "statements mentioned earlier for refreshing. Other Astrea-Queries:\n",
    "Object Property Definition\n",
    "• appear seven times (second most often) in the mappings.\n",
    "• code example can be found in Listing A.8 .\n",
    "• Those queries create SHACL property shapes to ensure that their node kind is “BlankNodeOrIRI”. Property shapes obviously can’t be literals, because they represent relations between subjects and objects. The properties have to be unambiguously defined,\n",
    "either using an IRI or a blank node as a placeholder.\n",
    "Comment Annotations\n",
    "• code example can be found in Listing A.9\n",
    "• used to create SHACL shapes containing descriptions (or alternative labels) for entities\n",
    "Label Annotations\n",
    "• code example can be found in Listing A.12\n",
    "• Looks very similar to Comment Annotations. Here the shape contains the name of\n",
    "entities. Both the “Label annotation” and the “Comment annotation” use “rdfs:label”\n",
    "in the CONSTRUCT query. This is because the “rdfs:label” can be used in both ways\n",
    "and to express names as well as descriptions or comments.\n",
    "3.2.2 Functionality of Astrea\n",
    "Finally, Figure 3.2 illustrates the functionality of Astrea in six steps. (cf. Andrea Cimmino\n",
    "et al., n.d., p. 504f)\n",
    "1. Ontology Manager is fed with a set of ontology URLs as input.\n",
    "2. Ontology Manager checks the owl:import statements.\n",
    "3. Ontology Manager downloads the referenced ontologies.\n",
    "4. Ontology Manager sends the downloaded ontologies to the KG-Manager.\n",
    "5. KG-Manager reads the mappings of the Astrea-KG.\n",
    "6. KG-Manager produces an RDF graph containing SHACL shapes fitting to the ontology\n",
    "construct mappings encoded in the CONSTRUCT query for every ontology sent by the\n",
    "Ontology Manager. An RDF graph containing all SHACL shapes is returned.\n",
    "3.3 RDFUnit Testing Suite: Generation of Test Cases 22\n",
    "Figure 3.2: Architecture and Functionality of the Astrea-Tool\n",
    "Since Astrea only takes ontologies into account where no instances are expected in the data,\n",
    "restrictions referring to those instances are not supported by the tool. In addition Astrea does\n",
    "not support restrictions that require a practitioner to establish them. (cf. Andrea Cimmino\n",
    "et al., n.d., 506f.) For more and very detailed information visit the official documentation\n",
    "of the Astrea-Tool and its github repository or read the official paper explaining the backgrounds of the tool. (cf. Andrea Cimmino et al., n.d.) Accordingly, modifying Astrea and\n",
    "its mappings is easily done by adjusting the SPARQL queries in the “Queries.csv” file.\n",
    "3.3 RDFUnit Testing Suite: Generation of Test Cases\n",
    "The project team of the research group “Agile Knowledge Engineering and Semantic Web\n",
    "(AKSW)” that developed “RDFUnit” describes the tool as a “test driven data-debugging\n",
    "framework that can run automatically generated (based on a schema) and manually generated\n",
    "test cases against an endpoint” (Auer et al. (2014)). They use SPARQL queries to execute\n",
    "the test cases with pattern-based transformation.\n",
    "3.3.1 Terminology & Basic Notions\n",
    "• Test cases are data constraints consisting of one or more triples. In addition, “a test\n",
    "case is an input on which the program [or dataset; author’s note] under test is executed\n",
    "during testing.” (cf. Zhu et al., 1997, p. 3).\n",
    "• Test suites (or test sets) are sets of test cases to apply to a dataset.\n",
    "• The status of a test case or suite can be Success, Fail or Error.\n",
    "3.3 RDFUnit Testing Suite: Generation of Test Cases 23\n",
    "• A Data Quality Test Pattern (DQTP) can be understood as a tuple consisting of a\n",
    "set of typed pattern variables and a SPARQL query template with placeholders for\n",
    "those variables. DQTPs can be used to compare the values of multiple properties. (cf.\n",
    "Listing A.13)\n",
    "• Test Pattern Bindings are valid instances of a DQTP. They are triples consisting of a\n",
    "variable mapping, a SPARQL query template and an error classification.\n",
    "• Data Quality Test Cases are formed when the mappings of the pattern binding are\n",
    "applied to the SPARQL query template. This creates an executable SPARQL query that\n",
    "returns results. Results can be: Success (empty result), violation (results are returned)\n",
    "or a timeout (test is marked for further inspection). (cf. Listing A.14)\n",
    "• A Test Auto Generator (TAG) converts RDFS/OWL axioms/schemes into concrete\n",
    "test cases. TAGs consist of two parts: The detection part querying against a schema\n",
    "(cf. Listing A.15) and an execution part instantiating a test case from the respective\n",
    "pattern (cf. Listing A.16).\n",
    "Sources: Auer et al. (2014) & (Kontokostas et al., 2014, p. 2f)\n",
    "3.3.2 Functionality of RDFUnit\n",
    "Figure 3.3 illustrates the operating principle of RDFUnit. Test cases can be generated using\n",
    "already existing RDFS/OWL schemes as input for the TAGs. RDFUnit has additional methods and features that can automatically improve a schema to create further test cases (But\n",
    "those test cases are explicitly labeled, so that everybody knows, that those are less reliable\n",
    "than other test cases). Integrated in RDFUnit is a pattern library that enables reusing test\n",
    "cases when vocabularies/namespaces are detected that were already used before. The pattern\n",
    "library of RDFUnit contains 17 DQTPs that can be applied to the shapes that appear most\n",
    "often in SHACL or OWL. Table A.3 contains every pattern together with a description and\n",
    "an example binding (Kontokostas et al., 2014, p. 3ff.). The team of AKSW created 32293\n",
    "total unique test cases using 297 LOVs (cf. Kontokostas et al., 2014, p. 5).\n",
    "The Inspection Report, created after RDFUnit is finished, provides information about\n",
    "the generated test cases: How many cases passed, how many cases failed or timed out and\n",
    "how many errors occurred in general. This inspection report clearly depicts the quality of\n",
    "the tested datasets. Less errors imply a better dataset. Or at least a dataset that fits the given\n",
    "shape. It also shows which axioms cause the most errors. But only because a dataset contains\n",
    "many errors or violations it is not straightforward of \"low quality\". The user still has to look\n",
    "up what has caused the errors. Sometimes, only a few missing statements in disadvantageous\n",
    "positions that are demanded by some vocabularies can cause millions of consequential errors\n",
    "3.3 RDFUnit Testing Suite: Generation of Test Cases 24\n",
    "Figure 3.3: Flowchart of the RDFUnit functionality. Left: Different possible input sources.\n",
    "Middle: Different ways to instantiate the patterns. Right: Different Data Quality Test Cases.\n",
    "(Kontokostas et al., 2014, p. 4)\n",
    "that build up on each other (cf. Kontokostas et al., 2014, p. 8). The Inspection Report looks\n",
    "like in Figure 3.4. It shows the header of every test report summarizing the most important\n",
    "numbers. The header shows the link to the dataset that has been tested, timecodes, how\n",
    "many test cases were executed and what their results were. In case there are violations in the\n",
    "dataset, those will be displayed tabularly beneath the header. This table has four columns\n",
    "and looks like Table 3.1.\n",
    "Figure 3.4: Summary of an inspection report by RDFUnit.\n",
    "The 1st column contains the type of violation that occurred. The 2nd one the message that\n",
    "belongs to the corresponding violation. The “Resource” column shows exactly which object\n",
    "of the dataset caused the violation and the “Test Case” column shows in which of the test\n",
    "cases that were generated the violation appeared. Every violation is explained, traceable and\n",
    "thus rectifiable.\n",
    "3.4 Data Structure at Boehringer Ingelheim 25\n",
    "Table 3.1: An example Inspection Report by RDFUnit\n",
    "Level Message Resource Test Case\n",
    "<Violation> <Reason of Violation> <Link to Source of Violation> <Test ID>\n",
    "ERROR Value does not match pattern https://ontology.com/object1 URN:1\n",
    "WARNING Expected Value missing https://ontology.com/object2 URN:1\n",
    "TIMEOUT Can’t connect to URI https://ontology.com/object3 URN:1\n",
    "3.4 Data Structure at Boehringer Ingelheim\n",
    "Boehringer Ingelheim has an intern graph database accessible for every employee who has\n",
    "the necessary rights by the URI “https://data.boehringer.com/ontology/”. In the\n",
    "following examples the “substance” ontology part of the database will be used to explain the\n",
    "data structure and how Astrea and RDFUnit can work together to verify the data at BI.\n",
    "Example RDF Triples of the BI intern ontology:\n",
    "1 @prefix sub : < https :// data . boehringer . com / ontology / substance / >.\n",
    "2 @prefix rdf : < http :// www . w3 . org /1999/02/22 - rdf - syntax - ns # > .\n",
    "3 @prefix owl : < http :// www . w3 . org /2002/07/ owl #> .\n",
    "4 @prefix rdfs : < http :// www . w3 . org /2000/01/ rdf - schema #> .\n",
    "5 @prefix xsd : < http :// www . w3 . org /2001/ XMLSchema #> .\n",
    "6 @prefix sh : < http :// www . w3 . org / ns / shacl #>\n",
    "Listing 3.7: Consider those prefixes to be active in every following listing of this section.\n",
    "1 sub : C1 a owl : class .\n",
    "2 sub : C2 a owl : class .\n",
    "3\n",
    "4 sub : A2 a owl : FunctionalProperty , owl : DatatypeProperty .\n",
    "5 sub : A3 a owl : FunctionalProperty , owl : DatatypeProperty .\n",
    "6\n",
    "7 sub : R1 a owl : ObjectProperty , owl : AsymmetricProperty , owl :\n",
    "IrreflexiveProperty .\n",
    "8 sub : R2 a owl : ObjectProperty , owl : AsymmetricProperty , owl :\n",
    "IrreflexiveProperty .\n",
    "Listing 3.8: Examples for creating classes & types. Reminder: “a” stands for “rdf:type” !\n",
    "Listing 3.8 shows six different subjects.\n",
    "• C1 and C2 are substances that get assigned to the rdf:type of “class”. Listing 3.9 labels\n",
    "them “Substance” and makes them a subclass of “owl:Thing”.\n",
    "• A2 and A3 are FunctionalProperties & DatatypeProperties. DatatypeProperties are\n",
    "subclasses of FunctionalProperties, making their declaration redundant. A2 and A3\n",
    "assign “ChEMBL codes” (codes for a chemical database) to classes (cf. Listing 3.9\n",
    "lines 5-8)\n",
    "3.4 Data Structure at Boehringer Ingelheim 26\n",
    "• Subjects R1 and R2 are asymmetric, irreflexive ObjectProperties used to depict molecule\n",
    "heritages (cf. Listing 3.9 lines 10-13).\n",
    "1 sub : C1 rdfs : comment \"Any matter of defined composition that has discrete\n",
    "existence , whose origin may be biological , mineral or chemical .\" @en .\n",
    "2 sub : C1 rdfs : label \" Substance \"@en .\n",
    "3 sub : C1 rdfs : subClassOf owl : Thing .\n",
    "4\n",
    "5 sub : A2 rdfs : comment \" ChEMBL codes identifies the substances depicted by\n",
    "the ChEMBL DB.\"\n",
    "6 sub : A2 rdfs : label \" has ChEMBL code \" @en .\n",
    "7 sub : A2 rdfs : domain sub : C2 .\n",
    "8 sub : A2 rdfs : range xsd : string .\n",
    "9\n",
    "10 sub : R1 rdfs : comment \" Parent molecule of its alternative form .\" @en .\n",
    "11 sub : R1 rdfs : label \" has parent molecule \" @en .\n",
    "12 sub : R1 rdfs : domain sub : C2 .\n",
    "13 sub : R1 rdfs : range sub : C2 .\n",
    "Listing 3.9: BI internal triples that further describe the subjects C1, A2 and R1 in the dataset.\n",
    "4 Final Results & Outlook to the Future 27\n",
    "4 Final Results & Outlook to the Future\n",
    "4.1 Verification of Data Quality\n",
    "4.1.1 Appyling Astrea to BI data\n",
    "Listing 4.1 shows the SHACL shape (or schema) of the C1 subject which is generated by the\n",
    "Astrea tool when applied to the set of data in Listings 3.8 and 3.9. The SHACL shapes for\n",
    "the subjects A2 and R1 from the former chapter can be found in the appendix (cf. Listings\n",
    "A.17 and A.18).\n",
    "1 < https :// astrea . linkeddata . es / shapes #68 cdee7760285c160898001ac30720b0 >\n",
    "2 a sh : NodeShape ;\n",
    "3 rdfs : label \" Substance \" @en ;\n",
    "4 rdfs : seeAlso \" https :// schema . org / Substance \"^^ xsd : anyURI ;\n",
    "5 sh : description \" Any matter of defined composition that has\n",
    "discrete existence , whose origin may be biological , mineral\n",
    "or chemical .\" @en ;\n",
    "6 sh : name \" Substance \"@en ;\n",
    "7 sh : nodeKind sh : IRI ;\n",
    "8 sh : property < https :// astrea . linkeddata . es / shapes #8\n",
    "f9c4d0490ddd476d5d86b9b49a14653 > ;\n",
    "9 sh : targetClass < https :// data . boehringer . com / ontology / substance /\n",
    "C1 > .\n",
    "Listing 4.1: SHACL Shape for Subject C1 of Listings 3.8 and 3.9\n",
    "4.1.2 Running RDFUnit with BI data shapes\n",
    "After Astrea generated the SHACL shapes corresponding to the “substance” ontology part of\n",
    "Boehringer Ingelheim’s database, RDFUnit accordingly created an inspection report. This\n",
    "report shows no violations in the dataset. The header of the report looks like Figure 3.4.\n",
    "All test cases passed without any error. The corresponding table that shows the violations in\n",
    "detail is consequently empty.\n",
    "4.2 Workflow-Concept of Astrea & RDFUnit\n",
    "Figure 4.1 shows how the process of data validation looks like if the tools are split. An\n",
    "employee has to create the shapes using the online tool of Astrea manually. Then they have\n",
    "to put the shapes into a specific folder before they execute RDFUnit with the necessary\n",
    "parameters (link to the ontology, path to shapes etc.). This is the process that was used for\n",
    "the section above. If necessary, an employee can manually edit the shapes as well (e.g. to\n",
    "fix small errors caused by trivial details). Astrea and RDFUnit work together to generate a\n",
    "report, that depicts irregularities and errors in the tested datasets.\n",
    "4.2 Workflow-Concept of Astrea & RDFUnit 28\n",
    "Figure 4.1: Toolchain of the manual concept for data quality testing at BI currently. (Created\n",
    "with Visual Paradigm Online)\n",
    "Figure 4.2 shows how the process looks like after connecting the tools. If they have been\n",
    "installed correctly, those tools can simply be combined by executing several commands in\n",
    "a console, since Astrea can easily be imported as a library into other Java projects (like\n",
    "RDFUnit). In Astrea’s documentation is a simple explanation of how to use the tool. A\n",
    "model has to be created which needs the URI to the ontology that the tool shall generate the\n",
    "shapes of. RDFUnit then only needs to know where to find the shapes and the dataset to\n",
    "validate. Listing 4.2 shows the final command to run RDFUnit.\n",
    "1 $ java - jar rdfunit - validate -0.8.24 - SNAPSHOT - standalone . jar\n",
    "2 -d < LINK TO DATASET >\n",
    "3 -e < LINK TO ONTOLOGY >\n",
    "4 -eu < username > -ep < password >\n",
    "5 -s < LINK TO SCHEMAS ( SHAPES ) >\n",
    "6 -r shacl\n",
    "Listing 4.2: Command to run RDFUnit in Linux.\n",
    "4.3 The Effect to Boehringer Ingelheim 29\n",
    "Figure 4.2: Toolchain of a possible automated concept for data quality testing. (Created\n",
    "with Visual Paradigm Online)\n",
    "4.3 The Effect to Boehringer Ingelheim\n",
    "This project paper summarizes and explains the basics about knowledge graphs and the semantic web. It shows how graph-based data stores work and what their advantages are. This\n",
    "topic is not completely new to Boehringer Ingelheim. There are already huge ontologies\n",
    "and RDF-datasets existing in the company. Despite that, this topic is still very new. The\n",
    "tools showcased in this project paper enable an easy verification of data from BI or other\n",
    "companies. By combining the tools as mentioned, it is possible to execute the automated test\n",
    "cases for every RDF-based dataset with only a few parameter adjustments. Since BI is a researching company, new data, new information and new knowledge is going to be generated\n",
    "continuously. Astrea and RDFUnit are going to play an important part in validating data that\n",
    "is going to be added to BI’s knowledge graph.\n",
    "4.4 The next steps\n",
    "The next steps in this development are going to be mainly about two things:\n",
    "1. Implement traceability to comprehend the results of Astrea & RDFUnit.\n",
    "2. Modify & adjust to the tools to get rid of misleading violations and to adapt to BI\n",
    "structures.\n",
    "4.4 The next steps 30\n",
    "At the moment, the internal functionality of Astrea and RDFUnit is not very transparent. It\n",
    "is not easy to reason why a shape looks like it does or why and how RDFUnit interprets\n",
    "something as an error or a warning. The tools help verifying data quality, implying that\n",
    "the data also becomes trustworthy, but now it is the tools that are not trustworthy enough to\n",
    "completely rely on them. A company like Boehringer Ingelheim or other global companies\n",
    "should be able to give suitable reasons for their statements and actions. Astrea and especially\n",
    "RDFUnit have to get analyzed very deeply and probably have to undergo many modifications\n",
    "before BI can go the next steps of creating a user-friendly framework that enables a simple\n",
    "usage for everyone. This project paper lies the foundation for such analyses and future\n",
    "projects.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "t= \"\"\"Humans are social beings. Whether we like it or not, nearly everything we do in our lives takes place in the company of others. Few of\n",
    "our activities are truly solitary and scarce are the times when we are\n",
    "really alone. Thus the study of how we are able to interact with one\n",
    "another, and what happens when we do, would seem to be one of the\n",
    "most fundamental concerns of anyone interested in human life. Yet\n",
    "strangely enough, it was not until relatively recently – from about the\n",
    "beginning of the nineteenth century onwards – that a specialist interest in this intrinsically social aspect of human existence was treated\n",
    "with any seriousness. Before that time, and even since, other kinds\n",
    "of interests have dominated the analysis of human life. Two of the\n",
    "most resilient, non-social approaches to human behaviour have been\n",
    "‘naturalistic’ and ‘individualistic’ explanations.\n",
    "Rather than seeing social behaviour as the product of interaction,\n",
    "these theories have concentrated on the presumed qualities inherent\n",
    "in individuals. On the one hand, naturalistic explanations suppose\n",
    "that all human behaviour – social interaction included – is a product\n",
    "of the inherited dispositions we possess as animals. We are, like animals,\n",
    "biologically programmed by nature. On the other hand, individualistic\n",
    "explanations baulk at such grand generalizations about the inevitability of behaviour. From this point of view we are all ‘individual’ and\n",
    "‘different’. Explanations of human behaviour must therefore always\n",
    "rest ultimately on the particular and unique psychological qualities\n",
    "of individuals. Sociological theories are in direct contrast to these\n",
    "2 An Introduction to Sociological Theories\n",
    "‘non-social’ approaches. Looking a little closer at them, and discovering\n",
    "what is wrong or incomplete about them, makes it easier to understand\n",
    "why sociological theories exist.\n",
    "Naturalistic theories\n",
    "Naturalistic explanations of human activity are common enough. For\n",
    "example, in our society it is often argued that it is only natural for\n",
    "a man and a woman to fall in love, get married and have children.\n",
    "It is equally natural for this nuclear family to live as a unit on their\n",
    "own, with the husband going out to work to earn resources for his\n",
    "dependants, while his wife, at least for the early years of her children’s\n",
    "lives, devotes herself to looking after them – to being a mother. As\n",
    "they grow up and acquire more independence, it is still only ‘natural’\n",
    "for the children to live at home with their parents, who are responsible for them, at least until their late teens. By then it is only natural\n",
    "for them to want to ‘leave the nest’, to start to ‘make their own way in\n",
    "the world’ and, in particular, to look for marriage partners. Thus\n",
    "they, too, can start families of their own.\n",
    "The corollary of these ‘natural’ practices is that it is somehow unnatural not to want to get married, or to marry for reasons other than\n",
    "love. It is equally unnatural for a couple not to want to have children,\n",
    "or for wives not to want to be mothers, or for mothers not to want to\n",
    "devote the whole of their lives to child-rearing. Though it is not right\n",
    "or natural for children to leave home much younger than eighteen,\n",
    "it is certainly not natural for them not to want to leave home at all\n",
    "in order to start a family of their own. However, these ‘unnatural’\n",
    "desires and practices are common enough in our society. There are\n",
    "plenty of people who prefer to stay single, or ‘marry with an eye on\n",
    "the main chance’. There are plenty of women who do not like the idea\n",
    "of motherhood, and there is certainly any number of women who do\n",
    "not want to spend their lives solely being wives and mothers. There\n",
    "are plenty of children who want to leave home long before they are\n",
    "eighteen while there are many who are quite happy to stay as members of their parents’ households until long after that age.\n",
    "Why is this? If human behaviour is, in fact, the product of a disposition inherent in the nature of the human being then why are such\n",
    "deviations from what is ‘natural’ so common? We can hardly put\n",
    "down the widespread existence of such ‘unnatural’ patterns of behaviour to some kind of large-scale, faulty genetic programming.\n",
    "In any case, why are there so many variations from these notions\n",
    "of ‘normal’ family practices in other kinds of human societies? Both\n",
    "An Introduction to Sociological Theories 3\n",
    "history and anthropology provide us with stark contrasts in family life.\n",
    "In his book on family life in Medieval Europe, Centuries of Childhood\n",
    "(1973), Philippe Ariès paints a picture of marriage, the family and\n",
    "child-rearing which sharply contradicts our notions of normality. Families were not then, as they are for us today, private and isolated units,\n",
    "cut off socially, and physically separated from the world at large.\n",
    "Families were deeply embedded in the community, with people living\n",
    "essentially public, rather than private, lives. They lived in households\n",
    "whose composition was constantly shifting: relatives, friends, children,\n",
    "visitors, passers-by and animals all slept under the same roof. Marriage\n",
    "was primarily a means of forging alliances rather than simply the\n",
    "outcome of ‘love’, while women certainly did not look upon mothering\n",
    "as their sole destiny. Indeed, child-rearing was a far less demanding\n",
    "and onerous task than it is in our world. Children were not cosseted\n",
    "and coddled to anywhere near the extent we consider ‘right’. Many\n",
    "more people – both other relatives and the community at large – were\n",
    "involved in child-rearing, and childhood lasted a far shorter time than\n",
    "it does today. As Ariès (1973) puts it, ‘as soon as he had been weaned,\n",
    "or soon after, the child became the natural companion of the adult’.\n",
    "In contemporary non-industrial societies, too, there is a wide range\n",
    "of variations in family practices. Here again, marriage is essentially a\n",
    "means of establishing alliances between groups, rather than simply a\n",
    "relationship between individuals. Monogamy – one husband and one\n",
    "wife – is only one form of marriage. Polygyny, marriage between a\n",
    "husband and more than one wife, and polyandry, between a wife and\n",
    "more than one husband, are found in many societies. Domestic life is\n",
    "also far more public and communal than it is in industrial societies.\n",
    "Each family unit is just a part of a much wider, cooperating group\n",
    "of mainly blood relatives associated with a local territory, usually a\n",
    "village. As in Medieval Europe, therefore, child-rearing is not considered the principal responsibility of parents alone, but involves a far\n",
    "greater number of people, relatives and non-relatives.\n",
    "Clearly, then, to hope to explain human life simply by reference to\n",
    "natural impulses common to all is to ignore the one crucial fact that\n",
    "sociology directs attention to: human behaviour varies according to\n",
    "the social settings in which people find themselves.\n",
    "Individualistic theories\n",
    "What of individualistic explanations? How useful is the argument that\n",
    "behaviour is the product of the psychological make-up of individuals?\n",
    "The employment of this kind of theory is extremely common. For\n",
    "4 An Introduction to Sociological Theories\n",
    "example, success or failure in education is often assumed to be merely\n",
    "a reflection of intelligence: bright children succeed and dim children\n",
    "fail. Criminals are often taken to be people with certain kinds of\n",
    "personality: they are usually seen as morally deficient individuals, lacking any real sense of right or wrong. Unemployed people are equally\n",
    "often condemned as ‘work-shy’, ‘lazy’ or ‘scroungers’ – inadequates\n",
    "who would rather ‘get something for nothing’ than work for it. Suicide\n",
    "is seen as the act of an unstable person – an act undertaken when, as\n",
    "coroners put it, ‘the balance of the mind was disturbed’. This kind of\n",
    "explanation is attractive for many people and has proved particularly\n",
    "resilient to sociological critique. But a closer look shows it to be\n",
    "seriously flawed.\n",
    "If educational achievement is simply a reflection of intelligence then\n",
    "why do children from manual workers’ homes do so badly compared\n",
    "with children from middle-class homes? It is clearly nonsensical to\n",
    "suggest that doing one kind of job rather than another is likely to\n",
    "determine the intelligence of your child. Achievement in education\n",
    "must in some way be influenced by the characteristics of a child’s\n",
    "background.\n",
    "Equally, the fact that the majority of people convicted of a crime\n",
    "come from certain social categories must cast serious doubt on the\n",
    "‘deficient personality’ theory. The conviction rate is highest for young\n",
    "males, especially blacks, who come from manual, working-class\n",
    "or unemployed backgrounds. Can we seriously believe that criminal\n",
    "personalities are likely to be concentrated in such social categories?\n",
    "As in the case of educational achievement, it is clear that the conviction of criminals must somehow be influenced by social factors.\n",
    "Again, is it likely that the million or so people presently unemployed are typically uninterested in working when the vast majority\n",
    "of them have been forced out of their jobs, either by ‘downsizing’ or\n",
    "by the failure of the companies they worked for – as a result of social\n",
    "forces quite outside their control?\n",
    "Suicide would seem to have the strongest case for being explained\n",
    "as a purely psychological act. But if it is simply a question of\n",
    "‘an unsound mind’, then why does the rate of suicide vary between\n",
    "societies? Why does it vary between different groups within the same\n",
    "society? Also, why do the rates within groups and societies remain\n",
    "remarkably constant over time? As in other examples, social factors\n",
    "must be exerting some kind of influence; explanations at the level of\n",
    "the personality are clearly not enough.\n",
    "Variations such as these demonstrate the inadequacy of theories of\n",
    "human behaviour which exclusively emphasize innate natural drives,\n",
    "An Introduction to Sociological Theories 5\n",
    "or the unique psychological make-up of individuals. If nature is at the\n",
    "root of behaviour, why does it vary according to social settings? If we\n",
    "are all different individuals acting according to the dictates of unique\n",
    "psychological influences, why do different people in the same social\n",
    "circumstances behave similarly and in ways others can understand?\n",
    "Clearly there is a social dimension to human existence, which requires\n",
    "sociological theorizing to explain it.\n",
    "All sociological theories thus have in common an emphasis on the\n",
    "way human belief and action is the product of social influences. They\n",
    "differ as to what these influences are, and how they should be investigated and explained. This book is about these differences.\n",
    "We shall now examine three distinct kinds of theory – consensus,\n",
    "conflict and action theories – each of which highlights specific social\n",
    "sources of human behaviour. Though none of the sociologists whose\n",
    "work we will spend the rest of the book examining falls neatly into\n",
    "any one of these three categories of theory, discussing them now will\n",
    "produce two benefits:\n",
    "• it will serve as an accessible introduction to theoretical debates in\n",
    "sociology; and\n",
    "• it will act as useful reference points against which to judge and\n",
    "compare the work of the subject’s major theorists.\n",
    "Society as a structure of rules\n",
    "The influence of culture on behaviour\n",
    "Imagine you live in a big city. How many people do you know well?\n",
    "Twenty? Fifty? A hundred? Now consider how many other people\n",
    "you encounter each day, about whom you know nothing. For example, how many complete strangers do people living in London\n",
    "or Manchester or Birmingham come into contact with each day? On\n",
    "the street, in shops, on buses and trains, in cinemas or night clubs\n",
    "– everyday life in a big city is a constant encounter with complete\n",
    "strangers. Yet even if city dwellers bothered to reflect on this fact,\n",
    "they would not normally leave their homes quaking with dread about\n",
    "how all these hundreds of strangers would behave towards them.\n",
    "Indeed, they hardly, if ever, think about it. Why? Why do we take\n",
    "our ability to cope with strangers so much for granted? It is because\n",
    "nearly all the people we encounter in our everyday lives do behave in\n",
    "ways we expect. We expect bus passengers, shoppers, taxi-drivers,\n",
    "6 An Introduction to Sociological Theories\n",
    "passers-by, and so on, to behave in quite definite ways even though\n",
    "we know nothing about them personally. City dwellers in particular\n",
    "– though it is true of all of us to some extent – routinely enter settings\n",
    "where others are going about their business both expecting not to\n",
    "know them, and yet also expecting to know how they will behave.\n",
    "And, more than this, we are nearly always absolutely right in both\n",
    "respects. We are only surprised if we encounter someone who is not a\n",
    "stranger – ‘Fancy meeting you here! Isn’t it a small world!’ – or if one\n",
    "of these strangers actually does behave strangely – ‘Mummy, why is\n",
    "that man shouting and waving his arms about?’ Why is this? Why do\n",
    "others do what we expect of them? Why is disorder or the unexpected\n",
    "among strangers so rare?\n",
    "Structural-consensus theory\n",
    "One of the traditional ways in which sociologists explain the order\n",
    "and predictability of social life is by regarding human behaviour as\n",
    "learned behaviour. This approach is known – for reasons that will\n",
    "become apparent – as structural-consensus theory. The key process\n",
    "this theory emphasizes is called socialization. This term refers to the\n",
    "way in which human beings learn the kinds of behaviour expected\n",
    "of them in the social settings in which they find themselves. From\n",
    "this point of view, societies differ because the kinds of behaviour\n",
    "considered appropriate in them differ. People in other societies think\n",
    "and behave differently because they have learned different rules about\n",
    "how to behave and think. The same goes for different groups within\n",
    "the same society. The actions and ideas of one group differ from\n",
    "those of another because its members have been socialized into different rules.\n",
    "Consensus sociologists use the term culture to describe the rules\n",
    "that govern thought and behaviour in a society. Culture exists prior\n",
    "to the people who learn it. At birth, humans are confronted by a\n",
    "social world already in existence. Joining this world involves learning\n",
    "‘how things are done’ in it. Only by learning the cultural rules of a\n",
    "society can a human interact with other humans. Because they have\n",
    "been similarly socialized, different individuals will behave similarly.\n",
    "Consensus theory thus argues that a society’s cultural rules determine, or structure, the behaviour of its members, channelling their\n",
    "actions in certain ways rather than others. They do so in much the\n",
    "same way that the physical construction of a building structures the\n",
    "actions of the people inside it. Take the behaviour of students in a\n",
    "An Introduction to Sociological Theories 7\n",
    "school. Once inside the school they will display quite regular patterns\n",
    "of behaviour. They will all walk along corridors, up and down stairs,\n",
    "in and out of classrooms, through doors, and so on. They will, by and\n",
    "large, not attempt to dig through floors, smash through walls, or\n",
    "climb out of windows. Their physical movements are constrained by\n",
    "the school building. Since this affects all the students similarly, their\n",
    "behaviour inside the school will be similar – and will exhibit quite\n",
    "definite patterns. In consensus theory, the same is true of social life.\n",
    "Individuals will behave similarly in the same social settings because\n",
    "they are equally constrained by cultural rules. Though these social\n",
    "structures are not visible in the way physical structures are, those who\n",
    "are socialized into their rules find them comparably determining.\n",
    "The levels at which these cultural rules operate can vary. Some\n",
    "rules, like laws for instance, operate at the level of the whole society\n",
    "and structure the behaviour of everyone who lives in it. Others are\n",
    "much less general, structuring the behaviour of people in quite specific social settings. For example, children in a classroom are expected\n",
    "to behave in an orderly and attentive fashion. In the playground\n",
    "much more license is given them, while away from school their behaviour often bears little resemblance to that expected of them during\n",
    "school hours. Similarly, when police officers or nurses or members of\n",
    "the armed forces are ‘on duty’, certain cultural rules structure their\n",
    "behaviour very rigidly. Out of uniform and off duty these constraints\n",
    "do not apply, though other ones do instead – those governing their\n",
    "behaviour as fathers and mothers, or husbands and wives, for instance.\n",
    "This shows how the theory of a social structure of cultural rules\n",
    "operates. The rules apply not to the individuals themselves, but to the\n",
    "positions in the social structure they occupy. Shoppers, police officers,\n",
    "traffic wardens, schoolteachers or pupils are constrained by the cultural expectations attached to these positions, but only when they\n",
    "occupy them. In other circumstances, in other locations in the social\n",
    "structure – as fathers or mothers, squash players, football supporters,\n",
    "church members, and so on – other rules come into play.\n",
    "Sociologists call positions in a social structure roles. The rules that\n",
    "structure the behaviour of their occupants are called norms. There\n",
    "are some cultural rules that are not attached to any particular role\n",
    "or set of roles. Called values, these are in a sense summaries of approved ways of living, and act as a base from which particular norms\n",
    "spring. So, for example: ‘education should be the key to success’;\n",
    "‘family relationships should be the most important thing to protect’;\n",
    "‘self-help should be the means to individual fulfilment’. All these\n",
    "are values, and they provide general principles from which norms\n",
    "8 An Introduction to Sociological Theories\n",
    "directing behaviour in schools and colleges, in the home and at work\n",
    "are derived.\n",
    "According to this sociological theory, socialization into norms\n",
    "and values produces agreement, or consensus, between people about\n",
    "appropriate behaviour and beliefs without which no human society\n",
    "can survive. This is why it is called structural-consensus theory.\n",
    "Through socialization, cultural rules structure behaviour, guarantee a\n",
    "consensus about expected behaviour, and thereby ensure social order.\n",
    "Clearly, in a complex society there are sometimes going to be competing norms and values. For example, while some people think it is\n",
    "wrong for mothers to go out to work, many women see motherhood\n",
    "at best as a real imposition and at worst as an infringement of their\n",
    "liberty. Children often encourage each other to misbehave at school\n",
    "and disapprove of their peers who refuse to do so. Teachers usually\n",
    "see this very much the other way round! The Tory Party Conference\n",
    "is annually strident in its condemnation of any speaker who criticizes\n",
    "the police. Some young blacks would be equally furious with any\n",
    "of their number who had other than a strongly belligerent attitude\n",
    "towards them.\n",
    "Consensus theorists explain such differences in behaviour and\n",
    "attitude in terms of the existence of alternative cultural influences,\n",
    "characteristic of different social settings. A good example of this\n",
    "emphasis is their approach to educational inequality.\n",
    "Educational inequality: a consensus theory analysis\n",
    "Educational research demonstrates, in the most conclusive fashion,\n",
    "that achievement in education is strongly linked to class membership,\n",
    "gender and ethnic origin. There is overwhelming evidence, for example, that working-class children of similar intelligence to children\n",
    "from middle-class backgrounds achieve far less academically than their\n",
    "middle-class counterparts.\n",
    "To explain this, consensus theorists turn to stock concepts in their\n",
    "approach to social life – norms, values, socialization and culture. Starting from the basic assumption that behaviour and belief are caused by\n",
    "socialization into particular rules, their explanation of working-class\n",
    "underachievement in education seeks to identify:\n",
    "• the cultural influences which propel middle-class children to academic success\n",
    "• the cultural influences which drag working-class children down to\n",
    "mediocrity.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP-Projektrealisierung",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
