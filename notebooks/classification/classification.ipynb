{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# import tensorflow as tf\n",
    "# from tensorflow import keras\n",
    "# from tensorflow.keras import layers\n",
    "\n",
    "from ast import literal_eval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"../../data/data_train.csv\").drop([\"Unnamed: 0\"], axis=1)\n",
    "data_test = pd.read_csv(\"../../data/data_test.csv\").drop([\"Unnamed: 0\"], axis=1)\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 26\u001b[0m\n\u001b[0;32m     22\u001b[0m     num_stops \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(stops)\n\u001b[0;32m     24\u001b[0m     \u001b[39mreturn\u001b[39;00m sentences, num_sentences, words_with_stopwords, num_words_with_stopwords, words_without_stopwords, num_words_without_stopwords, lemmas, stops, num_stops\n\u001b[1;32m---> 26\u001b[0m data_train[\u001b[39m'\u001b[39m\u001b[39msentences\u001b[39m\u001b[39m'\u001b[39m], data_train[\u001b[39m'\u001b[39m\u001b[39mnum_sentences\u001b[39m\u001b[39m'\u001b[39m], data_train[\u001b[39m'\u001b[39m\u001b[39mwords_with_stopwords\u001b[39m\u001b[39m'\u001b[39m], data_train[\u001b[39m'\u001b[39m\u001b[39mnum_words_with_stopwords\u001b[39m\u001b[39m'\u001b[39m], data_train[\u001b[39m'\u001b[39m\u001b[39mwords_without_stopwords\u001b[39m\u001b[39m'\u001b[39m], data_train[\u001b[39m'\u001b[39m\u001b[39mnum_words_without_stopwords\u001b[39m\u001b[39m'\u001b[39m], data_train[\u001b[39m'\u001b[39m\u001b[39mlemmas\u001b[39m\u001b[39m'\u001b[39m], data_train[\u001b[39m'\u001b[39m\u001b[39mstops\u001b[39m\u001b[39m'\u001b[39m], data_train[\u001b[39m'\u001b[39m\u001b[39mnum_stops\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mdata_train[\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(preprocess_text))\n\u001b[0;32m     27\u001b[0m data_test[\u001b[39m'\u001b[39m\u001b[39msentences\u001b[39m\u001b[39m'\u001b[39m], data_test[\u001b[39m'\u001b[39m\u001b[39mnum_sentences\u001b[39m\u001b[39m'\u001b[39m], data_test[\u001b[39m'\u001b[39m\u001b[39mwords_with_stopwords\u001b[39m\u001b[39m'\u001b[39m], data_test[\u001b[39m'\u001b[39m\u001b[39mnum_words_with_stopwords\u001b[39m\u001b[39m'\u001b[39m], data_test[\u001b[39m'\u001b[39m\u001b[39mwords_without_stopwords\u001b[39m\u001b[39m'\u001b[39m], data_test[\u001b[39m'\u001b[39m\u001b[39mnum_words_without_stopwords\u001b[39m\u001b[39m'\u001b[39m], data_test[\u001b[39m'\u001b[39m\u001b[39mlemmas\u001b[39m\u001b[39m'\u001b[39m], data_test[\u001b[39m'\u001b[39m\u001b[39mstops\u001b[39m\u001b[39m'\u001b[39m], data_test[\u001b[39m'\u001b[39m\u001b[39mnum_stops\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mdata_test[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(preprocess_text))\n\u001b[0;32m     28\u001b[0m data_train\u001b[39m.\u001b[39mto_csv(\u001b[39m\"\u001b[39m\u001b[39m../../data/data_with_features/data_train_with_features.csv\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[0;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m   4668\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4670\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\apply.py:1105\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1102\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m   1104\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1105\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\apply.py:1156\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1154\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1155\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m-> 1156\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[0;32m   1157\u001b[0m             values,\n\u001b[0;32m   1158\u001b[0m             f,\n\u001b[0;32m   1159\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[0;32m   1160\u001b[0m         )\n\u001b[0;32m   1162\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1163\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1164\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1165\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\_libs\\lib.pyx:2918\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[24], line 3\u001b[0m, in \u001b[0;36mpreprocess_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_text\u001b[39m(text):\n\u001b[1;32m----> 3\u001b[0m     doc \u001b[39m=\u001b[39m nlp(text)\n\u001b[0;32m      4\u001b[0m     sentences \u001b[39m=\u001b[39m [sent\u001b[39m.\u001b[39mtext \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m doc\u001b[39m.\u001b[39msents]\n\u001b[0;32m      5\u001b[0m     num_sentences \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(sentences)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\spacy\\language.py:1026\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1024\u001b[0m     error_handler \u001b[39m=\u001b[39m proc\u001b[39m.\u001b[39mget_error_handler()\n\u001b[0;32m   1025\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1026\u001b[0m     doc \u001b[39m=\u001b[39m proc(doc, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcomponent_cfg\u001b[39m.\u001b[39mget(name, {}))  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1028\u001b[0m     \u001b[39m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[0;32m   1029\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE109\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\spacy\\pipeline\\attributeruler.py:144\u001b[0m, in \u001b[0;36mAttributeRuler.__call__\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    143\u001b[0m     matches \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmatch(doc)\n\u001b[1;32m--> 144\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset_annotations(doc, matches)\n\u001b[0;32m    145\u001b[0m     \u001b[39mreturn\u001b[39;00m doc\n\u001b[0;32m    146\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\spacy\\pipeline\\attributeruler.py:178\u001b[0m, in \u001b[0;36mAttributeRuler.set_annotations\u001b[1;34m(self, doc, matches)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mIndexError\u001b[39;00m:\n\u001b[0;32m    169\u001b[0m     \u001b[39m# The original exception is just our conditional logic, so we\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     \u001b[39m# raise from.\u001b[39;00m\n\u001b[0;32m    171\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    172\u001b[0m         Errors\u001b[39m.\u001b[39mE1001\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    173\u001b[0m             patterns\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmatcher\u001b[39m.\u001b[39mget(span\u001b[39m.\u001b[39mlabel),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    176\u001b[0m         )\n\u001b[0;32m    177\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m--> 178\u001b[0m set_token_attrs(span[index], attrs)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Funktion zur Tokenisierung, Lemmatisierung, Stop-Word-Removal der Texte\n",
    "def preprocess_text(text):\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    num_sentences = len(sentences)\n",
    "    \n",
    "    words_without_stopwords = []\n",
    "    words_with_stopwords = []\n",
    "    lemmas = []\n",
    "    stops = []\n",
    "\n",
    "    for token in doc:\n",
    "        if not token.is_stop and not token.is_punct:\n",
    "            words_without_stopwords.append(token.text)\n",
    "            lemmas.append(token.lemma_)\n",
    "        elif not token.is_punct:\n",
    "            words_with_stopwords.append(token)\n",
    "            stops.append(token)\n",
    "\n",
    "    num_words_without_stopwords = len(words_without_stopwords)\n",
    "    num_words_with_stopwords = len(words_with_stopwords)\n",
    "    num_stops = len(stops)\n",
    "\n",
    "    return sentences, num_sentences, words_with_stopwords, num_words_with_stopwords, words_without_stopwords, num_words_without_stopwords, lemmas, stops, num_stops\n",
    "\n",
    "data_train['sentences'], data_train['num_sentences'], data_train['words_with_stopwords'], data_train['num_words_with_stopwords'], data_train['words_without_stopwords'], data_train['num_words_without_stopwords'], data_train['lemmas'], data_train['stops'], data_train['num_stops'] = zip(*data_train['text'].apply(preprocess_text))\n",
    "data_test['sentences'], data_test['num_sentences'], data_test['words_with_stopwords'], data_test['num_words_with_stopwords'], data_test['words_without_stopwords'], data_test['num_words_without_stopwords'], data_test['lemmas'], data_test['stops'], data_test['num_stops'] = zip(*data_test['text'].apply(preprocess_text))\n",
    "data_train.to_csv(\"../../data/data_with_features/data_train_with_features.csv\")\n",
    "data_test.to_csv(\"../../data/data_with_features/data_test_with_features.csv\")\n",
    "\n",
    "# etwa 10 Minuten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"../../data/data_with_features/data_train_with_features.csv\").drop([\"Unnamed: 0\"], axis=1)\n",
    "data_test = pd.read_csv(\"../../data/data_with_features/data_test_with_features.csv\").drop([\"Unnamed: 0\"], axis=1)\n",
    "\n",
    "data_train = data_train.sample(frac=1).reset_index(drop=True)\n",
    "data_test = data_test.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Officials of India and Bangladesh on Thursday agreed to construct gates along the border to allow free and safe passage for wild elephants.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = data_train[\"sentences\"]\n",
    "literal_eval(x[3])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classification</th>\n",
       "      <th>text</th>\n",
       "      <th>sentences</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>words_with_stopwords</th>\n",
       "      <th>num_words_with_stopwords</th>\n",
       "      <th>words_without_stopwords</th>\n",
       "      <th>num_words_without_stopwords</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>stops</th>\n",
       "      <th>num_stops</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scientific</td>\n",
       "      <td>the gravitational astronomy is now becoming a ...</td>\n",
       "      <td>['the gravitational astronomy is now becoming ...</td>\n",
       "      <td>148</td>\n",
       "      <td>[the, is, now, becoming, a, the, such, as, and...</td>\n",
       "      <td>2045</td>\n",
       "      <td>['gravitational', 'astronomy', 'reality', '.in...</td>\n",
       "      <td>2877</td>\n",
       "      <td>['gravitational', 'astronomy', 'reality', '.in...</td>\n",
       "      <td>[the, is, now, becoming, a, the, such, as, and...</td>\n",
       "      <td>2045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Scientific</td>\n",
       "      <td>a geometry lies in the foundation of physics ,...</td>\n",
       "      <td>['a geometry lies in the foundation of physics...</td>\n",
       "      <td>212</td>\n",
       "      <td>[a, in, the, of, and, a, of, is, very, for, th...</td>\n",
       "      <td>3052</td>\n",
       "      <td>['geometry', 'lies', 'foundation', 'physics', ...</td>\n",
       "      <td>3836</td>\n",
       "      <td>['geometry', 'lie', 'foundation', 'physics', '...</td>\n",
       "      <td>[a, in, the, of, and, a, of, is, very, for, th...</td>\n",
       "      <td>3052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>news</td>\n",
       "      <td>Sports Direct's Mike Ashley has won a lawsuit ...</td>\n",
       "      <td>[\"Sports Direct's Mike Ashley has won a lawsui...</td>\n",
       "      <td>3</td>\n",
       "      <td>['s, has, a, over, a, made, in, a, in, In, the...</td>\n",
       "      <td>29</td>\n",
       "      <td>['Sports', 'Direct', 'Mike', 'Ashley', 'won', ...</td>\n",
       "      <td>35</td>\n",
       "      <td>['Sports', 'Direct', 'Mike', 'Ashley', 'win', ...</td>\n",
       "      <td>['s, has, a, over, a, made, in, a, in, In, the...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>news</td>\n",
       "      <td>Officials of India and Bangladesh on Thursday ...</td>\n",
       "      <td>['Officials of India and Bangladesh on Thursda...</td>\n",
       "      <td>3</td>\n",
       "      <td>[of, and, on, to, along, the, to, and, for, ha...</td>\n",
       "      <td>25</td>\n",
       "      <td>['Officials', 'India', 'Bangladesh', 'Thursday...</td>\n",
       "      <td>32</td>\n",
       "      <td>['official', 'India', 'Bangladesh', 'Thursday'...</td>\n",
       "      <td>[of, and, on, to, along, the, to, and, for, ha...</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>story</td>\n",
       "      <td>Every one knows through what adventure King Fr...</td>\n",
       "      <td>['Every one knows through what adventure King ...</td>\n",
       "      <td>60</td>\n",
       "      <td>[Every, one, through, what, the, first, of, th...</td>\n",
       "      <td>1208</td>\n",
       "      <td>['knows', 'adventure', 'King', 'Francis', 'tak...</td>\n",
       "      <td>899</td>\n",
       "      <td>['know', 'adventure', 'King', 'Francis', 'take...</td>\n",
       "      <td>[Every, one, through, what, the, first, of, th...</td>\n",
       "      <td>1208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>reviews</td>\n",
       "      <td>this is a great movie its not 40yr old virgin ...</td>\n",
       "      <td>['this is a great movie its not 40yr old virgi...</td>\n",
       "      <td>4</td>\n",
       "      <td>[this, is, a, its, not, or, up, but, its, a, o...</td>\n",
       "      <td>41</td>\n",
       "      <td>['great', 'movie', '40yr', 'old', 'virgin', 'f...</td>\n",
       "      <td>39</td>\n",
       "      <td>['great', 'movie', '40yr', 'old', 'virgin', 'f...</td>\n",
       "      <td>[this, is, a, its, not, or, up, but, its, a, o...</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>reviews</td>\n",
       "      <td>This games makes even amazing games like starc...</td>\n",
       "      <td>['This games makes even amazing games like sta...</td>\n",
       "      <td>3</td>\n",
       "      <td>[This, even, and, has, it, all, and, does, it,...</td>\n",
       "      <td>14</td>\n",
       "      <td>['games', 'makes', 'amazing', 'games', 'like',...</td>\n",
       "      <td>22</td>\n",
       "      <td>['game', 'make', 'amazing', 'game', 'like', 's...</td>\n",
       "      <td>[This, even, and, has, it, all, and, does, it,...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>Scientific</td>\n",
       "      <td>many quantum information protocols involve non...</td>\n",
       "      <td>['many quantum information protocols involve n...</td>\n",
       "      <td>345</td>\n",
       "      <td>[many, to, their, beyond, the, by, is, by, a, ...</td>\n",
       "      <td>2380</td>\n",
       "      <td>['quantum', 'information', 'protocols', 'invol...</td>\n",
       "      <td>4000</td>\n",
       "      <td>['quantum', 'information', 'protocol', 'involv...</td>\n",
       "      <td>[many, to, their, beyond, the, by, is, by, a, ...</td>\n",
       "      <td>2380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998</th>\n",
       "      <td>reviews</td>\n",
       "      <td>Many parts of this book are difficult to under...</td>\n",
       "      <td>['Many parts of this book are difficult to und...</td>\n",
       "      <td>3</td>\n",
       "      <td>[Many, of, this, are, to, for, the, and, It, i...</td>\n",
       "      <td>31</td>\n",
       "      <td>['parts', 'book', 'difficult', 'understand', '...</td>\n",
       "      <td>23</td>\n",
       "      <td>['part', 'book', 'difficult', 'understand', 'b...</td>\n",
       "      <td>[Many, of, this, are, to, for, the, and, It, i...</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999</th>\n",
       "      <td>news</td>\n",
       "      <td>Google has clarified that it has no \"informati...</td>\n",
       "      <td>['Google has clarified that it has no \"informa...</td>\n",
       "      <td>2</td>\n",
       "      <td>[has, that, it, has, no, on, its, with, to, 's...</td>\n",
       "      <td>26</td>\n",
       "      <td>['Google', 'clarified', 'information', 'record...</td>\n",
       "      <td>32</td>\n",
       "      <td>['Google', 'clarify', 'information', 'record',...</td>\n",
       "      <td>[has, that, it, has, no, on, its, with, to, 's...</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     classification                                               text  \\\n",
       "0        Scientific  the gravitational astronomy is now becoming a ...   \n",
       "1        Scientific  a geometry lies in the foundation of physics ,...   \n",
       "2              news  Sports Direct's Mike Ashley has won a lawsuit ...   \n",
       "3              news  Officials of India and Bangladesh on Thursday ...   \n",
       "4             story  Every one knows through what adventure King Fr...   \n",
       "...             ...                                                ...   \n",
       "1995        reviews  this is a great movie its not 40yr old virgin ...   \n",
       "1996        reviews  This games makes even amazing games like starc...   \n",
       "1997     Scientific  many quantum information protocols involve non...   \n",
       "1998        reviews  Many parts of this book are difficult to under...   \n",
       "1999           news  Google has clarified that it has no \"informati...   \n",
       "\n",
       "                                              sentences  num_sentences  \\\n",
       "0     ['the gravitational astronomy is now becoming ...            148   \n",
       "1     ['a geometry lies in the foundation of physics...            212   \n",
       "2     [\"Sports Direct's Mike Ashley has won a lawsui...              3   \n",
       "3     ['Officials of India and Bangladesh on Thursda...              3   \n",
       "4     ['Every one knows through what adventure King ...             60   \n",
       "...                                                 ...            ...   \n",
       "1995  ['this is a great movie its not 40yr old virgi...              4   \n",
       "1996  ['This games makes even amazing games like sta...              3   \n",
       "1997  ['many quantum information protocols involve n...            345   \n",
       "1998  ['Many parts of this book are difficult to und...              3   \n",
       "1999  ['Google has clarified that it has no \"informa...              2   \n",
       "\n",
       "                                   words_with_stopwords  \\\n",
       "0     [the, is, now, becoming, a, the, such, as, and...   \n",
       "1     [a, in, the, of, and, a, of, is, very, for, th...   \n",
       "2     ['s, has, a, over, a, made, in, a, in, In, the...   \n",
       "3     [of, and, on, to, along, the, to, and, for, ha...   \n",
       "4     [Every, one, through, what, the, first, of, th...   \n",
       "...                                                 ...   \n",
       "1995  [this, is, a, its, not, or, up, but, its, a, o...   \n",
       "1996  [This, even, and, has, it, all, and, does, it,...   \n",
       "1997  [many, to, their, beyond, the, by, is, by, a, ...   \n",
       "1998  [Many, of, this, are, to, for, the, and, It, i...   \n",
       "1999  [has, that, it, has, no, on, its, with, to, 's...   \n",
       "\n",
       "      num_words_with_stopwords  \\\n",
       "0                         2045   \n",
       "1                         3052   \n",
       "2                           29   \n",
       "3                           25   \n",
       "4                         1208   \n",
       "...                        ...   \n",
       "1995                        41   \n",
       "1996                        14   \n",
       "1997                      2380   \n",
       "1998                        31   \n",
       "1999                        26   \n",
       "\n",
       "                                words_without_stopwords  \\\n",
       "0     ['gravitational', 'astronomy', 'reality', '.in...   \n",
       "1     ['geometry', 'lies', 'foundation', 'physics', ...   \n",
       "2     ['Sports', 'Direct', 'Mike', 'Ashley', 'won', ...   \n",
       "3     ['Officials', 'India', 'Bangladesh', 'Thursday...   \n",
       "4     ['knows', 'adventure', 'King', 'Francis', 'tak...   \n",
       "...                                                 ...   \n",
       "1995  ['great', 'movie', '40yr', 'old', 'virgin', 'f...   \n",
       "1996  ['games', 'makes', 'amazing', 'games', 'like',...   \n",
       "1997  ['quantum', 'information', 'protocols', 'invol...   \n",
       "1998  ['parts', 'book', 'difficult', 'understand', '...   \n",
       "1999  ['Google', 'clarified', 'information', 'record...   \n",
       "\n",
       "      num_words_without_stopwords  \\\n",
       "0                            2877   \n",
       "1                            3836   \n",
       "2                              35   \n",
       "3                              32   \n",
       "4                             899   \n",
       "...                           ...   \n",
       "1995                           39   \n",
       "1996                           22   \n",
       "1997                         4000   \n",
       "1998                           23   \n",
       "1999                           32   \n",
       "\n",
       "                                                 lemmas  \\\n",
       "0     ['gravitational', 'astronomy', 'reality', '.in...   \n",
       "1     ['geometry', 'lie', 'foundation', 'physics', '...   \n",
       "2     ['Sports', 'Direct', 'Mike', 'Ashley', 'win', ...   \n",
       "3     ['official', 'India', 'Bangladesh', 'Thursday'...   \n",
       "4     ['know', 'adventure', 'King', 'Francis', 'take...   \n",
       "...                                                 ...   \n",
       "1995  ['great', 'movie', '40yr', 'old', 'virgin', 'f...   \n",
       "1996  ['game', 'make', 'amazing', 'game', 'like', 's...   \n",
       "1997  ['quantum', 'information', 'protocol', 'involv...   \n",
       "1998  ['part', 'book', 'difficult', 'understand', 'b...   \n",
       "1999  ['Google', 'clarify', 'information', 'record',...   \n",
       "\n",
       "                                                  stops  num_stops  \n",
       "0     [the, is, now, becoming, a, the, such, as, and...       2045  \n",
       "1     [a, in, the, of, and, a, of, is, very, for, th...       3052  \n",
       "2     ['s, has, a, over, a, made, in, a, in, In, the...         29  \n",
       "3     [of, and, on, to, along, the, to, and, for, ha...         25  \n",
       "4     [Every, one, through, what, the, first, of, th...       1208  \n",
       "...                                                 ...        ...  \n",
       "1995  [this, is, a, its, not, or, up, but, its, a, o...         41  \n",
       "1996  [This, even, and, has, it, all, and, does, it,...         14  \n",
       "1997  [many, to, their, beyond, the, by, is, by, a, ...       2380  \n",
       "1998  [Many, of, this, are, to, for, the, and, It, i...         31  \n",
       "1999  [has, that, it, has, no, on, its, with, to, 's...         26  \n",
       "\n",
       "[2000 rows x 11 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Scientific       1.00      1.00      1.00       104\n",
      "        news       0.94      0.99      0.96        97\n",
      "     reviews       0.96      0.96      0.96       113\n",
      "       story       1.00      0.94      0.97        86\n",
      "\n",
      "    accuracy                           0.97       400\n",
      "   macro avg       0.97      0.97      0.97       400\n",
      "weighted avg       0.97      0.97      0.97       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "features = vectorizer.fit_transform(data_train[\"text\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, data_train[\"classification\"], test_size=0.2, random_state=42)\n",
    "\n",
    "model = svm.SVC(probability=True)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"the gravitational astronomy is now becoming a reality .in fact , the ground - based laser interferometers such as tama300 @xcite and the first ligo @xcite are beginning to take data at sensitivities where astrophysical events are predicted . for the detectors including geo600 and virgo , core - collapse supernovae especially in our galaxy ,have been supposed to be the most plausible sources of gravitational waves ( see , for example , @xcite for review ) . since the gravitational wave ( plus neutrinos ) is the only tool which gives us the information in the innermost part of evolved massive stars , the detection is important not only for the direct confirmation of gravitational waves but also for the understanding of supernova physics itself .so far , most of the theoretical predictions of gravitational waves from supernovae have focused on the bounce signal in the context of rotational @xcite and magnetorotational @xcite core collapse . in most of the previous studies , the iron core prior to core - collapse was assumed to rotate much more rapidly than predicted by the recent stellar evolution calculations @xcite .recently , the initial rotation periods were estimated to be larger than @xmath1 100 sec for the observed rotation periods of the radio pulsars @xcite .in such a slowly rotating case , the bounce signal becomes too small to be detected even by the laser interferometers in the next generation for a galactic supernova , owing to the suppression of the rotation - induced deformation at core - bounce ( see , e.g. , @xcite ) .besides the rapid rotation of the cores , two other ingredients have been considered to be important in the much later phases after core bounce , namely convective motions and anisotropic neutrino emissions .both of them contribute to the non - spherical parts in the energy momentum tensor of the einstein equations , thus being the potential sources of the gravitational wave ( see , @xcite for a review ) .one of the possibility as the origin of the asphericities may be large scale density inhomogeneities formed in the central core prior to collapse ( e.g. , @xcite ) .@xcite performed three dimensional sph simulations and pointed out that the neutrino - originated gravitational waves , which dominate over the one from the convections , are within the detection limits for the advanced ligo for the galactic supernova ( see also , @xcite ) . another possibility to induceanisotropy is the ( moderate ) rotation of the core .@xcite calculated the gravitational waves based on the two - dimensional ( 2d ) boltzmann transport simulations of slowly rotating core @xcite and found that the neutrino - originated gravitational waves exceed the bounce signal large enough to be detectable by the advanced ligo with good signal - to - noise ratio for the galactic supernova ( see , also @xcite for the properties of neutrino - originated gravitational waves in the rapidly rotating case ) .more recently , the new ingredient of the gravitational - wave emissions is reported @xcite , namely the g - mode excitations of the protoneutron stars , which was observed in the 2d approximate boltzmann transport simulations at much later postbounce phase ( @xmath2 ms ) @xcite .there is an another ingredient for producing large asphericity , to which much attention has been paid recently in the context of the studies about the explosion mechanisms , that is the so - called standing accretion shock instability ( often called `` sasi '' ) . in the numerical simulations by @xcite , it was found that the standing shock wave is shown to be unstable to non - radial perturbations , and that the perturbations grow up to the non linear regime with clear low - mode ( @xmath3 ) dominance , leading to the global deformation of the shock wave later .here @xmath4 stands for the azimuthal index of the legendre polynomials .the importance of sasi is also stressed by the recent studies , demonstrating that such an explosion is favorable to reproduce the observed synthesized elements of sn1987a @xcite and also to explain the origin of the natal kicks of young pulsars @xcite .these situations motivate us to study how the gravitational waveforms are originated from the asphericities by sasi .    in this paper , we present the results of numerical experiments , in which we study how the asphericities induced by the growth of sasi produce the gravitational waveforms . to obtain the neutrino - driven explosions , we parameterize the neutrino fluxes emitted from the central protoneutron star and approximate the neutrino transfer by the light - bulb scheme . based on the long - term two dimensional hydrodynamic results , we calculate the gravitational waveforms . by doing the spectrum analysis ,we study the detectability of such signals from a nearby core - collapse supernova . it is noted that much attention has been paid recently to the core - collapse supernovae as one of the promising sources of the cosmological gravitational wave backgrounds ( see , e.g. , @xcite and references therein ) .thus we calculate the sasi - induced gravitational wave backgrounds and discuss the detectability by the currently proposed space - based detectors such as lisa , bbo , and decigo @xcite .the plan of this paper is as follows : in section [ sec2 ] , we outline the initial models , the numerical methods , and shortly summarize the methods for calculating the waveforms .we show the main numerical results in section [ sec3 ] .we summarize and discuss our results in section [ sec4 ] .the numerical methods employed in this paper are essentially the same as those used in our previous paper @xcite . the basic evolution equations , describing the compressible accretion flows of matter attracted by the protoneutron star and irradiated by neutrinos emitted from the neutrino sphere , are written as follows , @xmath5 @xmath6 @xmath7 @xmath8 @xmath9 where @xmath10 are density , velocity , internal energy , pressure , electron fraction , and gravitational potential , respectively .we denote the lagrangian derivative as @xmath11 and @xmath12 is the radius .@xmath13 is the mass of the central object .the self - gravity of matter in the accretion flow is ignored ( see @xcite for the effect ) .the parameters of @xmath14 and @xmath15 are related to the standard heating and cooling via neutrino absorptions and emissions by free nucleons ( see also @xcite ) .@xmath16 is the minor additional heating by the inelastic neutrino - helium interactions estimated by @xcite as considered in @xcite .in addition , we newly take into account the neutrino cooling by neutrino pair annihilation to @xmath17 pairs (:@xmath18 ) , nucleons - nucleons bremsstrahlung (: @xmath19 ) @xcite , and plasmon decay into @xmath20 (:@xmath21 ) @xcite .the numerical code employed in this paper is based on the modified version of the zeus-2d @xcite for the applications to the supernova studies @xcite , in which the tabulated realistic equation of state ( eos ) based on the relativistic mean field theory @xcite was implemented .furthermore , we have added the equation for electron fraction ( eq .  ( [ eq : ye_flow ] ) ) and included the neutrino coolings / heatings parametrically as the source term of the energy equations ( eq .( [ eq : energy ] ) ) , both of which are solved in the operator - splitting fashion . in the simulations ,spherical coordinates are used without imposing the equatorial symmetry .the computation domain covers the whole meridian section with 60 angular mesh points , except for a model in which we have adopted 120 angular grid points .since the latter model did not produce any significant difference from other models , we will report in the following the results obtained from the models with 60 angular mesh points .we use 300 radial mesh points to cover @xmath22 , where @xmath23 is the inner boundary and chosen to be roughly the radius of neutrino sphere .the initial conditions are provided in the same manner of @xcite as the steady state solution of @xcite . in constructing the initial conditions , we assume a fixed density @xmath24  g  @xmath25 at the inner boundary . and the initial mass accretion rates and the initial mass of the central object are set to be @xmath26  s@xmath27 and @xmath28 , respectively . to induce the non - spherical instability, we have added @xmath29 velocity perturbations to the initial state mentioned above .at the outer boundary , we adopt the fixed boundary condition consistent with the initial condition . on the other hand ,the absorbing boundary is used at the inner boundary .the temperatures of electron - type neutrinos are also constant and set to be @xmath30  mev and @xmath31  mev , which are the typical values in the post - bounce phase .the temperature of mu and tau neutrinos is chosen to be @xmath32  mev . in the standard model , the luminosity of electron - type neutrino @xmath33 and anti - neutrino @xmath34are set to be @xmath35  ergs  s@xmath27 .in addition , we examined two cases of lower luminosities of @xmath36  ergs  s@xmath27 .in all the computed models , the luminosity of mu and tau neutrinos is set to be half value of electron - type neutrinos , keeping consistency with the results obtained by the previous detailed numerical studies ( e.g. , @xcite ) .we follow the methods based on @xcite in order to compute the gravitational waveform from anisotropic neutrino emissions. we will summarize it in the following for convenience .since we assume axisymmetry , the transverse - traceless gravitational field @xmath37 is shown to have one nonvanishing component for the observer in the equatorial plane : @xmath38 where @xmath39 is the gravitational constant , @xmath40 is the speed of light , @xmath41 is the distance of the source to the observer , @xmath42 represents the direction - dependent neutrino luminosity emitted per unit of solid angle into direction of @xmath43 , and @xmath44 denotes the quantity , which depends on the angle measured from the symmetry axis ( @xmath45 ) ( see figure [ phi ] ) , @xmath46 the integration with respect to the azimuthal angle ( @xmath47 ) , albeit fairly straightforward , is pretty helpful to the later discussions , which is not published to our knowledge .in addition , from eq .( [ graph1 ] ) , one can readily see that no gravitational waves are emitted if the neutrino radiation is isotropic .we estimate @xmath42 as follows , @xmath48 where @xmath49 is the sum of contributions from the neutrino coolings , of each species ( @xmath50 , @xmath51 , @xmath52 , and @xmath53 ) computed from the source term of eq .( [ eq : energy ] ) . in the above estimation, neutrinos are assumed to be emitted radially in each angular bin .    as a guide to see the anisotropy of the neutrino emissions , we calculate the anisotropy parameter according to @xcite as follows ,@xmath54 it should be noted that the gravitational waves from neutrinos have a different feature from the ones from matter motions , in the sense that the former has _memory effect _ , which means that the gravitational amplitude jumps from zero to a nonvanishing value and it keeps the non - vanishing value even after the energy source of gravitational waves disappeared ( see @xcite for details , and @xcite for the examples of the astrophysical emitters of such gravitational waves and references therein ) . in eq .( [ tt ] ) , this nature can be directly seen as the time - integral .as for the gravitational waves , @xmath55 , of the quadrupole radiation of mass motions , we employ the standard strain formula , @xmath56 where the form of @xmath57 can be found in references ( see equation ( 12 ) in @xcite ) . in using the formula , the azimuthal gradient of the gravitational potential is set to be zero , since we neglect the self - gravity of the computational domain and treat the gravity as in eq .( [ eq : domain_g ] ) . in the following computations , we assume that the observer is located in the equatorial plane since the most of gravitational wave is radiated in the plane ( @xmath58 in eq .( [ htt ] ) ) , and that the source is assumed to be located at our galactic center ( @xmath59 ) . in order to assess the detectability of the gravitational waves obtained in this study , we employ the characteristic gravitational wave strain , @xmath60 for a given frequency @xmath61 @xcite . here@xmath62 is the energy spectra of gravitational waves defined as follows , @xmath63 we employ the fast fourier transformation technique in order to perform the fourier transformation of @xmath64 to @xmath65 .we discuss the contribution of the gravitational waves mentioned above to the cosmological background gravitational radiation . for simplicity ,we assume that all core - collapse supernovae have identical emission characteristics . according to @xcite , the density parameter , which represents the sum of energy densities radiated by a large number of independent core - collapse supernovae at each redshift ,can be written as , @xmath66 where @xmath67 is the cosmic critical density , @xmath68 is the event rate of core - collapse supernovae per comoving volume , and @xmath69 .the cosmological model enters with @xmath70^{-1}$ ] and , for a flat geometry , @xmath71^{1/2}\\\\,.\\\\ ] ] we will use the parameters @xmath72 , @xmath73 , and @xmath74 with @xmath75 . as for the core - collapse supernova rate ,we employ the parameterization by @xcite as follows , @xmath76    we take @xmath77 and the present - day rate @xmath78 , which are consistent with the super - k limits on the diffuse neutrino background @xcite . since the parameter @xmath79 is much less constrained than @xmath80 and @xmath81 ( see , for example , @xcite and references therein ) , we examine the two cases of @xmath82 in the later discussion .ccccc 6.5 & 540 & @xmath83 & 51.0 & @xmath84 + 6.0 & 1000 & @xmath85 & 59.0 & @xmath86 + 5.5 & 1000 & @xmath87 & 63.2 & @xmath88 +only in the case of @xmath89 , we can observe the continuous increase of the average shock radius , reaching the outer boundary of the computational domain ( 2000 km in radius ) in @xmath90 ms with the explosion energy of @xmath91 ergs .in other models , we terminate the simulations at about 1 sec , not seeing the increase of the shock radius . bearing in mind the evidences that the sasi - induced explosions are favorable for explaining the observed quantities of supernovae mentioned earlier , we take the exploding model of @xmath92 as a reference in the following . for later convenience ,the values of several important quantities are summarized in table [ table1 ] .      in the left panel of figure [ fig2 ] ,the total amplitudes including the contributions both from the anisotropic neutrino emissions and the matter motions , are shown . comparing the right panel , in which the amplitudes only from the matter motions are shown, we can see the neutrino - originated gravitational waves are up to two order - of - magnitudes larger than those from the matter motions during the sasi operation ( at @xmath93 ms as explained shortly ) .this reflects the small mass in the regions outside the neutrino sphere in the postbounce phase ( at most @xmath94 throughout the simulation time ) . in the following , we pay attention to the neutrino - originated gravitational wave .looking at the left panel of figure [ fig2 ] , the waveforms show the monotonic increase with time regardless of the input neutrino luminosities . to understand this trend, we look into the hydrodynamical behaviors induced by the sasi , taking the case of @xmath89 .    in figure[ color1 ] , snapshots showing the hydrodynamical features with the resulting gravitational wave amplitudes ( inserted figures ) are shown .up to @xmath1 100 ms after the onset of the simulation , no significant changes in the amplitudes and no deviations of the dynamics from spherical symmetry are found .it is noted that no gravitational waves are emitted when the motion of the regions outside is spherical , since the neutrino luminosity from the center is taken to be isotropic .after @xmath1 100 ms , the regions outside the neutrino sphere begin to oscillate with increasing average radius and wave amplitudes .the reason why the sign of the growing amplitudes is positive is as follows . from the angular dependence of @xmath95 in figure [ phi ] and eq .( [ graph1 ] ) , it can be seen that the neutrino emissions from the regions with @xmath45 smaller than @xmath96 contribute to the positive amplitude , while the negative sign comes from the regions from @xmath96 to @xmath97 .it is noted that this feature is north - south symmetric ( eq .( [ graph1 ] ) ) . due to the dominance of @xmath98 modes of the deformed shock waves during the sasi operation mentioned below, the neutrino emissions become stronger in the regions close to the symmetry axis . by these two factors ,the amplitudes are found to increase monotonically with time .this property is common to the lower luminosity models as seen from the left panel of figure [ fig2 ] . as a side - remark ,the contribution of each neutrino species to the waveform is presented in figure [ contribution ] , which also shows the dominant contribution from @xmath99 .next we discuss in more detail how the features of the waveforms are related to the growth of the sasi . for the purpose ,we decompose the fluctuations of the shock surface into the spherical harmonic components ; @xmath100 since the system is axisymmetric , only @xmath101 harmonics , that is legendre polynomials , show up .the coefficients , @xmath102 , can be calculated by the orthogonality of the legendre polynomials ; @xmath103 the position of the shock surface , @xmath104 , is estimated from the iso - entropic surface of @xmath105 .the left panel of figure [ saturation ] shows the evolution of the amplitude of each mode ( @xmath106 ) normalized by the average shock radius of @xmath107 .the dominance of the fundamental mode ( @xmath108 ) initially is simply due to the initial velocity perturbation assumed in this simulation . from @xmath109, @xmath110 begins to develop rapidly and at @xmath111 , the amplitude becomes of the same order as that of the fundamental mode , which has already been saturated by this time .this marks the beginning of the nonlinear phase .this transition from linear to nonlinear phase corresponds to the time of the rapid increase of the average shock radius as seen from the top right ( @xmath112 ms ) to middle left panel ( @xmath113 ms ) in figure [ color1 ] .simultaneously , this makes the deformation of the shock more elongated along the symmetry axis , leading to the slightly steep rise of the neutrino - originated gravitational wave afterwards for the reason mentioned above ( see the left panel of figure [ fig2 ] ) . hereit should be noted that the dominance of @xmath110 mode induces the rapid increase of the gravitational waves from the matter motions , albeit its amplitudes being very small , and is clearly understood from the quadrupole nature of gravitational radiations ( compare the left and right panels of figure [ saturation ] ) .the anisotropy parameter @xmath114 in eq.([anisop ] ) also helps us to see the relation between the anisotropy of the neutrino radiation fields and the properties of the waveforms .the time evolution of @xmath115 is presented in figure [ alpha_figure ] . in case of @xmath116, @xmath115 takes larger values after the saturation , leading to more greater wave amplitudes . in the right panel of figure [ alpha_figure ] , the ratios of the wave amplitudes of @xmath116 and @xmath117 to @xmath118 , are shown . only near after the nonlinear phase sets in ( @xmath112 ms ) , it is seen that the sasi - induced anisotropy of the neutrino emissions determines the waveforms , whose amplitudes monotonically grow with time . as a reference , we draw the lines indicating the ratio of the input neutrino luminosities .it can be also seen that the increase of the amplitudes is greater than the one estimated only by the difference of the input luminosities after the onset of the nonlinear phase .finally , we have to discuss whether the above discussions are subject to change when the numerical resolution becomes more better . in figure[ resol_figure ] , we compare the waveforms when the angular resolution is doubled in comparison with the model computed so far .as easily guessed , the difference appears after the sasi saturates ( @xmath112 ms ) and the non - linear phase sets in . in the lower resolution calculation, the anisotropy of the shock propagation tends to become more larger than the high resolution case , which results in the larger neutrino anisotropy , and thus leading to the larger wave amplitudes .the difference is not greater than @xmath119 , and thus the qualitative features discussed so far are found to be unchanged .now we move on to discuss the features of the waveforms by the spectrum analysis . from figure [ spectrum ], one can see the dominance of the neutrino - originated gravitational waves at lower frequency of @xmath0 hz .as seen in the left panel of figure [ fig2 ] , the waveform from the neutrino shows a long - time variability in comparison with the rapidly varying ( @xmath120 waveforms from matter motions , which is due to the local hydrodynamical instabilities .it is the memory effect of the neutrino - originated gravitational waves mentioned earlier that absorbs the rapid time variations of the neutrino anisotropy .    to see clearly the dominance of the neutrino - originated gravitational wave than the matter - originated one at the lower frequency , we define @xmath121 , the frequency below which the dominance occurs , and the corresponding gravitational wave amplitude , @xmath122 ( see table [ table1 ] ) . from the table, it is found that the typical frequency is smaller for the higher luminosity case . in figure[ all_spectrum ] , the gravitational wave spectra are plotted with the sensitivity curves of the laser interferometers . it can be seen that the detection of the gravitational wave at the low frequency range becomes more promising thanks to the contributions from the neutrinos .it can be seen that the gravitational waves from the neutrinos , which are dominated below @xmath0 hz , seem marginally within the detection limits of the currently running detector of the first ligo and the detection seems more feasible for the detectors in the next generation such as lcgt and the advanced ligo if a supernova occurs in our galactic center .    finally , we are in a position to discuss the contribution of the gravitational waves mentioned so far to the background gravitational radiation .we calculate @xmath123 from eqs .( [ fourier2 ] ) and ( [ los ] ) for the highest luminosity case of @xmath92 .the resulting amplitude is presented in figure [ gwb ] .for the frequency greater than @xmath0 hz , the signal is dominated by the contribution from matter motions , while at lower frequencies the signal is dominated by the one from the neutrinos ( compare figure [ all_spectrum ] ) . `` low rate '' and `` high rate''in the figure correspond to the cases of @xmath124 in eq .( [ evol ] ) , respectively , indicating the uncertainty of the core - collapse supernova rate as mentioned before .we terminated the simulation for the high luminosity case ( see table [ table1 ] ) , seeing the shock wave propagate out of the iron core .for the frequencies of @xmath125 obtained from our simulations , we extended the spectra using the zero frequency limit @xcite as implemented in the study of @xcite .`` upper bound '' denotes the most optimistic estimation . in the case of @xmath89 ,about 30 @xmath126 of the available gravitational binding energy of the neutron star of @xmath127 may be emitted during the simulation of @xmath128 ms followed here . from figure [ fig2 ] showing the monotonic increase of the neutrino - originated amplitudes , we multiply the amplitude for the case of `` high rate '' by an enhancement factor of 3 and consider it as an upper bound . then from figure[ gwb ] , we may say that the background radiation considered here could be larger for the frequency above @xmath1 1 hz than the upper limit of the background radiation generated in the inflationary epoch ( the horizontal line in the figure ) .however , the frequency range is just outside the sensitivity of the proposed detectors , such as decigo @xcite .we presented the results of numerical experiments , in which we studied how the asphericities induced by the growth of the standing accretion shock instability could produce the gravitational waveforms in the postbounce phase of core - collapse supernovae . to obtain the neutrino - driven explosions , we parametrized the neutrino fluxes emitted from the central protoneutron star and approximated the neutrino transfer by the light - bulb scheme . by doing the spectrum analysis of the waveforms , we investigated the detectability of the signals from a single core - collapse supernova and the cosmological ones by the ground - based and space - based laser interferometers , respectively .our main results can be summarized as follows .the amplitudes of the gravitational waves from the anisotropic neutrino emissions are larger up to two orders of magnitudes than the ones from the matter motions during the sasi operations .it is found that the wave amplitudes from the neutrinos show the monotonic increase with time , regardless of the neutrino luminosities from the protoneutron star .we point out that this feature can be understood by the specific nature of sasi , which makes the deformation of the shock waves of @xmath98 modes dominant , leading to the enhanced neutrino emissions in the regions close to the symmetry axis .in fact , we show that the amplitudes become larger when the growth of the sasi enters the nonlinear phase , in which the deformation of the shocks and the neutrino anisotropy become large .    \\\\2 . from the spectrum analysis of the waveforms, we find that the amplitudes from the anisotropic neutrino emissions are dominant over the ones from the matter motions at frequency @xmath129 hz .the detection of such signals from a galactic supernova may be marginal for the currently running detector of the first ligo and promising for the detectors in the next generation such as lcgt and the advanced ligo .\\\\3 . as for the background radiation, we indicate that the contribution of the gravitational signals considered here could be larger at frequency @xmath130 1 hz than the primordial gravitational wave backgrounds generated in the inflationary epoch .unfortunately , however , it is found that this frequency range is just outside of the sensitivity of the proposed detectors , such as decigo .we give a brief comparison with recently published models .the monotonic increase with time in the wave amplitudes of the neutrino - originated gravitational waves is consistent with the model s15r of @xcite , in which the operation of the sasi was seen .however , the amplitudes here are typically larger ( up to 1 order of magnitude ). this should be mainly because the neutrino luminosity here is taken to be higher than the one obtained in @xcite , which is about @xmath131 erg / s during the sasi operation .as for the total gravitational - wave energy emission , typical values of the computed models here ( @xmath132 , see table 1 ) are one order magnitude smaller than the one in @xcite .this should be owing to the excision of the protoneutron star , by which the contribution from the high frequency domain of the energy emissions are eliminated in this study .it should be noted that the larger oscillations of the protoneutron star in the postbounce phase @xcite and the resulting efficient gravitational emissions @xcite can not be treated in principle here .the simulation highlighted here is nothing but an idealized study for the physical understanding of relation between the asphericities induced by the sasi and the resulting gravitational waves . remembering the caveats about the assumptions of the artificially constructed initial condition , the fixed accretion rate , the absorbing boundary condition , and the fixed neutrino luminosity and energies , it is by no means definitive at all . especially , much better neutrino transfer is indispensable for more reliable calculations of the neutrino - originated gravitational wave , which we only considered the radial transport .one more major deficit is the axial symmetry assumed in the present two - dimensional ( 2d ) simulations . in three - dimensional environments ,the pronounced dominance of @xmath133 along the symmetry axis , which is a coordinate singularity in the 2d computations , may become weaker , owing to the additional spatial degree of freedom in the azimuthal ( @xmath134 ) direction . in the 3d case, we think that the qualitative features of the plus mode waveform computed in the 2d case here will be unchanged , but quantitatively , we expect that the amplitudes become smaller owing to the reduced anisotropy along the symmetry axis .thus the amplitudes calculated in this study could be an upper bound , in which the maximal anisotropy of the shock waves and thus neutrino emissions outside the neutrino sphere could be achieved .furthermore we think that it is interesting to investigate the properties of the cross mode gravitational waves , which are of genuine 3d origin and could be possibly produced from the transfer of the @xmath98 modes to some modes with nonvanishing @xmath135 in @xmath136 .this study is a prelude to the forthcoming 3d simulations to clarify those aspects , which will be presented elsewhere .buras , r. , rampp , m. , janka , h .-, & kifonidis , k.  2003 , physical review letters , 90 , 241101 burrows , a.  & hayes , j.  1996 , physical review letters , 76 , 352 burrows , a. , young , t. , pinto , p. , eastman , r. , & thompson , t.  a.  2000 , , 539 , 865 burrows , a. , livne , e. , dessart , l. , ott , c.  d. , & murphy , j.  2006 , , 640 , 878      buonanno , a , boulder 2002 , particle physics and cosmology , 855 - 892 .gr - qc/0303085 dimmelmeier , h. , font , j.  a. , mller , e.  2002 , , 393 , 523 epstein , r.  1978 , , 223 , 1037 fryer , c.  l. , holz , d.  e. , & hughes , s.  a.  2002 , , 565 , 430 fryer , c.  l.  2004 , , 601 , l175 fryer , c.  l. , holz , d.  e. , & hughes , s.  a.  2004 , apj , 609 , 288 flanagan ,  . . , & hughes , s.  a.  1998 , , 57 , 4566 hannestad , s. , & raffelt , g.  1998 , , 507 , 339 haxton , w.  c.  1988 , physical review letters , 60 , 1999 heger , a. , woosley , s.  e. , & spruit , h.  c.  2005 , , 626 , 350 hiramatsu , t. , kotake , k. , kudoh , h. , & taruya , a.  2005 , , 364 , 1063      kifonidis , k. , plewa , t. , scheck , l. , janka , h .- t ., mller , e.  2006 , , 453 , 661 kotake , k. , yamada , s. , & sato , k.  2003 , , 595 , 304 kotake , k. , yamada , s. , & sato , k.  2003 , , 68 , 044023 kotake , k. , yamada , s. , sato , k. , sumiyoshi , k. , ono , h. , & suzuki , h.  2004 , , 69 , 124004 kotake , k. , sato , k. , & takahashi , k.  2006 , reports of progress in physics , 69 , 971 lcgt collaboration , int . j. mod . phys .d , * 5 * , 557 , ( 1999 ) .liebendrfer , m. , mezzacappa , a. , thielemann , f .- k . , messer , o.  e. , hix , w.  r. , & bruenn , s.  w.  2001 , , 63 , 103004    malek , m. , et al .  2003 , physical review letters , 90 , 061101 mnchmeyer , r. , schaefer , g. , mueller , e. , & kates , r.  e.  1991 , , 246 , 417 meakin , c.  a. , & arnett , d.  2006 , , 637 , l53 mller , e. , rampp , m. , buras , r. , janka , h .-t . , & shoemaker , d.  h.  2004 , , 603 , 221 mueller , e. , & janka , h .-1997 , , 317 , 140 new , k.  c.  b.  2003 , living reviews in relativity , 6 , 2 obergaulinger , m. , aloy , m.  a. , mller , e.  2006 , , 450 , 1107 ott , c.  d. , burrows , a. , livne , e. , & walder , r.  2004 , , 600 , 834\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.   Duis autem vel eum iriure dolor in hendrerit in vulputate velit esse molestie consequat, vel illum dolore eu feugiat nulla facilisis at vero eros et accumsan et iusto odio dignissim qui blandit praesent luptatum zzril delenit augue duis dolore te feugait nulla facilisi. Lorem ipsum dolor sit amet.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Klasse 0: ['0.10%', '97.30%', '2.55%', '0.04%']\n",
      "Vorhergesagte Klasse: ['news']\n"
     ]
    }
   ],
   "source": [
    "new_text = data_test[\"text\"][432]\n",
    "new_text = t\n",
    "new_text_features = vectorizer.transform([new_text])\n",
    "probabilities = model.predict_proba(new_text_features)\n",
    "predicted_class = model.predict(new_text_features)\n",
    "\n",
    "# Wahrscheinlichkeiten und vorhergesagte Klasse ausgeben\n",
    "for i, probs in enumerate(probabilities):\n",
    "    class_probabilities = [\"{:.2f}%\".format(prob * 100) for prob in probs]\n",
    "    print(\"Klasse {}: {}\".format(i, class_probabilities))\n",
    "print(\"Vorhergesagte Klasse:\", predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Scientific'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test[\"classification\"][432]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_csv(\"../../data/data_with_features/data_train_with_features.csv\").drop([\"Unnamed: 0\"], axis=1)\n",
    "data_test = pd.read_csv(\"../../data/data_with_features/data_test_with_features.csv\").drop([\"Unnamed: 0\"], axis=1)\n",
    "\n",
    "data_train = data_train.sample(frac=1).reset_index(drop=True)\n",
    "data_test = data_test.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "features = vectorizer.fit_transform(data_train[\"text\"])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, data_train[\"classification\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mSequential([\n\u001b[0;32m      2\u001b[0m     layers\u001b[39m.\u001b[39mDense(\u001b[39m64\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m, input_shape\u001b[39m=\u001b[39m(features\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m],)),\n\u001b[0;32m      3\u001b[0m     layers\u001b[39m.\u001b[39mDense(\u001b[39m64\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[0;32m      4\u001b[0m     layers\u001b[39m.\u001b[39mDense(\u001b[39m4\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m ])\n\u001b[0;32m      7\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m               loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msparse_categorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      9\u001b[0m               metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(features.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_int = {label: i for i, label in enumerate(np.unique(data_train[\"classification\"]))}\n",
    "y_train = np.array([label_to_int[label] for label in y_train])\n",
    "y_test = np.array([label_to_int[label] for label in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "100/100 [==============================] - 5s 43ms/step - loss: 0.7395 - accuracy: 0.7937\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 0.0427 - accuracy: 0.9994\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 4s 42ms/step - loss: 0.0049 - accuracy: 0.9994\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 4s 41ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 6.8192e-04 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 4s 35ms/step - loss: 3.5111e-04 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 4s 44ms/step - loss: 2.1026e-04 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 4s 42ms/step - loss: 1.3837e-04 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 4s 40ms/step - loss: 9.6169e-05 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 4s 39ms/step - loss: 7.0114e-05 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23b90075e50>"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train.toarray(), y_train, epochs=10, batch_size=16, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13/13 [==============================] - 1s 36ms/step - loss: 0.0166 - accuracy: 0.9975\n",
      "Test Loss: 0.016573261469602585\n",
      "Test Accuracy: 0.9975000023841858\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test.toarray(), y_test, verbose=1)\n",
    "print('Test Loss:', loss)\n",
    "print('Test Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"\"\"I've been thinking e-everyday\n",
    "I've been thinking 'bout what you say\n",
    "But words just get in the way, yeah\n",
    "And I stress 'cause I don't wanna make a mess\n",
    "When it comes to you\n",
    "I'll give my best, yeah yeah\n",
    "I'm trying to impress\n",
    "\n",
    "Oh oh oh oh oh and everyday\n",
    "Is like I see you for the first time\n",
    "Oh oh oh oh oh and over and over I try\n",
    "But words won't come my way\n",
    "\n",
    "Baby no oh oh oh oh\n",
    "This ain't just a love song\n",
    "Another love song\n",
    "Just random words\n",
    "On the same sad chords\n",
    "It's true, my song is all about you\n",
    "\n",
    "All my friends say I try too much\n",
    "They say it's just a little crush\n",
    "But you took over my heart\n",
    "And I stress 'cause I always\n",
    "Tend to make a mess\n",
    "Even though I try to give my best\n",
    "Yeah yeah, I'm trying to impress\n",
    "Yeah yeah! \n",
    "\n",
    "Oh oh oh oh oh and everyday\n",
    "Is like I see you for the first time\n",
    "Oh oh oh oh oh and over and over I try\n",
    "But words won't come my way\n",
    "\n",
    "Baby no oh oh oh oh\n",
    "This ain't just a love song\n",
    "Another love song\n",
    "Just random words\n",
    "On the same sad chords\n",
    "It's true, my song is all about\n",
    "Yo-o-o-ou\n",
    "Yo-o-o-ou\n",
    "Just random words\n",
    "On the same sad chords\n",
    "It's true, my song is all about\n",
    "\n",
    "You, the one that I can't escape\n",
    "The one that can take my breath\n",
    "The only one that keeps me coming back\n",
    "And 'cause my words fall short\n",
    "I'm singing you this song\n",
    "This song\n",
    "\n",
    "Baby no oh oh oh oh\n",
    "This ain't just a love song\n",
    "Another love song\n",
    "Just random words\n",
    "On the same sad chords\n",
    "It's true, this song is all about youuuu\n",
    "\n",
    "Baby no oh oh oh oh\n",
    "This ain't just a love song\n",
    "Another love song\n",
    "Just random words\n",
    "On the same sad chords\n",
    "It's true, my song is all about youuuu\n",
    "Yo-o-o-ou\n",
    "Yo-o-o-ou\n",
    "Just random words\n",
    "On the same sad chords\n",
    "It's true, this song is all about you\"\"\"\n",
    "\n",
    "lorem_ipsum = \"Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.   Duis autem vel eum iriure dolor in hendrerit in vulputate velit esse molestie consequat, vel illum dolore eu feugiat nulla facilisis at vero eros et accumsan et iusto odio dignissim qui blandit praesent luptatum zzril delenit augue duis dolore te feugait nulla facilisi. Lorem ipsum dolor sit amet.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 13ms/step\n",
      "Vorhergesagte Klasse: Scientific, Wahrscheinlichkeit: 0.9995403289794922\n"
     ]
    }
   ],
   "source": [
    "new_text = data_test[\"text\"][0]\n",
    "new_text = t\n",
    "new_text_features = vectorizer.transform([new_text])\n",
    "predictions = model.predict(new_text_features.toarray())\n",
    "predicted_class = np.argmax(predictions, axis=1)\n",
    "predicted_probability = np.max(predictions, axis=1)\n",
    "\n",
    "int_to_label = {i: label for label, i in label_to_int.items()}\n",
    "\n",
    "predicted_labels = [int_to_label[prediction] for prediction in predicted_class]\n",
    "for label, probability in zip(predicted_labels, predicted_probability):\n",
    "    print(f\"Vorhergesagte Klasse: {label}, Wahrscheinlichkeit: {probability}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'news'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test[\"classification\"][101]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Scientific': 0, 'news': 1, 'reviews': 2, 'story': 3}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'story'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test[\"classification\"][78]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"collective excitations of superfluid helium confined in various porous media have been studied by neutron scattering since early 90 s , and by now a wealth of information about helium in aerogel , vycor and geltech has been collected @xcite-@xcite .aerogel is an open gel structure formed by silica strands ( sio@xmath0 ) .typical pore sizes range from few   to few hundred   , without any characteristic pore size .vycor is a porous glass , where pores form channels of about 70    diameter .geltech resembles aerogel , except that the nominal pore size is 25    @xcite .liquid @xmath1he is adsorbed in these matrices in the form of atomic layers , the first layer is expected to be solid ; on a more strongly binding substrate , such as graphite , one expects two solid layers .energies and lifetimes of phonon  roton excitations for confined @xmath1he are nearly equal to their bulk superfluid @xmath1he values @xcite , but differences appear at partial fillings .the appearance of ripplons is tied to the existence of a free liquid surface ; neutron scattering experiments show clearly their presence in adsorbed films @xcite with few layers of helium .an exclusive feature of adsorbed films is the appearance of `` layer modes '' .the existence of such excitations has been proposed in the seventies @xcite from theoretical calculations of the excitations of two  dimensional 4 and comparison with specific heat data .direct experimental evidence for the existence of collective excitations below the roton minimum has first been presented by lauter and collaborators @xcite , identification of these excitations with longitudinally polarized phonons that propagate in the liquid layer adjacent to the substrate has been provided by microscopic calculations of the excitations of films @xcite .    in an experimental situation , the topology gives rise to non  uniform filling of the pores .but from the theoretical point of view different materials are characterized solely by their substrate potentials , because as long as the wavelength of the excitation in concern is much shorter than any porosity length - scale , the topology of the confining matrix is immaterial .we therefore examine the energetics of the layer  roton as a function of the substrate  potential strength which determines , in turn , the areal density in the first liquid layer . for that purpose ,we have carried out a number of calculations of the structure of helium films as a function of potential strength . the microscopic theory behind these calculationsis described in ref .our model assumes the usual 3 - 9 potential @xmath2{1\\\\over z^9 } - { c_3\\\\over z^3}\\\\ , ; \\\\label{eq : usub}\\\\ ] ] we have varied the potential strength @xmath3 from 8  k to 50  k and the range @xmath4 from 1000  k  @xmath5 to 2500  k@xmath5 . in all cases , we have considered rather thick films of an areal density of 0.45  @xmath6 .[ fig : profiles ] shows density profiles for these potential strengths close to the substrate ; the density profiles are practically independent of the potential range @xmath4 .     of the substrate potential .the substrate is at @xmath7 ., scaledwidth=48.0% ]to introduce excitations to the system one applies a small , time dependent perturbation that momentarily drives the quantum liquid out of its ground state . generalizing the feynman cohen wave function @xcite , we write the excited state in the form @xmath8^{1/2}}\\\\,,\\\\ ] ] where @xmath9 is the exact or an optimized variational ground state , and the excitation operator is @xmath10 the time dependent excitation functions @xmath11 are determined by an action principle @xmath12 where @xmath13 is the weak external potential driving the excitations .the truncation of the sequence of fluctuating correlations @xmath14 in eq .( [ eq : deltau ] ) defines the level of approximation in which we treat the excitations .one recovers the feynman theory of excitations @xcite for non  uniform systems @xcite by setting @xmath15 for @xmath16 .the two  body term @xmath17 describes the time  dependence of the short  ranged correlations .it is plausible that this term is relevant when the wavelength of an excitation becomes comparable to the interparticle distance .consequently , the excitation spectrum can be quite well understood @xcite by retaining only the time dependent one and two  body terms in the excitation operator ( [ eq : deltau ] ) .the simplest non  trivial implementation of the theory leads to a density  density response function of the form @xcite@xmath18          \\\\phi^{(t)}({\\\\bf r}')\\\\sqrt{\\\\rho({\\\\bf r}')}\\\\nonumber \\\\label{eq : cbfresponse}\\\\end{aligned}\\\\ ] ] where the @xmath19 are feynman excitation functions , and @xmath20\\\\delta_{st } + \\\\sigma_{st}(\\\\omega)\\\\right]^{-1 } \\\\label{eq : gdef}\\\\ ] ] the phonon propagator .the fluctuating pair correlations give rise to the dynamic self energy correction @xcite , @xmath21 here , the summation is over the feynman states @xmath22 ; they form a partly discrete , partly continuous set due to the inhomogeneity of the liquid . the expression for the three  phonon coupling amplitudes @xmath23 can be found in ref .  .this self energy renormalizes the feynman `` phonon '' energies @xmath24 , and adds a finite lifetime to states that can decay .the form of the self energy given in eq .( [ eq : selfen ] ) is the generalization of the correlated basis functions ( cbf )  @xcite theory to inhomogeneous systems . as a final refinement of the theory , we scale the feynman energies @xmath25 appearing in the energy denominator of the self energy given in eq .( [ eq : selfen ] ) such that the roton minimum of the spectrum used in the energy denominator of eq .( [ eq : selfen ] ) agrees roughly with the roton minimum predicted by the calculated @xmath26 .this is a computationally simple way of adding the self energy correction to the excitation energies in the denominator of eq .( [ eq : selfen ] ) .we shall use this approximation for the numerical parts of this paper .layer phonons are identified by a transition density that is localized in the first liquid layer of the system .they appear in the dynamic structure function @xmath27 as a peak below the roton minimum .a grayscale map of a typical dynamic structure function is shown in fig .[ fig : skwplot ] , we have for clarity chosen a momentum transfer parallel to the substrate ; neutron scattering at other angles would broaden the roton minima @xcite .the figure shows in fact one bulk and two layer  roton minima , but the higher one , which corresponds to an excitation propagating in the second liquid layer , has an energy too close to the bulk roton to be experimentally distinguishable .the transition densities corresponding to the three pronounced excitations at @xmath28@xmath29 are depicted in fig .[ fig : trans ] .clearly , the two `` layer  modes '' are actually located in the two first layers adjacent to the substrate whereas the `` bulk '' mode is spread throughout the system .however , the figure also shows that the notion that the wave propagates in the first or the second layer is also not quite accurate : the lowest mode also has some overlap with the second layer , but especially the second mode spreads over both layers .     for a 4 film for the potential strength @xmath30k . the two layer rotons , the bulk roton , and the ripplon are indicated by arrows.,scaledwidth=48.0% ]    we have carried out two independent calculations of the two  dimensional roton excitation : first , we calculated the roton energy as a function of the density for a rigorously two dimensional liquid. we can assess the accuracy of our predictions with the shadow  wave  function calculation of @xmath27 of ref ., who obtained a roton energy of @xmath31k at the equilibrium density of @xmath32@xmath6 .second , we have calculated the dynamic structure function @xmath33 in the relevant momentum region for the above family of substrate potentials .the results are compiled in fig .[ fig : rotonenergy ] where we also collect several experimental values .    .also shown is the energy of a two  dimensional roton obtained with shadow wave functions @xcite at the density of 0.0421@xmath6 .the short  dashed horizontal lines show experimental values of the roton energy on aerogel @xcite ( 7.39 and 7.14  k ) , vycor@xcite ( 7.23 k ) , and geltech@xcite ( 6.71 k ) , their energies are marked on the right margin.,scaledwidth=48.0% ]    although exactly the same method has been used for the computation of the purely two - dimensional system and for the films , the results are quite different .we have obtained for the film calculation an effective layer density by integrating the three - dimensional densities shown in fig .[ fig : profiles ] to the first minimum .this is evidently not very well defined for the weakly bound systems , but it is not legitimate either for the case of strong binding where the first layer is well defined .in fact , the integrated density for the strongest substrate is 0.08  @xmath6 , which is well beyond the solidification density of the purely two  dimensional system .evidently , the zero  point motion in @xmath34 direction can effectively suppress the phase transition .we make therefore three conclusions : ( i ) the position of the layer roton minimum is indeed a sensitive measure for the strength of the substrate potential , ( ii ) purely two  dimensional models are manifestly inadequate for their understanding , and , hence , ( iii ) purely two  dimensional models are also questionable for interpreting thermodynamic data of adsorbed films .with one exception @xcite , the bulk roton energy in porous media have been reported to be practically identical to that in the bulk liquid , ref . reports a slight increase of the roton energy in aerogel at partial filling .a roton energy above the one of the bulk liquid can be explained by assuming that the density of the helium liquid in the medium is below that of the bulk liquid .this can , in turn , be qualitatively explained by the cost in energy to form a surface .    to be quantitative ,we have performed calculations of the energetics and structure of 4 in a gap between attractive silica walls @xcite and obtained the energy of the bulk roton ( c.f .[ fig : skwplot ] ) as a function of filling . fig .[ fig:3d - roton ] shows , as a typical example , the roton energetics in in a gap of 25    width .the independent parameter is the areal density @xmath35 , the corresponding three dimensional density was obtained by averaging the density profile over the full volume .it is seen that the equilibrium density is well below the bulk value . in other words ,the roton energy in a confined liquid should correspond to the one of a liquid that would , without confinement , have a negative pressure .the energy increase of the roton minimum found in this model is about 0.5 k , which is consistent with the experiments of ref . .to verify this interpretation of the data , it would be very useful to have comparable measurements for porous media with a more uniform distribution of pore sizes .in particular , comparably small pores should allow to densities that are even below the bulk spinodal density @xcite , thus facilitating experiments on 4 in density areas that were up to now inaccessible .\""
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test[\"text\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"\"\"What are graph databases and how can quality\n",
    "be verified for their data?\n",
    "Abstract\n",
    "Loosing customers, missing opportunities and coming to wrong decisions are symptoms of\n",
    "the lack of accurate data in many enterprises. Every company can achieve big advantages\n",
    "by processing data correctly and efficiently, and in order to work with a huge amount of\n",
    "data, the data has to be trustworthy and of high quality. Boehringer Ingelheim is a global,\n",
    "family-owned researching pharmaceutical company that focuses on researching, developing,\n",
    "producing and selling prescription drugs for humans and animals. Pharmaceutical companies like Boehringer Ingelheim invest billions of euros and years of work into researching\n",
    "and developing; and sometimes without even finding a satisfying result. The development of\n",
    "a drug can cost 1.0 to 1.6 billion US-Dollars and can last for over 13 years; only for one age\n",
    "class. Because of that, data quality and trustworthy data in general play an important role for\n",
    "the company.\n",
    "This project paper simply explains how data quality can be verified using automatic generation of test cases from graph databases and how a framework could look like that ensures high\n",
    "quality data. It defines necessary vocabulary and explains required concepts, languages and\n",
    "functionalities like RDF, OWL, SHACL, SPARQL, the Semantic Web, graph databases (like\n",
    "knowledge graphs), the Astrea-Tool and the RDFUnit Testing Suite. The final result of this\n",
    "project paper is a software concept, that links the Astrea-Tool and RDFUnit Testing Suite to\n",
    "enable automatic generation of data shapes, as well as test cases for those data shapes. This\n",
    "final software concept only needs data stored in RDF triples and the corresponding ontologies to automatically create an inspection report, which clearly depicts errors or irregularities\n",
    "in any dataset.\n",
    "III\n",
    "1 Introduction 1\n",
    "1 Introduction\n",
    "1.1 The Importance of Data Quality\n",
    "“Accurate data is a fundamental requirement of good information systems” (Olson, 2008, p.\n",
    "6) and the lack of it can have negative consequences for companies, like loosing customers,\n",
    "missing opportunities and coming to wrong decisions. In order to ensure data quality a company must put effort in form of time and money into data quality assurance programs. Nowadays, the worlds biggest internet companies and web services Microsoft, Apple, Facebook,\n",
    "Google and Amazon use the data of their users to generate huge profits. Every company can\n",
    "achieve big advantages by processing data correctly and efficiently. But in order to work\n",
    "with huge datasets, it has to be trustworthy and of high quality. Data quality experts estimate\n",
    "that some businesses, including governmental and educational organizations, lose 15-25% of\n",
    "their profit due to working with poor-quality data (cf. Olson, 2008, p. 9). Data quality issues\n",
    "lead to a huge waste of time, energy and money for people, companies and their staff. The\n",
    "problem is, that those issues seem to be accepted or ignored nowadays. They have become\n",
    "invisible since many people declare those costs as normal and routine (cf. Olson, 2008, p.\n",
    "14). This shows, that a rethinking is needed to improve the data quality verification processes and that it needs big investments to overcome the current standards. Nevertheless, the\n",
    "outcome could be very substantial for everyone.\n",
    "Boehringer Ingelheim and Data Quality in the Pharmaceutical Industry\n",
    "Boehringer Ingelheim Pharma GmbH & Co.KG (BI) is a global, family-owned researching\n",
    "pharmaceutical company whose headquarter is located in Ingelheim am Rhein, Germany. BI\n",
    "focuses on researching, developing, producing and selling prescription drugs for humans and\n",
    "animals. BI has developed several so-called “Blockbusters” like e.g. “Jardiance” to treat type\n",
    "2 diabetes or “Spiriva” for chronic obstructive pulmonary diseases. Those “Blockbusters”\n",
    "pulled in sales amounting to billions. The year 2020 was, despite Covid-19, a good year\n",
    "for BI. The generated turnover was 19.57 billion euros; 3% more than in the preceding\n",
    "year. The goal of Boehringer Ingelheim is to become the number one in animal health\n",
    "and biopharmaceutical contract manufacture. (cf. Boehringer Ingelheim Pharma GmbH &\n",
    "Co. KG, 2020). An ambitious goal, since medicine, pharmacy and health are critical and\n",
    "important areas of research, that can be very expensive, long-lasting and risky. Companies\n",
    "like Boehringer Ingelheim invest billions of euros and years of work into researching and\n",
    "developing; and sometimes without even finding a satisfying result. The goal of everyone\n",
    "working for BI is to improve the steps behind drug development. This includes searching\n",
    "for a possible active ingredient, synthesizing it, testing it on animals and people (includes\n",
    "1.2 Objective of this Paper and Project 2\n",
    "finding volunteers), adjusting the drug, finding the correct dosage, waiting for the clinical\n",
    "studies to finish, gaining approval by the authorities, patenting the drug, manufacturing and\n",
    "selling it. This process can last for over 13 years; only for one age class. On average, 1.0\n",
    "to 1.6 billion US-Dollars have to be invested into the development of one drug. (cf. Die\n",
    "forschenden Pharma-Unternehmen, 2018). Ensuring data quality at Boehringer Ingelheim\n",
    "or other pharmaceutical companies can help saving time and money in every area that was\n",
    "just mentioned. Wrong data leads to wrong information, which leads to wrong knowledge\n",
    "and to wrong conclusions, which lead to wrong results in research, to wrong decisions and\n",
    "a lot of wasted time, money and effort. And in addition, especially in pharmacy, wrong data\n",
    "can lead to dangerous situations for humans.\n",
    "1.2 Objective of this Paper and Project\n",
    "The objective of this project is to automate generation and execution of data quality checkings. How could a framework look like that checks high quality data and helps bringing\n",
    "buried data to the fore for BI or other pharmaceutical companies? A framework that allows\n",
    "evaluating the quality of already existing or completely new datasets? This paper documents\n",
    "the approach to this project. It defines necessary vocabulary and explains required concepts,\n",
    "languages and functionalities. It contains reviews of the singular software tools and an explanation of how they could be connected.\n",
    "1.3 Proposed Approach to the Project\n",
    "In order to understand the necessity of the final software concept and how it works, it is\n",
    "inevitable to clearly define technical terms to fully understand the basics that lie behind\n",
    "Knowledge Graphs (KGs) or the semantic web, and to get to know standardized formats\n",
    "and languages supporting those basics. This is why chapter 2 will focus on explaining the\n",
    "background information and context of the project. Chapter 3 contains the practical content,\n",
    "like the review of the Astrea-Tool and RDFUnit Testing Suite. The last chapter is about\n",
    "the final result and gives a glimpse of how this project will affect the future at Boehringer\n",
    "Ingelheim.\n",
    "2 Foundations 3\n",
    "2 Foundations\n",
    "2.1 Definition of Data Quality\n",
    "The six following dimensions are very commonly used to describe and rate data quality.\n",
    "(Sources: (Olson, 2008, p. 24f.) & (Herzog et al., 2007, p. 8f.) & (Fleckenstein and\n",
    "Fellows, 2018, p. 103f.))\n",
    "1. Accuracy: Is the information correct?\n",
    "2. Timeliness: Is the information up-to-date?\n",
    "3. Relevance: Does the information help to answer the important (relevant) questions?\n",
    "4. Completeness: Is information missing?\n",
    "5. Credibility: Are there multiple versions of the same information?\n",
    "6. Validity: Does the information conform to the definition?\n",
    "Additional often used data quality dimensions are Currency, Consistency, Flexibility, Precision, Format, Interpretability, Content, Efficiency, Importance, Sufficiency, Usableness,\n",
    "Usefulness, Clarity, Comparability, Conciseness, Freedom of bias, Informativeness, level of\n",
    "detail, Quantitativeness, Scope, Understandability. (cf. Haug et al., 2011, p. 4f.)\n",
    "2.2 Definition of Knowledge\n",
    "It is not easy to define the term “knowledge”, especially in the English language. “Knowledge” has basically two different meanings: On the one hand “having knowledge of something” and on the other hand “having knowledge about something”. (cf. Machlup, 1981, p.\n",
    "27ff.) In other languages, like German, there are two words to describe those two cases of\n",
    "knowledge (“wissen” & “kennen”). Due to the lack of such a distinction in English, it is\n",
    "important to correctly define what kind of knowledge is meant when someone talks about\n",
    "it, so that everyone has the same understanding of the term. The knowledge portrayed by\n",
    "Knowledge Graphs (or other graph databases) does not represent what is really “known”\n",
    "about something in the epistemological sense. It is not about having a skill or understanding\n",
    "something, but instead it represents the information about different entities, things or objects\n",
    "and how they relate to each other in form of data and metadata. By doing this, Knowledge\n",
    "Graphs become the perfect tool to transform implicit knowledge buried in huge datasets (cf.\n",
    "Figure 2.1) into explicit knowledge. (cf. Blumauer and Nagy, 2020, p. 91)\n",
    "Explicit Knowledge is knowledge which can be encoded with literals, strings, mathematical equations or more and is knowledge that can be stored or be processed. Explicit knowl-\n",
    "2.3 Basics about the Semantic Web 4\n",
    "edge can be understood in an objective way and as something that exists physically in a kind\n",
    "of static collection of statements, facts or ideas. (cf. Schilcher, 2006, p. 19)\n",
    "Implicit Knowledge is knowledge “that can only be understood by the author himself.”\n",
    "(Blumauer and Nagy, 2020, p. 35). It is knowledge that cannot really get noted down because\n",
    "it is based on someones personal experiences, memories and feelings. Implicit Knowledge\n",
    "is close to practical skills and everyone could interpret it in another way (like a mindmap).\n",
    "People know very much, but cannot tell everything they know because they are not aware of\n",
    "some aspects of their knowledge and it is hard to separate the knowledge used in daily life\n",
    "from emotions, feelings and instinct.\n",
    "Figure 2.1: A huge part of the knowledge someone has, no matter if private person or\n",
    "company, is implicit. (Blumauer and Nagy, 2020, p. 91)\n",
    "An Example: If someone wants to explain how to make a cake, the cook would write down\n",
    "the used ingredients and when and how long they put them into the oven. This is called a\n",
    "recipe and is the perfect example for explicit knowledge. The knowledge by the cook is noted\n",
    "down so that everyone can easily copy it.\n",
    "But the cook bakes very often and collected a lot of practical baking skills. They know when\n",
    "the dough is perfectly finished, know little tricks to improve the final result and know how\n",
    "to react to unforeseen situations because of their experience. The cook has a lot of implicit\n",
    "knowledge which they sometimes use without even realizing and therefore it cannot be noted\n",
    "down in a recipe. That is why food in the restaurant or cakes in the bakery often taste better\n",
    "than the self-made ones at home.\n",
    "2.3 Basics about the Semantic Web\n",
    "The Internet, or World Wide Web (WWW), as it is known today was founded in 1989 by Tim\n",
    "Berners-Lee as a project at the European Organization for Nuclear Research facility, also\n",
    "known as CERN. Berners-Lee’s goal was to create a “wide-area hypermedia information\n",
    "retrieval initiative aiming to give universal access to a large universe of documents.” (cf.\n",
    "2.3 Basics about the Semantic Web 5\n",
    "Frysyk, 1994). The basic WWW is about connecting documents that contain data in the\n",
    "form of text or pictures. But already in 2001 Tim Berners-Lee talked about the next big step,\n",
    "namely the “Semantic Web”, which is more about connecting the data itself than connecting\n",
    "documents. Due to the World Wide Web Consortium (W3C), an organization founded by\n",
    "Tim Berners-Lee to create standardized web formats, the Semantic Web is about two things:\n",
    "(cf. W3C et al., 2013)\n",
    "1. Common formats for data integration and combination from diverse sources.\n",
    "2. Recording how data relates to real world objects.\n",
    "Metadata is the key in the mentioned idea of the Semantic Web. “Metadata is ’data about\n",
    "data’.” (Riley, 2017, p. 1). It provides more information about the actual data, allowing us\n",
    "to derive knowledge out of it. It is not only used to describe the appearance of the data, but\n",
    "also to describe and denote its meaning and relation to other data. For Example: A weather\n",
    "station stores data about temperature, air moisture and wind strength; but it stores not only\n",
    "the raw values, but also timestamps and coordinates. This is metadata and it can be found\n",
    "everywhere (Riley, 2017, p. 1). Using this additional metadata, the weather stations can\n",
    "create statistics for specific time periods or locations, which they can use to trace diverse\n",
    "weather developments. Other Systems, when they can rely on metadata, are able to understand words and phrases that are equivalent. (Davies et al., 2007, 3) A common example:\n",
    "When searching Google for „Jaguar“ in the context of motor industry, the Google Search\n",
    "Engine „knows“ the user is not searching for the animal because in this context the semantic\n",
    "relations to the animal are very poor (Davies et al., 2007, 3). There are different types of\n",
    "metadata that should give a good overview and understanding of what metadata really is and\n",
    "why it can be found basically everywhere (cf. Riley, 2017, 6):\n",
    "• Descriptive metadata: Understand what the data is about or what the data means.\n",
    "• Administrative metadata:\n",
    "– Technical metadata: How to decode the data or how data has to be processed.\n",
    "– Preservation metadata: How to store files for a longer time.\n",
    "– Metadata about rights: Additional information about the intellectual property\n",
    "rights.\n",
    "• Structural Metadata: Relationships between data.\n",
    "• Markup languages: Integration of metadata for additional structural or semantic features (e.g. XML).\n",
    "This already shows that metadata is the first step to ensure high quality data. It provides\n",
    "information about the different data quality dimensions from chapter 2.1, e.g. about time\n",
    "2.3 Basics about the Semantic Web 6\n",
    "(Timeliness), format (Validity) and planned usage/description (Relevance) of the data. More\n",
    "metadata helps evaluating the quality of data.\n",
    "Standardization is also an important factor in semantic web development. In the last\n",
    "decades Tim Berners-Lee and the W3C published several standards and recommendations\n",
    "to facilitate the development of the Semantic Web and Semantic Web applications. In addition, those standardization simplifies the compatibility of different applications. The most\n",
    "important standards, that will also play a part in this paper, are (Blumauer and Nagy, 2020,\n",
    "25):\n",
    "1. The Resource Description Framework (RDF) as a recommended approach to describe\n",
    "and store metadata.\n",
    "2. The Resource Description Framework Schema (RDFS) enabled the representation of\n",
    "the data in the WWW.\n",
    "3. The Web Ontology Language (OWL) was developed to allow users defining and instantiating web ontologies.\n",
    "4. The Simple Protocol and RDF Query Language (SPARQL) was developed to retrieve\n",
    "and manipulate data stored in RDF.\n",
    "5. The Shapes Constraint Language (SHACL) is used to validate graph-based data against\n",
    "a set of conditions.\n",
    "Figure 2.2: The Semantic Web is structured in different layers of formalisms and recommendations. (cf. Kingsley Uyi Idehen, 13.07.2017)\n",
    "2.4 Structure of Knowledge Graphs 7\n",
    "Figure 2.2 shows the structure of the Semantic Web and the interaction of the mentioned\n",
    "recommendations. The next chapters will introduce everything needed to follow this project.\n",
    "2.4 Structure of Knowledge Graphs\n",
    "A knowledge graph is a database that integrates data using structures known from the geometric graph theory. “The knowledge graph represents a collection of interlinked descriptions of entities.” (cf. Ontotext.com, 2018). Just like graphs in mathematics, knowledge\n",
    "graphs consist of nodes and directed edges between those nodes. Those edges connect several nodes with specific properties. Imagine a relational database in form of a table (cf.\n",
    "Figure 2.3): In a graph database, the first node conforms to the row, the property to the\n",
    "column and the second node to the value in this row and column. (like in Figure 2.4).\n",
    "Figure 2.3: What a typical relational database looks like. (Created with Visual Paradigm\n",
    "Online)\n",
    "Figure 2.4: How the data of Figure 2.3 “looks like” when stored in a graphical database.\n",
    "(Created with Visual Paradigm Online)\n",
    "2.4.1 Resource Description Framework\n",
    "The Resource Description Framework (RDF) is a standard model for data interchange in\n",
    "the semantic web. Data in RDF is stored in so-called “triples”, which consist of a subject, a\n",
    "predicate and an object. When several triples are connected to each other, this is called “RDF\n",
    "graph” (cf. RDF Working Group (2014)). The advantage of RDF and the storing of data in\n",
    "those triples is, that it is easy to read by machines as well as humans. RDF statements can\n",
    "2.4 Structure of Knowledge Graphs 8\n",
    "be visualized using directed graphs (cf. Figure 2.5) (cf. Blumauer and Nagy, 2020, p. 96ff.).\n",
    "Subjects and objects are represented as nodes. The predicate is a directed edge connecting\n",
    "two nodes. There are three different kinds of nodes:\n",
    "1. Unique Resource Identifier (URI) or International Resource Identifier (IRI)\n",
    "2. literal nodes\n",
    "3. blank nodes\n",
    "Figure 2.5: RDF triple consisting of a subject, a predicate and an object. (Created with\n",
    "Visual Paradigm Online)\n",
    "URIs are “Unique Resource Identifiers” that are used to clearly identify a thing or object\n",
    "in the real world. A URI points to a namespace or vocabulary in which the subject, predicate\n",
    "or object of the data is unambiguously defined. For example the predicate “hasTitle”: Does\n",
    "it mean the title of a book or the jobtitle in a company?. Namespaces/Vocabularies were\n",
    "created to avoid those complications and to differ between words with multiple possible\n",
    "meanings. IRIs are basically URIs with a wider range of possible characters (e.g Chinese\n",
    "symbols) that can be used to address an ontology. This is what a typical URI looks like:\n",
    "\"http://xmlns.com/foaf/0.1/\". Its structure is very similar to URLs used to address documents\n",
    "on web servers, because URLs are just special URIs. (cf. DuCharme, 2013, Chapter 2:\n",
    "URLs, URIs, IRIs and Namespaces)\n",
    "Literal nodes instead denote a literal value like strings or other data types. In Figure A.1\n",
    "the unique subject “Person:1337” has three objects that are literal values (cf. Blumauer and\n",
    "Nagy, 2020, p. 96). The “Person:” in this example is a so-called “prefix” that points to a\n",
    "specific namespace in which the subject 1337 is uniquely defined and described.\n",
    "Blank nodes are used to group data. Figure A.2 shows the address of a person stored\n",
    "using triples without blank nodes (cf. DuCharme, 2013, Chapter 2: URLs, URIs, IRIs and\n",
    "Namespaces). Figure A.3 shows how a blank node is used to group the postal address of the\n",
    "person. This helps arranging the graph and improves readability.\n",
    "The Serialization of RDF triples in an RDF graph data base is necessary to make the data\n",
    "machine-readable. There are several different formats that can be used to serialize the triples.\n",
    "Some examples are: Turtle, JSON-LD, N3 or RDF/XML (cf. Blumauer and Nagy, 2020,\n",
    "2.4 Structure of Knowledge Graphs 9\n",
    "p. 97f.). Every format has its advantages and disadvantages, but since Turtle is specially\n",
    "designed for human-understanding, Turtle will be the format used for RDF statements in\n",
    "this paper. Listing 2.1 shows the serialization of Figure A.1. In the first two lines of the\n",
    "code the prefixes “eo” and “foaf” get defined. The URIs at the end of the line point to an\n",
    "vocabulary/namespace where several objects and relations are unambiguously defined. The\n",
    "storing of data starts in line 3. Whatever or whoever is uniquely defined as “Person1337” in\n",
    "the example ontology, that is called “eo”, has the relation \"has_firstName\", which is clearly\n",
    "defined by the “Friend-of-a-Friend” (foaf) vocabulary, pointing to a literal value of type\n",
    "string with the content of “James”. Same is with his lastname and his age. In addition, what\n",
    "is not part of Figure A.1, Subject “Person1337” knows the object that is clearly defined as\n",
    "“Person1338” in the ontology. A dot always signals the end of a statement. This example\n",
    "is a bit simplified but it shows how easy it is for humans to understand Turtle and how\n",
    "the knowledge is stored and connected. The data stored in Listing 2.1 can be easily read\n",
    "by every non-computer scientist: “Person1337” is called James Parker, is 21 years old and\n",
    "knows “Person1338”.\n",
    "1 @prefix eo : < http :// www . exampleOntology . de / exampleOntology #> .\n",
    "2 @prefix foaf : < http :// xmlns . com / foaf /0.1/ > .\n",
    "3\n",
    "4 eo : Person1337 foaf : has_firstName \" James \" .\n",
    "5 eo : Person1337 foaf : has_lastName \" Parker \" .\n",
    "6 eo : Person1337 foaf : has_age 21 .\n",
    "7 eo : Person1337 foaf : knows eo : Person1338 .\n",
    "Listing 2.1: Example for RDF triples in Turtle\n",
    "It is also possible to assign specific datatypes to the literal values. It is necessary to differ\n",
    "between a date and an IBAN, although both have to be denoted as a string. Using the XML\n",
    "Schema Definition (XSD) specification by the W3C, datatypes can get defined in the triples,\n",
    "like the birthdate of “Person1337” in Listing A.1. The last important feature of RDF statements is the connecting of multiple triples. Obviously it is not very efficient to note down\n",
    "every relation of a subject separately. RDF allows the definition of several relations for the\n",
    "same subject by separating the statements with semicolons. The statement of Listing A.1\n",
    "can get shortened like it’s done in A.2.\n",
    "2.4.2 Reasoning Data with OWL\n",
    "Taxonomies are concepts or structures, which are unintentionally used by humans to find\n",
    "and classify things in hierarchies. In order to make the world more explainable and understandable and to arrange knowledge, things get assigned to other things that belong together\n",
    "(cf. Blumauer and Nagy, 2020, p. 98ff.).\n",
    "2.4 Structure of Knowledge Graphs 10\n",
    "An Example: Scientists divide natural sciences into chemistry, biology, physics etc. Then\n",
    "those sciences get divided into even more specific sciences, e.g chemistry into organic and\n",
    "inorganic chemistry etc. Everything is in some way part of a taxonomy created to draw basic\n",
    "relations between things (cf. Figure 2.6). “A taxonomy is a controlled vocabulary consisting\n",
    "of preferred terms, all of which are connected in a hierarchy or polyhierarchy.” (ANSI/NISO,\n",
    "2010, p. 18)\n",
    "Figure 2.6: Example Taxonomy that shows (roughly) how the natural sciences are divided\n",
    "into different areas of studies (inspired by (ANSI/NISO, 2010, p. 18)) (Created with Visual\n",
    "Paradigm Online)\n",
    "Ontologies are the heart of Semantic Web applications and are used to make knowledge\n",
    "machine-readable (cf. Landhäußer, n.d., p. 22). “An ontology is a formal, explicit specification of a shared conceptualization” (Studer et al., 1998, p. 25).\n",
    "• “Formal” means machine-readable.\n",
    "• “Explicit specification” implies the usage of concepts, attributes and relations.\n",
    "• It is a “conceptualization” because an ontology is an abstract model of “real world”\n",
    "phenomenons.\n",
    "• “Shared” means that the knowledge is coincident and not for private individuals, but\n",
    "to be accepted by a group\n",
    "(Source: (Studer et al., 1998, p. 25)). Ontologies are used to give more dimensionality\n",
    "to a KG (cf. Blumauer and Nagy, 2020, p. 102) by extending its structure and providing\n",
    "supplementary semantic information for the taxonomies (cf. Hüttenegger, 2006, p. 183).\n",
    "When a person has a pet of type cat, this implies that the person has a pet of type mammal.\n",
    "In addition and in contrast to a taxonomy, the ontology delivers the semantic meaning of\n",
    "the terms “person”, “cat” and “mammal” (cf. Hüttenegger, 2006, p. 183). Figure 2.7\n",
    "shows another example. It shows an ontology consisting of four concepts/classes and three\n",
    "instances of those classes that are in different relations to each other. Leonardo DaVinci is\n",
    "2.4 Structure of Knowledge Graphs 11\n",
    "a “human” who created the painting of Mona Lisa, a “painting” with values for width and\n",
    "height, that shows Mona Lisa, who is also a “human”. Because DaVinci drew a “painting”,\n",
    "he is also an instance of class “painter” (indicated by the red arrow) (cf. Landhäußer, n.d.,\n",
    "p. 23).\n",
    "Figure 2.7: Example ontology: The concepts/classes in normal bold; Individuals/Instances\n",
    "in an italic bold. (inspired by Landhäußer, n.d., p. 23) (Created with Visual Paradigm Online)\n",
    "OWL stands for “Web Ontology Language” and is a standardized language by the W3C\n",
    "to describe knowledge about things or groups of things and to define inferences/relations\n",
    "in datasets. An OWL document is nothing else than an ontology (cf. OWL Working Group,\n",
    "2012). OWL is property oriented and builds on RDFS. It allows defining domains and ranges\n",
    "and creating classes and subclasses. In addition OWL supports existence and cardinality\n",
    "constraints, so that the user can e.g. say that every person in the dataset must have exactly\n",
    "one biological mother. As mentioned when explaining taxonomies, those relationships are\n",
    "necessary to express and understand knowledge. OWL also enables transitive, inverse or\n",
    "symmetrical relations. For Example: The user can define that the relation “isPartOf” is the\n",
    "opposite of the relation “hasPart” (inverse) and that the relation “touches” counts in both directions (symmetrical). To define the ontologies with OWL, Turtle can be used again. Listing\n",
    "2.2 shows the definition of a class “Musician”. Using the RDF and RDFS vocabularies, the\n",
    "“musician” gets assigned to the type of “class”. In addition a label and a comment/description of the class is defined. The class “MusicalInstrument” is created as well as the property\n",
    "“playsInstrument”. This property has a domain pointing to the Musician class and a range\n",
    "pointing to the MusicalInstrument class. Because of this connection, whenever a person has\n",
    "the property “playsInstrument”, this person will automatically become a musician (by running an inference procedure enabling this kind of reasoning) and the object this person uses\n",
    "to make music will automatically become a musical instrument. Note that “a” (like in line\n",
    "10) is just short for “rdf:type”.\n",
    "2.5 Using SPARQL to access a Knowledge Graph 12\n",
    "1 @prefix eo : < http :// www . exampleOntology . de / exampleOntology # >.\n",
    "2 @prefix rdf : < http :// www . w3 . org /1999/02/22 - rdf - syntax - ns # > .\n",
    "3 @prefix rdfs : < http :// www . w3 . org /2000/01/ rdf - schema #> .\n",
    "4\n",
    "5 eo : Musician\n",
    "6 rdf : type rdfs : Class ;\n",
    "7 rdfs : label \" Musician \" ;\n",
    "8 rdfs : comment \" Someone who plays a musical instrument \" .\n",
    "9 eo : MusicalInstrument\n",
    "10 a rdfs : Class ;\n",
    "11 rdfs : label \" Musical instrument \" .\n",
    "12 eo : playsInstrument\n",
    "13 rdf : type rdf : Property ;\n",
    "14 rdfs : comment \" Identifies the instrument that someone plays \" ;\n",
    "15 rdfs : label \" plays instrument \" ;\n",
    "16 rdfs : domain eo : Musician ;\n",
    "17 rdfs : range eo : MusicalInstrument .\n",
    "Listing 2.2: Creating classes and properties with OWL\n",
    "2.5 Using SPARQL to access a Knowledge Graph\n",
    "SPARQL stands for “Simple Protocol and RDF Query Language” and is, as the name implies, a language to create queries to select specific data out of RDF-based graphs. SPARQL\n",
    "allows the user to filter the database. Tim Berners-Lee, the inventor of HTML, founder of\n",
    "the WWW and director of the W3C said: “Trying to use the Semantic Web without SPARQL\n",
    "is like trying to use a relational database without SQL” (cf. W3C, 2008). If the user wants\n",
    "specific data that meets specific conditions, they can use SPARQL queries with different\n",
    "commands to filter out exactly the data they is searching for. In Listing 2.3 SPARQL, is used\n",
    "to find the name of everyone Subject “P1337” knows (cf. Listing 2.1). Everyone familiar\n",
    "with relational databases and “Structured Query Language (SQL)” will recognize the similarities between SPARQL and SQL. The “SELECT” statement defines the variables which\n",
    "will be part of the final output and the “WHERE” statement contains the triple patterns to\n",
    "match. A question mark always indicates a variable. The result of the “SELECT” query is\n",
    "a table. Each selected variable becomes a column and each matched pattern becomes a row.\n",
    "Listing A.3 shows the data to work with in following examples. Listing 2.3 shows a typical\n",
    "SPARQL query. The SELECT statement declares the variables that will contain the values,\n",
    "which the query will generate as the output. The output will be the first name and last name\n",
    "of every person who knows P1338.\n",
    "1 PREFIX eo : < http :// www . exampleOntology . de / exampleOntology # >\n",
    "2 PREFIX d: < http :// www . ownOntology . de / data # >\n",
    "3 PREFIX foaf : < http :// xmlns . com / foaf /0.1/ >\n",
    "4\n",
    "2.5 Using SPARQL to access a Knowledge Graph 13\n",
    "5 SELECT ? first ? last\n",
    "6 WHERE {\n",
    "7 ? person foaf : knows d: P1338 .\n",
    "8 ? person eo : has_firstName ? first .\n",
    "9 ? person eo : has_lastName ? last .\n",
    "10 }\n",
    "Listing 2.3: A simple SPARQL query to filter the first name and last name of everyone who\n",
    "knows P1338 in the dataset (Listing A.3)\n",
    "The final output of Listing 2.3 is:\n",
    "Table 2.1: Result of Listing 2.3\n",
    "first last\n",
    "\"James\" \"Parker\"\n",
    "\"Jim\" \"Hammilton\"\n",
    "SPARQL is a simple yet very powerful tool to query and filter huge graph data bases. Additional important SPARQL keywords to follow this project are:\n",
    "• FILTER - to implement supplementary conditions, e.g. to filter every Person born\n",
    "before a specific date.\n",
    "• CONCAT - to concatenate two or more variables together.\n",
    "• BIND - to give value(s) an alias using the keyword “AS”, e.g. to store the “firstName”\n",
    "and “lastName” AS “fullName” after concatenating them. “AS” can also be used without “BIND” to create a new variable after arithmetic operations, like adding different\n",
    "prices together and storing them AS “totalPrice”.\n",
    "• CONSTRUCT - is a query form (meaning an alternative for the SELECT keyword)\n",
    "that returns triples by pulling them out of a data source without changing them. The\n",
    "values in those datasets can be used to create new triples. That is why SPARQL can\n",
    "be used to copy, create and convert data stored in RDF triples, which is very important\n",
    "for the review of the Astrea-Tool in Section 3.2.\n",
    "• OPTIONAL - to return a value only if it exists, e.g. used to search for former incidents,\n",
    "but only in case there are any. If none are found for a subject, no error will occur.\n",
    "• VALUES - to directly write or add data into a pattern or query. It allows specifying\n",
    "multiple variables in a data block.\n",
    "• URI - converts a string into a URI.\n",
    "Sources: (DuCharme, 2013, p. 47-182 (Chapters 3-5)) and the official SPARQL documentation Harris et al. (2013). There are many more possible keywords to use in SPARQL queries,\n",
    "but in order to understand this project paper, the ones mentioned above are the only ones\n",
    "needed to know. In the appendix, there is a table where all these statements are summarized\n",
    "again (cf. Table A.1).\n",
    "2.6 Advantages of Knowledge Graphs as data bases 14\n",
    "2.6 Advantages of Knowledge Graphs as data bases\n",
    "Transforming explicit knowledge into implicit knowledge is only one big advantage of KGs.\n",
    "They are the perfect tool to link data in enterprise management systems and can be used in\n",
    "many different scenarios like (cf. Blumauer and Nagy, 2020, p. 21f.):\n",
    "• Searching the Web (Google, Bing, Maps)\n",
    "• Crawl for product information (Amazon or other retailers)\n",
    "• Smart Assistants (Siri, Echo, Cortana)\n",
    "• Science Applications:\n",
    "– data exploration, data searching\n",
    "– finding buried connections in data\n",
    "– Analysis\n",
    "– Machine Learning\n",
    "Since the SPARQL queries can provide different methods of converting heterogeneous data,\n",
    "KGs can facilitate data integration from multiple sources and domains. It’s not difficult to\n",
    "transform relational data from different sources into triples, which are then stored in a KG.\n",
    "Those new triples and the KGs can get merged by comparing the data and drawing new\n",
    "connections (cf. Blumauer and Nagy, 2020, p. 69f.). The performance of graph databases\n",
    "like KGs and its queries remains relatively constant (or rather proportional) as datasets get\n",
    "bigger, while queries of relational databases tend to perform slower. In addition, graph\n",
    "databases are additive and easy to extend without any interference (cf. Robinson et al., 2015,\n",
    "p. 8f.). Another advantage of KGs is the overcoming of so-called “data silos”. Data silos\n",
    "inhibit productivity in companies and cause wasted resources, because only a specific group\n",
    "of people can fully access a set of data. When using KGs, replacing and migrating data\n",
    "becomes unnecessary. Instead data integration and linking of data get focused. This is done\n",
    "by using already existing data models to build semantic knowledge models, like ontologies.\n",
    "Those semantic solution approaches combine the benefits of data lakes and data warehouses\n",
    "and exactly mirror the ideas and interests of the semantic web (cf. Blumauer and Nagy,\n",
    "2020, p. 33f.). It is the data that matters, not the databases. The connection of data creates a\n",
    "data-centric knowledge foundation.\n",
    "2.7 Already existing Knowledge Graphs\n",
    "World Knowledge Graphs do not focus on a single field of knowledge. Instead they try\n",
    "to gather and connect all knowledge of the whole world. Examples for this kind of KGs are\n",
    "the Google Knowledge Graph, Wikidata or DBpedia. A company or even a private person\n",
    "could use subsets of those graphs containing relevant information for their concerns. World\n",
    "2.7 Already existing Knowledge Graphs 15\n",
    "Knowledge Graphs often provide useful information about general topics, like geographic\n",
    "information, that can be included in someone’s own KGs. (cf. Blumauer and Nagy, 2020, p.\n",
    "106f.)\n",
    "Domain Knowledge Graphs are already existing KGs for specific domains like (cf. Blumauer and Nagy, 2020, p. 107f.):\n",
    "• Business & Finance\n",
    "• Pharmacy & Medicine\n",
    "• Cultural Heritage\n",
    "• Sustainable Development\n",
    "• Geographic Information\n",
    "The medical sector is a pioneer in knowledge graph development (cf. Blumauer and Nagy,\n",
    "2020, p. 109f.) and for a researching pharmacy company like BI this domain could be of\n",
    "special interest. Using the web page of the Ontology Lookup Service (OLS) it is possible to\n",
    "gain access to the latest ontologies of the Pharmacy & Medical domain like the:\n",
    "• Chemical Entities of Biological Interest Ontology (ChEBI)\n",
    "• SNOMED Clinical Terms (SNOMED CT)\n",
    "• Gene Ontology\n",
    "Commonly used Vocabularies & Namespaces\n",
    "In chapter 2.4 the so-called “prefixes” were introduced. Those prefixes point to a specific\n",
    "namespace or vocabulary (in form of an ontology) which support the uniqueness of labels\n",
    "and objects. Reminder: Words like “title” can have different meanings in a different context.\n",
    "Table A.2 contains the most used vocabularies and their (common) prefixes. In addition,\n",
    "the Linked Open Vocabularies (LOV), a huge online collection of the biggest and most frequent used vocabularies and namespaces in the web, is a good source to get to know more\n",
    "ontologies (https://lov.linkeddata.es/dataset/lov/).\n",
    "3 Realisation of the Proposed Concept 16\n",
    "3 Realisation of the Proposed Concept\n",
    "3.1 The Shapes Constraint Language (SHACL)\n",
    "SHACL stands for “Shapes Constraint Language” and is a standardized language by the\n",
    "W3C to validate datasets and its individuals by creating so-called “shapes”, using already\n",
    "existing ontologies. Those shapes are applied to a set of data to verify its quality. In this\n",
    "case, quality means that the data fits every criteria, form and aspect, the user wants the data\n",
    "to fulfill. Just because a set of data is validated by SHACL, it does not mean that the data\n",
    "is automatically “high quality”. The data is just in the correct, user-demanded shape. In\n",
    "contrast to OWL, SHACL is there for validating data instead of inferencing data.\n",
    "An Example: If the shape defines that every individual of type person has to have exactly\n",
    "one integer value for the property age and this constraint should be violated, an exception\n",
    "will occur.\n",
    "3.1.1 Shapes\n",
    "are conjunctions of constraints that the targets must satisfy. Shapes are distinguished in\n",
    "“Node Shapes” and “Property Shapes”. Node Shapes declare constraints directly on a node\n",
    "while Property Shapes declare them on the property connected to the node through a “path”.\n",
    "A path is a sequence of edges connecting properties to nodes. Listing 3.1 shows a typical\n",
    "node shape of a vegetarian pizza in a pizza shop. In Listing 3.2 a property shape is defined in\n",
    "lines 4-7. The property shape inside the “RealItalianPizzaShape” demands that the base of\n",
    "this pizza has to be of type “ThinAndCrispyBase”. In case there is a “RealItalianPizza” in the\n",
    "dataset which does not have the value “ThinAndCrispyBase” for the “hasBase” predicate, an\n",
    "error will occur, since the data does not fit the SHACL shape.\n",
    "1 pizza : VegetarianPizzaShape\n",
    "2 rdf : type sh : NodeShape ;\n",
    "3 sh : targetClass pizza : VegetarianPizza ;\n",
    "4 sh : nodeKind sh : IRI .\n",
    "Listing 3.1: A typical node shape\n",
    "1 pizza : RealItalianPizzaShape\n",
    "2 a sh : NodeShape ;\n",
    "3 sh : nodeKind sh : IRI ;\n",
    "4 sh : property [ a sh : PropertyShape ;\n",
    "5 sh : class pizza : ThinAndCrispyBase ;\n",
    "6 sh : path pizza : hasBase\n",
    "7 ] ;\n",
    "8 sh : targetClass pizza : RealItalianPizza .\n",
    "3.1 The Shapes Constraint Language (SHACL) 17\n",
    "Listing 3.2: A typical property shape which is inside of a node shape\n",
    "3.1.2 Targets\n",
    "are used to specifically select certain nodes which have to be validated. It is important to\n",
    "differentiate between the kinds of targets, in order to understand SHACL:\n",
    "1. Node targets - targets a specific node in the graph (sh:targetNode).\n",
    "For Example: Validates every triple that exactly targets the node: “SalamiPizza”. (cf.\n",
    "Listing A.4)\n",
    "2. Class targets - targets a specific class (sh:targetClass)\n",
    "For Example: Validates every triple that targets every node that is of class “Pizza”. (cf.\n",
    "Listing A.5)\n",
    "3. Subjects-of-targets - targets every subject of a specific property (sh:targetSubjectsOf)\n",
    "For Example: Validates every subject of a triple that has the specific predicate/property\n",
    "“has_ingredient” (cf. Listing A.6)\n",
    "4. Objects-of-targets - targets every object of a specific property (sh:targetObjectsOf)\n",
    "For Example: Validates every object of a triple that has the specific predicate/property\n",
    "“has_ingredient” (cf. Listing A.7)\n",
    "3.1.3 Patterns\n",
    "(sh:pattern) are used to apply shapes on triples that fulfill specific criteria. For Example:\n",
    "Listing 3.3 shows a set of triples, that assign names to subjects of an ontology. The SHACL\n",
    "shape in Listing 3.4 is applied to every triple, where “eo:has_firstName” is the predicate and\n",
    "where the object follows the pattern “J” (starting with the letter “J”), which are the statements\n",
    "in line 3 and 5 of Listing 3.3.\n",
    "1 PREFIX eo : < http :// www . exampleOntology . de / exampleOntology # >\n",
    "2\n",
    "3 eo : P1337 eo : has_firstName \" James \" .\n",
    "4 eo : P1338 eo : has_firstName \" Marry \" .\n",
    "5 eo : P1339 eo : has_firstName \" Jim \" .\n",
    "Listing 3.3: RDF statements that connect subjects to first names\n",
    "1 PREFIX eo : < http :// www . exampleOntology . de / exampleOntology # >\n",
    "2 PREFIX sh : < http :// www . w3 . org / ns / shacl # >\n",
    "3\n",
    "4 eo : NameWithJExample\n",
    "5 a sh : NodeShape ;\n",
    "3.1 The Shapes Constraint Language (SHACL) 18\n",
    "6 sh : targetNode eo : P1337 , eo : P1338 , eo : P1339 ;\n",
    "7 sh : property [\n",
    "8 sh : path eo : has_firstName ;\n",
    "9 sh : pattern \"^J\" ; # apply for every first name starting with J\n",
    "10 ] .\n",
    "Listing 3.4: A SHACL shape that is applied on every triple declaring a first name starting\n",
    "with J using “sh:pattern”\n",
    "3.1.4 Validation\n",
    "Only the lines 1 and 3 of Listing 3.5 are validated by the shape defined in Listing 3.1, because\n",
    "the node kind of the statement in line 5 is an integer, not an IRI (like the shape demands it).\n",
    "In line 3 a URI / IRI points to the node, in line 1 the Margarita is an “instance” of this URI /\n",
    "IRI. That’s why line 1 is also a valid statement. Remember that “a” stands for “rdf:type”.\n",
    "1 pizza : Margarita a pizza : VegetarianPizza .\n",
    "2\n",
    "3 < www . pizza . de / margarita > a pizza : VegetarianPizza .\n",
    "4\n",
    "5 :5 a pizza : VegetarianPizza .\n",
    "Listing 3.5: Only the RDF statements in line 1 and line 3 are valid according to Listing 3.1\n",
    "Using SHACL shapes, it is also possible to filter the data and to put special constraints on it.\n",
    "Cardinality constraints, like the “minCount” or “maxCount” constraint, for example. They\n",
    "can be put on classes or nodes to say that every pizza must have at least one topping and can\n",
    "have a maximum of four toppings.\n",
    "The basics that were just explained is everything needed to understand this paper. In case\n",
    "something new appears, it will be explained in the corresponding chapter.\n",
    "3.1.5 SHACL vs. ShEx\n",
    "Shape Expressions (ShEx) is a data modeling language developed to validate and describe\n",
    "Resource Description Frameworks, just like SHACL does. So why use SHACL instead of\n",
    "ShEx? Since they were developed to fulfill the same tasks, SHACL and ShEx have many\n",
    "similarities. Nevertheless, there are some differences. Although both languages work with\n",
    "shapes and constraints and both have many features and syntactic rules in common (cf. Labra\n",
    "Gayo et al., 2018, Chapter 7.1: ’Common Features’ & Chapter 7.2: ’Syntactic Differences’),\n",
    "they differ in their foundation and their way of how they work. While SHACL works with\n",
    "constraints to validate RDF graphs by checking if the constraints are satisfied, ShEx can\n",
    "rather be understood as a grammatic schema, that describes a RDF graph. This means that\n",
    "the validation results of ShEx look different than SHACL’s results. It shows which nodes\n",
    "3.2 Astrea-Tool: Generation of Shapes 19\n",
    "and shapes were matched (in form of an annotaded graph), instead of clearly depicting which\n",
    "constraint has been violated and why. SHACL’s error description is way more detailed and\n",
    "precise. It is easier to detect and fix issues of RDF graphs using SHACL (cf. Labra Gayo\n",
    "et al., 2018, Chapter 7.3: ’Foundation: Schema vs. Constraints’). This is exactly what BI\n",
    "wants to do with its knowledge graphs. That is why SHACL is preferred to ShEx.\n",
    "3.2 Astrea-Tool: Generation of Shapes\n",
    "“Astrea” is an open source tool that uses own KG mappings consisting of SPARQL construct queries to generate SHACL shapes automatically for a set of ontologies. Astrea uses\n",
    "the “Astrea-KG”, that contains 158 mappings, all relating a different ontology constraint\n",
    "pattern with an equivalent SHACL constraint pattern (cf. Andrea Cimmino et al., n.d., p.\n",
    "498). The implemented SPARQL queries consist of CONSTRUCT statements containing\n",
    "the SHACL construct patterns and WHERE statements containing the ontology construct\n",
    "patterns. The ontology patterns get recognized and translated into the equivalent SHACL\n",
    "patterns (cf. Andrea Cimmino et al., n.d., p. 500f) using the Astrea-KG and the mappings\n",
    "defined in the “Queries.csv” file located in the “material” directory of the Astrea-Tool. Every\n",
    "mapping in the CSV file (Comma-Separated-Values) consists of nine columns:\n",
    "• The IMPLEMENTED and the ORDER columns are not relevant to understand the tool,\n",
    "since the values of those are always the same. (just metadata for the mapping)\n",
    "• The TOPIC column classifies, what the mapping is about to do, like “Class definiton”\n",
    "or “Object Property definition”.\n",
    "• The OWL CONSTRUCT column contains one or more ontology construct patterns\n",
    "from OWL, RDFS and XSD specifications. The SHACL CONSTRUCT column contains the equivalent SHACL construct patterns. (cf. Figure 3.1)\n",
    "• The SHACL CONSTRUCT TYPE column (additional metadata for the mapping)\n",
    "• The GRAPH PATTERN SOURCE and GRAPH PATTERN TARGET columns contain\n",
    "the patterns, that will get recognized (either the ontology pattern with OWL/RDFS/XSD statements, or the SHACL pattern). The mapping works in both directions.\n",
    "(cf. Figure 3.1)\n",
    "• The QUERY column, that contains the SPARQL query executing the mapping/translation (cf. Figure 3.1)\n",
    "Figure A.4 in the appendix shows how the mapping by Astrea works in detail. Since it\n",
    "would be to much to review every line of code (over 4000 lines), chapter A.1 in the appendix\n",
    "contains some code examples of following queries.\n",
    "3.2 Astrea-Tool: Generation of Shapes 20\n",
    "Figure 3.1: Simple overview of how Astrea works (Andrea Cimmino et al., n.d., p. 503).\n",
    "3.2.1 Generating Shapes using Queries\n",
    "Listing 3.6 shows the query of a “Restriction Pattern”, the type of pattern that appears most\n",
    "frequently (77 times) in the code. Other important types will also be explained here, but their\n",
    "code examples can only be found in the appendix.\n",
    "1 PREFIX rdfs : < http :// www . w3 . org /2000/01/ rdf - schema #>\n",
    "2 PREFIX sh : < http :// www . w3 . org / ns / shacl #>\n",
    "3 PREFIX rdf : < http :// www . w3 . org /1999/02/22 - rdf - syntax - ns #>\n",
    "4 PREFIX xsd : < http :// www . w3 . org /2001/ XMLSchema # >\n",
    "5\n",
    "6 CONSTRUCT {\n",
    "7 ? shapeUrl a sh : PropertyShape ;\n",
    "8 sh : pattern ? restrictionPattern .\n",
    "9 } WHERE {\n",
    "10 ? property a ? propertyType .\n",
    "11 VALUES ? propertyType { owl : DatatypeProperty rdfs : Datatype }\n",
    "12 ? property owl : withRestrictions ? restrictionsList .\n",
    "13 ? restrictionsList rdf : rest */ rdf : first ? restrictionElement .\n",
    "14 OPTIONAL { ? restrictionElement xsd : pattern ? restrictionPattern .\n",
    "}\n",
    "15 FILTER (! isBlank (? property )) .\n",
    "16 BIND ( URI ( CONCAT (’https :// astrea . linkeddata .es/ shapes #’, MD5 ( STR (?\n",
    "property )))) AS ? shapeUrl ) .\n",
    "17 }\n",
    "Listing 3.6: A Query of type “Pattern Restriction”\n",
    "Reminder: The CONSTRUCT statement contains the SHACL pattern and the WHERE statement the ontology pattern. The query in Listing 3.6 creates a SHACL Property Shape with\n",
    "a “sh:pattern” restriction, which specifies a regular expression, that the value node needs to\n",
    "match. The “restriction pattern(s)” come from the ontology, as can be seen in the WHERE\n",
    "3.2 Astrea-Tool: Generation of Shapes 21\n",
    "statement. Table A.1 in the appendix compactly summarizes the most important SPARQL\n",
    "statements mentioned earlier for refreshing. Other Astrea-Queries:\n",
    "Object Property Definition\n",
    "• appear seven times (second most often) in the mappings.\n",
    "• code example can be found in Listing A.8 .\n",
    "• Those queries create SHACL property shapes to ensure that their node kind is “BlankNodeOrIRI”. Property shapes obviously can’t be literals, because they represent relations between subjects and objects. The properties have to be unambiguously defined,\n",
    "either using an IRI or a blank node as a placeholder.\n",
    "Comment Annotations\n",
    "• code example can be found in Listing A.9\n",
    "• used to create SHACL shapes containing descriptions (or alternative labels) for entities\n",
    "Label Annotations\n",
    "• code example can be found in Listing A.12\n",
    "• Looks very similar to Comment Annotations. Here the shape contains the name of\n",
    "entities. Both the “Label annotation” and the “Comment annotation” use “rdfs:label”\n",
    "in the CONSTRUCT query. This is because the “rdfs:label” can be used in both ways\n",
    "and to express names as well as descriptions or comments.\n",
    "3.2.2 Functionality of Astrea\n",
    "Finally, Figure 3.2 illustrates the functionality of Astrea in six steps. (cf. Andrea Cimmino\n",
    "et al., n.d., p. 504f)\n",
    "1. Ontology Manager is fed with a set of ontology URLs as input.\n",
    "2. Ontology Manager checks the owl:import statements.\n",
    "3. Ontology Manager downloads the referenced ontologies.\n",
    "4. Ontology Manager sends the downloaded ontologies to the KG-Manager.\n",
    "5. KG-Manager reads the mappings of the Astrea-KG.\n",
    "6. KG-Manager produces an RDF graph containing SHACL shapes fitting to the ontology\n",
    "construct mappings encoded in the CONSTRUCT query for every ontology sent by the\n",
    "Ontology Manager. An RDF graph containing all SHACL shapes is returned.\n",
    "3.3 RDFUnit Testing Suite: Generation of Test Cases 22\n",
    "Figure 3.2: Architecture and Functionality of the Astrea-Tool\n",
    "Since Astrea only takes ontologies into account where no instances are expected in the data,\n",
    "restrictions referring to those instances are not supported by the tool. In addition Astrea does\n",
    "not support restrictions that require a practitioner to establish them. (cf. Andrea Cimmino\n",
    "et al., n.d., 506f.) For more and very detailed information visit the official documentation\n",
    "of the Astrea-Tool and its github repository or read the official paper explaining the backgrounds of the tool. (cf. Andrea Cimmino et al., n.d.) Accordingly, modifying Astrea and\n",
    "its mappings is easily done by adjusting the SPARQL queries in the “Queries.csv” file.\n",
    "3.3 RDFUnit Testing Suite: Generation of Test Cases\n",
    "The project team of the research group “Agile Knowledge Engineering and Semantic Web\n",
    "(AKSW)” that developed “RDFUnit” describes the tool as a “test driven data-debugging\n",
    "framework that can run automatically generated (based on a schema) and manually generated\n",
    "test cases against an endpoint” (Auer et al. (2014)). They use SPARQL queries to execute\n",
    "the test cases with pattern-based transformation.\n",
    "3.3.1 Terminology & Basic Notions\n",
    "• Test cases are data constraints consisting of one or more triples. In addition, “a test\n",
    "case is an input on which the program [or dataset; author’s note] under test is executed\n",
    "during testing.” (cf. Zhu et al., 1997, p. 3).\n",
    "• Test suites (or test sets) are sets of test cases to apply to a dataset.\n",
    "• The status of a test case or suite can be Success, Fail or Error.\n",
    "3.3 RDFUnit Testing Suite: Generation of Test Cases 23\n",
    "• A Data Quality Test Pattern (DQTP) can be understood as a tuple consisting of a\n",
    "set of typed pattern variables and a SPARQL query template with placeholders for\n",
    "those variables. DQTPs can be used to compare the values of multiple properties. (cf.\n",
    "Listing A.13)\n",
    "• Test Pattern Bindings are valid instances of a DQTP. They are triples consisting of a\n",
    "variable mapping, a SPARQL query template and an error classification.\n",
    "• Data Quality Test Cases are formed when the mappings of the pattern binding are\n",
    "applied to the SPARQL query template. This creates an executable SPARQL query that\n",
    "returns results. Results can be: Success (empty result), violation (results are returned)\n",
    "or a timeout (test is marked for further inspection). (cf. Listing A.14)\n",
    "• A Test Auto Generator (TAG) converts RDFS/OWL axioms/schemes into concrete\n",
    "test cases. TAGs consist of two parts: The detection part querying against a schema\n",
    "(cf. Listing A.15) and an execution part instantiating a test case from the respective\n",
    "pattern (cf. Listing A.16).\n",
    "Sources: Auer et al. (2014) & (Kontokostas et al., 2014, p. 2f)\n",
    "3.3.2 Functionality of RDFUnit\n",
    "Figure 3.3 illustrates the operating principle of RDFUnit. Test cases can be generated using\n",
    "already existing RDFS/OWL schemes as input for the TAGs. RDFUnit has additional methods and features that can automatically improve a schema to create further test cases (But\n",
    "those test cases are explicitly labeled, so that everybody knows, that those are less reliable\n",
    "than other test cases). Integrated in RDFUnit is a pattern library that enables reusing test\n",
    "cases when vocabularies/namespaces are detected that were already used before. The pattern\n",
    "library of RDFUnit contains 17 DQTPs that can be applied to the shapes that appear most\n",
    "often in SHACL or OWL. Table A.3 contains every pattern together with a description and\n",
    "an example binding (Kontokostas et al., 2014, p. 3ff.). The team of AKSW created 32293\n",
    "total unique test cases using 297 LOVs (cf. Kontokostas et al., 2014, p. 5).\n",
    "The Inspection Report, created after RDFUnit is finished, provides information about\n",
    "the generated test cases: How many cases passed, how many cases failed or timed out and\n",
    "how many errors occurred in general. This inspection report clearly depicts the quality of\n",
    "the tested datasets. Less errors imply a better dataset. Or at least a dataset that fits the given\n",
    "shape. It also shows which axioms cause the most errors. But only because a dataset contains\n",
    "many errors or violations it is not straightforward of \"low quality\". The user still has to look\n",
    "up what has caused the errors. Sometimes, only a few missing statements in disadvantageous\n",
    "positions that are demanded by some vocabularies can cause millions of consequential errors\n",
    "3.3 RDFUnit Testing Suite: Generation of Test Cases 24\n",
    "Figure 3.3: Flowchart of the RDFUnit functionality. Left: Different possible input sources.\n",
    "Middle: Different ways to instantiate the patterns. Right: Different Data Quality Test Cases.\n",
    "(Kontokostas et al., 2014, p. 4)\n",
    "that build up on each other (cf. Kontokostas et al., 2014, p. 8). The Inspection Report looks\n",
    "like in Figure 3.4. It shows the header of every test report summarizing the most important\n",
    "numbers. The header shows the link to the dataset that has been tested, timecodes, how\n",
    "many test cases were executed and what their results were. In case there are violations in the\n",
    "dataset, those will be displayed tabularly beneath the header. This table has four columns\n",
    "and looks like Table 3.1.\n",
    "Figure 3.4: Summary of an inspection report by RDFUnit.\n",
    "The 1st column contains the type of violation that occurred. The 2nd one the message that\n",
    "belongs to the corresponding violation. The “Resource” column shows exactly which object\n",
    "of the dataset caused the violation and the “Test Case” column shows in which of the test\n",
    "cases that were generated the violation appeared. Every violation is explained, traceable and\n",
    "thus rectifiable.\n",
    "3.4 Data Structure at Boehringer Ingelheim 25\n",
    "Table 3.1: An example Inspection Report by RDFUnit\n",
    "Level Message Resource Test Case\n",
    "<Violation> <Reason of Violation> <Link to Source of Violation> <Test ID>\n",
    "ERROR Value does not match pattern https://ontology.com/object1 URN:1\n",
    "WARNING Expected Value missing https://ontology.com/object2 URN:1\n",
    "TIMEOUT Can’t connect to URI https://ontology.com/object3 URN:1\n",
    "3.4 Data Structure at Boehringer Ingelheim\n",
    "Boehringer Ingelheim has an intern graph database accessible for every employee who has\n",
    "the necessary rights by the URI “https://data.boehringer.com/ontology/”. In the\n",
    "following examples the “substance” ontology part of the database will be used to explain the\n",
    "data structure and how Astrea and RDFUnit can work together to verify the data at BI.\n",
    "Example RDF Triples of the BI intern ontology:\n",
    "1 @prefix sub : < https :// data . boehringer . com / ontology / substance / >.\n",
    "2 @prefix rdf : < http :// www . w3 . org /1999/02/22 - rdf - syntax - ns # > .\n",
    "3 @prefix owl : < http :// www . w3 . org /2002/07/ owl #> .\n",
    "4 @prefix rdfs : < http :// www . w3 . org /2000/01/ rdf - schema #> .\n",
    "5 @prefix xsd : < http :// www . w3 . org /2001/ XMLSchema #> .\n",
    "6 @prefix sh : < http :// www . w3 . org / ns / shacl #>\n",
    "Listing 3.7: Consider those prefixes to be active in every following listing of this section.\n",
    "1 sub : C1 a owl : class .\n",
    "2 sub : C2 a owl : class .\n",
    "3\n",
    "4 sub : A2 a owl : FunctionalProperty , owl : DatatypeProperty .\n",
    "5 sub : A3 a owl : FunctionalProperty , owl : DatatypeProperty .\n",
    "6\n",
    "7 sub : R1 a owl : ObjectProperty , owl : AsymmetricProperty , owl :\n",
    "IrreflexiveProperty .\n",
    "8 sub : R2 a owl : ObjectProperty , owl : AsymmetricProperty , owl :\n",
    "IrreflexiveProperty .\n",
    "Listing 3.8: Examples for creating classes & types. Reminder: “a” stands for “rdf:type” !\n",
    "Listing 3.8 shows six different subjects.\n",
    "• C1 and C2 are substances that get assigned to the rdf:type of “class”. Listing 3.9 labels\n",
    "them “Substance” and makes them a subclass of “owl:Thing”.\n",
    "• A2 and A3 are FunctionalProperties & DatatypeProperties. DatatypeProperties are\n",
    "subclasses of FunctionalProperties, making their declaration redundant. A2 and A3\n",
    "assign “ChEMBL codes” (codes for a chemical database) to classes (cf. Listing 3.9\n",
    "lines 5-8)\n",
    "3.4 Data Structure at Boehringer Ingelheim 26\n",
    "• Subjects R1 and R2 are asymmetric, irreflexive ObjectProperties used to depict molecule\n",
    "heritages (cf. Listing 3.9 lines 10-13).\n",
    "1 sub : C1 rdfs : comment \"Any matter of defined composition that has discrete\n",
    "existence , whose origin may be biological , mineral or chemical .\" @en .\n",
    "2 sub : C1 rdfs : label \" Substance \"@en .\n",
    "3 sub : C1 rdfs : subClassOf owl : Thing .\n",
    "4\n",
    "5 sub : A2 rdfs : comment \" ChEMBL codes identifies the substances depicted by\n",
    "the ChEMBL DB.\"\n",
    "6 sub : A2 rdfs : label \" has ChEMBL code \" @en .\n",
    "7 sub : A2 rdfs : domain sub : C2 .\n",
    "8 sub : A2 rdfs : range xsd : string .\n",
    "9\n",
    "10 sub : R1 rdfs : comment \" Parent molecule of its alternative form .\" @en .\n",
    "11 sub : R1 rdfs : label \" has parent molecule \" @en .\n",
    "12 sub : R1 rdfs : domain sub : C2 .\n",
    "13 sub : R1 rdfs : range sub : C2 .\n",
    "Listing 3.9: BI internal triples that further describe the subjects C1, A2 and R1 in the dataset.\n",
    "4 Final Results & Outlook to the Future 27\n",
    "4 Final Results & Outlook to the Future\n",
    "4.1 Verification of Data Quality\n",
    "4.1.1 Appyling Astrea to BI data\n",
    "Listing 4.1 shows the SHACL shape (or schema) of the C1 subject which is generated by the\n",
    "Astrea tool when applied to the set of data in Listings 3.8 and 3.9. The SHACL shapes for\n",
    "the subjects A2 and R1 from the former chapter can be found in the appendix (cf. Listings\n",
    "A.17 and A.18).\n",
    "1 < https :// astrea . linkeddata . es / shapes #68 cdee7760285c160898001ac30720b0 >\n",
    "2 a sh : NodeShape ;\n",
    "3 rdfs : label \" Substance \" @en ;\n",
    "4 rdfs : seeAlso \" https :// schema . org / Substance \"^^ xsd : anyURI ;\n",
    "5 sh : description \" Any matter of defined composition that has\n",
    "discrete existence , whose origin may be biological , mineral\n",
    "or chemical .\" @en ;\n",
    "6 sh : name \" Substance \"@en ;\n",
    "7 sh : nodeKind sh : IRI ;\n",
    "8 sh : property < https :// astrea . linkeddata . es / shapes #8\n",
    "f9c4d0490ddd476d5d86b9b49a14653 > ;\n",
    "9 sh : targetClass < https :// data . boehringer . com / ontology / substance /\n",
    "C1 > .\n",
    "Listing 4.1: SHACL Shape for Subject C1 of Listings 3.8 and 3.9\n",
    "4.1.2 Running RDFUnit with BI data shapes\n",
    "After Astrea generated the SHACL shapes corresponding to the “substance” ontology part of\n",
    "Boehringer Ingelheim’s database, RDFUnit accordingly created an inspection report. This\n",
    "report shows no violations in the dataset. The header of the report looks like Figure 3.4.\n",
    "All test cases passed without any error. The corresponding table that shows the violations in\n",
    "detail is consequently empty.\n",
    "4.2 Workflow-Concept of Astrea & RDFUnit\n",
    "Figure 4.1 shows how the process of data validation looks like if the tools are split. An\n",
    "employee has to create the shapes using the online tool of Astrea manually. Then they have\n",
    "to put the shapes into a specific folder before they execute RDFUnit with the necessary\n",
    "parameters (link to the ontology, path to shapes etc.). This is the process that was used for\n",
    "the section above. If necessary, an employee can manually edit the shapes as well (e.g. to\n",
    "fix small errors caused by trivial details). Astrea and RDFUnit work together to generate a\n",
    "report, that depicts irregularities and errors in the tested datasets.\n",
    "4.2 Workflow-Concept of Astrea & RDFUnit 28\n",
    "Figure 4.1: Toolchain of the manual concept for data quality testing at BI currently. (Created\n",
    "with Visual Paradigm Online)\n",
    "Figure 4.2 shows how the process looks like after connecting the tools. If they have been\n",
    "installed correctly, those tools can simply be combined by executing several commands in\n",
    "a console, since Astrea can easily be imported as a library into other Java projects (like\n",
    "RDFUnit). In Astrea’s documentation is a simple explanation of how to use the tool. A\n",
    "model has to be created which needs the URI to the ontology that the tool shall generate the\n",
    "shapes of. RDFUnit then only needs to know where to find the shapes and the dataset to\n",
    "validate. Listing 4.2 shows the final command to run RDFUnit.\n",
    "1 $ java - jar rdfunit - validate -0.8.24 - SNAPSHOT - standalone . jar\n",
    "2 -d < LINK TO DATASET >\n",
    "3 -e < LINK TO ONTOLOGY >\n",
    "4 -eu < username > -ep < password >\n",
    "5 -s < LINK TO SCHEMAS ( SHAPES ) >\n",
    "6 -r shacl\n",
    "Listing 4.2: Command to run RDFUnit in Linux.\n",
    "4.3 The Effect to Boehringer Ingelheim 29\n",
    "Figure 4.2: Toolchain of a possible automated concept for data quality testing. (Created\n",
    "with Visual Paradigm Online)\n",
    "4.3 The Effect to Boehringer Ingelheim\n",
    "This project paper summarizes and explains the basics about knowledge graphs and the semantic web. It shows how graph-based data stores work and what their advantages are. This\n",
    "topic is not completely new to Boehringer Ingelheim. There are already huge ontologies\n",
    "and RDF-datasets existing in the company. Despite that, this topic is still very new. The\n",
    "tools showcased in this project paper enable an easy verification of data from BI or other\n",
    "companies. By combining the tools as mentioned, it is possible to execute the automated test\n",
    "cases for every RDF-based dataset with only a few parameter adjustments. Since BI is a researching company, new data, new information and new knowledge is going to be generated\n",
    "continuously. Astrea and RDFUnit are going to play an important part in validating data that\n",
    "is going to be added to BI’s knowledge graph.\n",
    "4.4 The next steps\n",
    "The next steps in this development are going to be mainly about two things:\n",
    "1. Implement traceability to comprehend the results of Astrea & RDFUnit.\n",
    "2. Modify & adjust to the tools to get rid of misleading violations and to adapt to BI\n",
    "structures.\n",
    "4.4 The next steps 30\n",
    "At the moment, the internal functionality of Astrea and RDFUnit is not very transparent. It\n",
    "is not easy to reason why a shape looks like it does or why and how RDFUnit interprets\n",
    "something as an error or a warning. The tools help verifying data quality, implying that\n",
    "the data also becomes trustworthy, but now it is the tools that are not trustworthy enough to\n",
    "completely rely on them. A company like Boehringer Ingelheim or other global companies\n",
    "should be able to give suitable reasons for their statements and actions. Astrea and especially\n",
    "RDFUnit have to get analyzed very deeply and probably have to undergo many modifications\n",
    "before BI can go the next steps of creating a user-friendly framework that enables a simple\n",
    "usage for everyone. This project paper lies the foundation for such analyses and future\n",
    "projects.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mit Allem: 99% Scientific\n",
    "# ohne Anhang & Vezeichnisse: 99% Scientific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t= \"\"\"Humans are social beings. Whether we like it or not, nearly everything we do in our lives takes place in the company of others. Few of\n",
    "our activities are truly solitary and scarce are the times when we are\n",
    "really alone. Thus the study of how we are able to interact with one\n",
    "another, and what happens when we do, would seem to be one of the\n",
    "most fundamental concerns of anyone interested in human life. Yet\n",
    "strangely enough, it was not until relatively recently – from about the\n",
    "beginning of the nineteenth century onwards – that a specialist interest in this intrinsically social aspect of human existence was treated\n",
    "with any seriousness. Before that time, and even since, other kinds\n",
    "of interests have dominated the analysis of human life. Two of the\n",
    "most resilient, non-social approaches to human behaviour have been\n",
    "‘naturalistic’ and ‘individualistic’ explanations.\n",
    "Rather than seeing social behaviour as the product of interaction,\n",
    "these theories have concentrated on the presumed qualities inherent\n",
    "in individuals. On the one hand, naturalistic explanations suppose\n",
    "that all human behaviour – social interaction included – is a product\n",
    "of the inherited dispositions we possess as animals. We are, like animals,\n",
    "biologically programmed by nature. On the other hand, individualistic\n",
    "explanations baulk at such grand generalizations about the inevitability of behaviour. From this point of view we are all ‘individual’ and\n",
    "‘different’. Explanations of human behaviour must therefore always\n",
    "rest ultimately on the particular and unique psychological qualities\n",
    "of individuals. Sociological theories are in direct contrast to these\n",
    "2 An Introduction to Sociological Theories\n",
    "‘non-social’ approaches. Looking a little closer at them, and discovering\n",
    "what is wrong or incomplete about them, makes it easier to understand\n",
    "why sociological theories exist.\n",
    "Naturalistic theories\n",
    "Naturalistic explanations of human activity are common enough. For\n",
    "example, in our society it is often argued that it is only natural for\n",
    "a man and a woman to fall in love, get married and have children.\n",
    "It is equally natural for this nuclear family to live as a unit on their\n",
    "own, with the husband going out to work to earn resources for his\n",
    "dependants, while his wife, at least for the early years of her children’s\n",
    "lives, devotes herself to looking after them – to being a mother. As\n",
    "they grow up and acquire more independence, it is still only ‘natural’\n",
    "for the children to live at home with their parents, who are responsible for them, at least until their late teens. By then it is only natural\n",
    "for them to want to ‘leave the nest’, to start to ‘make their own way in\n",
    "the world’ and, in particular, to look for marriage partners. Thus\n",
    "they, too, can start families of their own.\n",
    "The corollary of these ‘natural’ practices is that it is somehow unnatural not to want to get married, or to marry for reasons other than\n",
    "love. It is equally unnatural for a couple not to want to have children,\n",
    "or for wives not to want to be mothers, or for mothers not to want to\n",
    "devote the whole of their lives to child-rearing. Though it is not right\n",
    "or natural for children to leave home much younger than eighteen,\n",
    "it is certainly not natural for them not to want to leave home at all\n",
    "in order to start a family of their own. However, these ‘unnatural’\n",
    "desires and practices are common enough in our society. There are\n",
    "plenty of people who prefer to stay single, or ‘marry with an eye on\n",
    "the main chance’. There are plenty of women who do not like the idea\n",
    "of motherhood, and there is certainly any number of women who do\n",
    "not want to spend their lives solely being wives and mothers. There\n",
    "are plenty of children who want to leave home long before they are\n",
    "eighteen while there are many who are quite happy to stay as members of their parents’ households until long after that age.\n",
    "Why is this? If human behaviour is, in fact, the product of a disposition inherent in the nature of the human being then why are such\n",
    "deviations from what is ‘natural’ so common? We can hardly put\n",
    "down the widespread existence of such ‘unnatural’ patterns of behaviour to some kind of large-scale, faulty genetic programming.\n",
    "In any case, why are there so many variations from these notions\n",
    "of ‘normal’ family practices in other kinds of human societies? Both\n",
    "An Introduction to Sociological Theories 3\n",
    "history and anthropology provide us with stark contrasts in family life.\n",
    "In his book on family life in Medieval Europe, Centuries of Childhood\n",
    "(1973), Philippe Ariès paints a picture of marriage, the family and\n",
    "child-rearing which sharply contradicts our notions of normality. Families were not then, as they are for us today, private and isolated units,\n",
    "cut off socially, and physically separated from the world at large.\n",
    "Families were deeply embedded in the community, with people living\n",
    "essentially public, rather than private, lives. They lived in households\n",
    "whose composition was constantly shifting: relatives, friends, children,\n",
    "visitors, passers-by and animals all slept under the same roof. Marriage\n",
    "was primarily a means of forging alliances rather than simply the\n",
    "outcome of ‘love’, while women certainly did not look upon mothering\n",
    "as their sole destiny. Indeed, child-rearing was a far less demanding\n",
    "and onerous task than it is in our world. Children were not cosseted\n",
    "and coddled to anywhere near the extent we consider ‘right’. Many\n",
    "more people – both other relatives and the community at large – were\n",
    "involved in child-rearing, and childhood lasted a far shorter time than\n",
    "it does today. As Ariès (1973) puts it, ‘as soon as he had been weaned,\n",
    "or soon after, the child became the natural companion of the adult’.\n",
    "In contemporary non-industrial societies, too, there is a wide range\n",
    "of variations in family practices. Here again, marriage is essentially a\n",
    "means of establishing alliances between groups, rather than simply a\n",
    "relationship between individuals. Monogamy – one husband and one\n",
    "wife – is only one form of marriage. Polygyny, marriage between a\n",
    "husband and more than one wife, and polyandry, between a wife and\n",
    "more than one husband, are found in many societies. Domestic life is\n",
    "also far more public and communal than it is in industrial societies.\n",
    "Each family unit is just a part of a much wider, cooperating group\n",
    "of mainly blood relatives associated with a local territory, usually a\n",
    "village. As in Medieval Europe, therefore, child-rearing is not considered the principal responsibility of parents alone, but involves a far\n",
    "greater number of people, relatives and non-relatives.\n",
    "Clearly, then, to hope to explain human life simply by reference to\n",
    "natural impulses common to all is to ignore the one crucial fact that\n",
    "sociology directs attention to: human behaviour varies according to\n",
    "the social settings in which people find themselves.\n",
    "Individualistic theories\n",
    "What of individualistic explanations? How useful is the argument that\n",
    "behaviour is the product of the psychological make-up of individuals?\n",
    "The employment of this kind of theory is extremely common. For\n",
    "4 An Introduction to Sociological Theories\n",
    "example, success or failure in education is often assumed to be merely\n",
    "a reflection of intelligence: bright children succeed and dim children\n",
    "fail. Criminals are often taken to be people with certain kinds of\n",
    "personality: they are usually seen as morally deficient individuals, lacking any real sense of right or wrong. Unemployed people are equally\n",
    "often condemned as ‘work-shy’, ‘lazy’ or ‘scroungers’ – inadequates\n",
    "who would rather ‘get something for nothing’ than work for it. Suicide\n",
    "is seen as the act of an unstable person – an act undertaken when, as\n",
    "coroners put it, ‘the balance of the mind was disturbed’. This kind of\n",
    "explanation is attractive for many people and has proved particularly\n",
    "resilient to sociological critique. But a closer look shows it to be\n",
    "seriously flawed.\n",
    "If educational achievement is simply a reflection of intelligence then\n",
    "why do children from manual workers’ homes do so badly compared\n",
    "with children from middle-class homes? It is clearly nonsensical to\n",
    "suggest that doing one kind of job rather than another is likely to\n",
    "determine the intelligence of your child. Achievement in education\n",
    "must in some way be influenced by the characteristics of a child’s\n",
    "background.\n",
    "Equally, the fact that the majority of people convicted of a crime\n",
    "come from certain social categories must cast serious doubt on the\n",
    "‘deficient personality’ theory. The conviction rate is highest for young\n",
    "males, especially blacks, who come from manual, working-class\n",
    "or unemployed backgrounds. Can we seriously believe that criminal\n",
    "personalities are likely to be concentrated in such social categories?\n",
    "As in the case of educational achievement, it is clear that the conviction of criminals must somehow be influenced by social factors.\n",
    "Again, is it likely that the million or so people presently unemployed are typically uninterested in working when the vast majority\n",
    "of them have been forced out of their jobs, either by ‘downsizing’ or\n",
    "by the failure of the companies they worked for – as a result of social\n",
    "forces quite outside their control?\n",
    "Suicide would seem to have the strongest case for being explained\n",
    "as a purely psychological act. But if it is simply a question of\n",
    "‘an unsound mind’, then why does the rate of suicide vary between\n",
    "societies? Why does it vary between different groups within the same\n",
    "society? Also, why do the rates within groups and societies remain\n",
    "remarkably constant over time? As in other examples, social factors\n",
    "must be exerting some kind of influence; explanations at the level of\n",
    "the personality are clearly not enough.\n",
    "Variations such as these demonstrate the inadequacy of theories of\n",
    "human behaviour which exclusively emphasize innate natural drives,\n",
    "An Introduction to Sociological Theories 5\n",
    "or the unique psychological make-up of individuals. If nature is at the\n",
    "root of behaviour, why does it vary according to social settings? If we\n",
    "are all different individuals acting according to the dictates of unique\n",
    "psychological influences, why do different people in the same social\n",
    "circumstances behave similarly and in ways others can understand?\n",
    "Clearly there is a social dimension to human existence, which requires\n",
    "sociological theorizing to explain it.\n",
    "All sociological theories thus have in common an emphasis on the\n",
    "way human belief and action is the product of social influences. They\n",
    "differ as to what these influences are, and how they should be investigated and explained. This book is about these differences.\n",
    "We shall now examine three distinct kinds of theory – consensus,\n",
    "conflict and action theories – each of which highlights specific social\n",
    "sources of human behaviour. Though none of the sociologists whose\n",
    "work we will spend the rest of the book examining falls neatly into\n",
    "any one of these three categories of theory, discussing them now will\n",
    "produce two benefits:\n",
    "• it will serve as an accessible introduction to theoretical debates in\n",
    "sociology; and\n",
    "• it will act as useful reference points against which to judge and\n",
    "compare the work of the subject’s major theorists.\n",
    "Society as a structure of rules\n",
    "The influence of culture on behaviour\n",
    "Imagine you live in a big city. How many people do you know well?\n",
    "Twenty? Fifty? A hundred? Now consider how many other people\n",
    "you encounter each day, about whom you know nothing. For example, how many complete strangers do people living in London\n",
    "or Manchester or Birmingham come into contact with each day? On\n",
    "the street, in shops, on buses and trains, in cinemas or night clubs\n",
    "– everyday life in a big city is a constant encounter with complete\n",
    "strangers. Yet even if city dwellers bothered to reflect on this fact,\n",
    "they would not normally leave their homes quaking with dread about\n",
    "how all these hundreds of strangers would behave towards them.\n",
    "Indeed, they hardly, if ever, think about it. Why? Why do we take\n",
    "our ability to cope with strangers so much for granted? It is because\n",
    "nearly all the people we encounter in our everyday lives do behave in\n",
    "ways we expect. We expect bus passengers, shoppers, taxi-drivers,\n",
    "6 An Introduction to Sociological Theories\n",
    "passers-by, and so on, to behave in quite definite ways even though\n",
    "we know nothing about them personally. City dwellers in particular\n",
    "– though it is true of all of us to some extent – routinely enter settings\n",
    "where others are going about their business both expecting not to\n",
    "know them, and yet also expecting to know how they will behave.\n",
    "And, more than this, we are nearly always absolutely right in both\n",
    "respects. We are only surprised if we encounter someone who is not a\n",
    "stranger – ‘Fancy meeting you here! Isn’t it a small world!’ – or if one\n",
    "of these strangers actually does behave strangely – ‘Mummy, why is\n",
    "that man shouting and waving his arms about?’ Why is this? Why do\n",
    "others do what we expect of them? Why is disorder or the unexpected\n",
    "among strangers so rare?\n",
    "Structural-consensus theory\n",
    "One of the traditional ways in which sociologists explain the order\n",
    "and predictability of social life is by regarding human behaviour as\n",
    "learned behaviour. This approach is known – for reasons that will\n",
    "become apparent – as structural-consensus theory. The key process\n",
    "this theory emphasizes is called socialization. This term refers to the\n",
    "way in which human beings learn the kinds of behaviour expected\n",
    "of them in the social settings in which they find themselves. From\n",
    "this point of view, societies differ because the kinds of behaviour\n",
    "considered appropriate in them differ. People in other societies think\n",
    "and behave differently because they have learned different rules about\n",
    "how to behave and think. The same goes for different groups within\n",
    "the same society. The actions and ideas of one group differ from\n",
    "those of another because its members have been socialized into different rules.\n",
    "Consensus sociologists use the term culture to describe the rules\n",
    "that govern thought and behaviour in a society. Culture exists prior\n",
    "to the people who learn it. At birth, humans are confronted by a\n",
    "social world already in existence. Joining this world involves learning\n",
    "‘how things are done’ in it. Only by learning the cultural rules of a\n",
    "society can a human interact with other humans. Because they have\n",
    "been similarly socialized, different individuals will behave similarly.\n",
    "Consensus theory thus argues that a society’s cultural rules determine, or structure, the behaviour of its members, channelling their\n",
    "actions in certain ways rather than others. They do so in much the\n",
    "same way that the physical construction of a building structures the\n",
    "actions of the people inside it. Take the behaviour of students in a\n",
    "An Introduction to Sociological Theories 7\n",
    "school. Once inside the school they will display quite regular patterns\n",
    "of behaviour. They will all walk along corridors, up and down stairs,\n",
    "in and out of classrooms, through doors, and so on. They will, by and\n",
    "large, not attempt to dig through floors, smash through walls, or\n",
    "climb out of windows. Their physical movements are constrained by\n",
    "the school building. Since this affects all the students similarly, their\n",
    "behaviour inside the school will be similar – and will exhibit quite\n",
    "definite patterns. In consensus theory, the same is true of social life.\n",
    "Individuals will behave similarly in the same social settings because\n",
    "they are equally constrained by cultural rules. Though these social\n",
    "structures are not visible in the way physical structures are, those who\n",
    "are socialized into their rules find them comparably determining.\n",
    "The levels at which these cultural rules operate can vary. Some\n",
    "rules, like laws for instance, operate at the level of the whole society\n",
    "and structure the behaviour of everyone who lives in it. Others are\n",
    "much less general, structuring the behaviour of people in quite specific social settings. For example, children in a classroom are expected\n",
    "to behave in an orderly and attentive fashion. In the playground\n",
    "much more license is given them, while away from school their behaviour often bears little resemblance to that expected of them during\n",
    "school hours. Similarly, when police officers or nurses or members of\n",
    "the armed forces are ‘on duty’, certain cultural rules structure their\n",
    "behaviour very rigidly. Out of uniform and off duty these constraints\n",
    "do not apply, though other ones do instead – those governing their\n",
    "behaviour as fathers and mothers, or husbands and wives, for instance.\n",
    "This shows how the theory of a social structure of cultural rules\n",
    "operates. The rules apply not to the individuals themselves, but to the\n",
    "positions in the social structure they occupy. Shoppers, police officers,\n",
    "traffic wardens, schoolteachers or pupils are constrained by the cultural expectations attached to these positions, but only when they\n",
    "occupy them. In other circumstances, in other locations in the social\n",
    "structure – as fathers or mothers, squash players, football supporters,\n",
    "church members, and so on – other rules come into play.\n",
    "Sociologists call positions in a social structure roles. The rules that\n",
    "structure the behaviour of their occupants are called norms. There\n",
    "are some cultural rules that are not attached to any particular role\n",
    "or set of roles. Called values, these are in a sense summaries of approved ways of living, and act as a base from which particular norms\n",
    "spring. So, for example: ‘education should be the key to success’;\n",
    "‘family relationships should be the most important thing to protect’;\n",
    "‘self-help should be the means to individual fulfilment’. All these\n",
    "are values, and they provide general principles from which norms\n",
    "8 An Introduction to Sociological Theories\n",
    "directing behaviour in schools and colleges, in the home and at work\n",
    "are derived.\n",
    "According to this sociological theory, socialization into norms\n",
    "and values produces agreement, or consensus, between people about\n",
    "appropriate behaviour and beliefs without which no human society\n",
    "can survive. This is why it is called structural-consensus theory.\n",
    "Through socialization, cultural rules structure behaviour, guarantee a\n",
    "consensus about expected behaviour, and thereby ensure social order.\n",
    "Clearly, in a complex society there are sometimes going to be competing norms and values. For example, while some people think it is\n",
    "wrong for mothers to go out to work, many women see motherhood\n",
    "at best as a real imposition and at worst as an infringement of their\n",
    "liberty. Children often encourage each other to misbehave at school\n",
    "and disapprove of their peers who refuse to do so. Teachers usually\n",
    "see this very much the other way round! The Tory Party Conference\n",
    "is annually strident in its condemnation of any speaker who criticizes\n",
    "the police. Some young blacks would be equally furious with any\n",
    "of their number who had other than a strongly belligerent attitude\n",
    "towards them.\n",
    "Consensus theorists explain such differences in behaviour and\n",
    "attitude in terms of the existence of alternative cultural influences,\n",
    "characteristic of different social settings. A good example of this\n",
    "emphasis is their approach to educational inequality.\n",
    "Educational inequality: a consensus theory analysis\n",
    "Educational research demonstrates, in the most conclusive fashion,\n",
    "that achievement in education is strongly linked to class membership,\n",
    "gender and ethnic origin. There is overwhelming evidence, for example, that working-class children of similar intelligence to children\n",
    "from middle-class backgrounds achieve far less academically than their\n",
    "middle-class counterparts.\n",
    "To explain this, consensus theorists turn to stock concepts in their\n",
    "approach to social life – norms, values, socialization and culture. Starting from the basic assumption that behaviour and belief are caused by\n",
    "socialization into particular rules, their explanation of working-class\n",
    "underachievement in education seeks to identify:\n",
    "• the cultural influences which propel middle-class children to academic success\n",
    "• the cultural influences which drag working-class children down to\n",
    "mediocrity.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP-Projektrealisierung",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
