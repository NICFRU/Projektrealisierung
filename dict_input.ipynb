{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import networkx as nx\n",
    "import spacy\n",
    "from summa import keywords\n",
    "from summa.summarizer import summarize\n",
    "\n",
    "# Laden des Spacy-Modells\n",
    "import evaluate\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import AutoModelForSeq2SeqLM, PegasusForConditionalGeneration, PegasusTokenizer, AutoTokenizer\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reduce_repetitions(text):\n",
    "    text = re.sub(r'\\.{2,}', '.', text)\n",
    "    text = re.sub(r'\\!{2,}', '!', text)\n",
    "    text = re.sub(r'\\,{2,}', ',', text)\n",
    "    text = re.sub(r'\\;{2,}', ';', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def textrank_extractive(text, compression_rate=0.5,split='\\. '):\n",
    "    # Tokenisierung\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    #doc = nlp(text.replace(\"\\n\\n\", \" \"))\n",
    "\n",
    "    # Split the text at each \". \" that is not followed by a single letter\n",
    "    doc = re.split(fr'(?<!\\b\\w\\w){split}', reduce_repetitions(re.sub(' +', ' ', text.replace(\"\\n\", \" \").replace('-',' ').replace('_',' ').replace(\"\\'\", \"\").replace(\"!\", \".\").replace(\"?\", \".\").replace(\";\", \"\"))))\n",
    "    sentences = [sent for sent in doc if len(sent.replace(\"-\", \" \").split()) > 2]\n",
    "\n",
    "    # Speichern der Spacy-Dokumente der Sätze für spätere Verwendung\n",
    "    sentence_docs = [nlp(sentence) for sentence in sentences]\n",
    "\n",
    "    # Extrahiere Schlüsselsätze mit TextRank\n",
    "    num_sentences = max(1, int(len(sentences) * compression_rate))\n",
    "    extracted_sentences = summarize(text, words=num_sentences, split=True)\n",
    "\n",
    "    # Erzeuge eine Matrix mit den Ähnlichkeiten zwischen den Sätzen\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "    for i, doc_i in enumerate(sentence_docs):\n",
    "        for j, doc_j in enumerate(sentence_docs):\n",
    "            similarity = similarity_function(doc_i, doc_j)\n",
    "            similarity_matrix[i, j] = similarity\n",
    "    \n",
    "    # Konstruiere einen Graphen mit den Sätzen als Knoten und den Ähnlichkeiten als Kanten\n",
    "    graph = nx.from_numpy_array(similarity_matrix)\n",
    "\n",
    "    # Berechne den TextRank-Score für jeden Satz\n",
    "    scores = nx.pagerank_numpy(graph)\n",
    "\n",
    "    # Wähle die besten Sätze basierend auf ihren TextRank-Scores aus\n",
    "    top_sentences = sorted(scores, key=scores.get, reverse=True)[:num_sentences]\n",
    "\n",
    "    # Sortiere die ausgewählten Sätze nach ihrer Position im Text\n",
    "    top_sentences = sorted(top_sentences)\n",
    "\n",
    "    # Gebe die extrahierten Schlüsselsätze zurück\n",
    "    extracted_sentences = [sentences[index] for index in top_sentences]\n",
    "    return extracted_sentences\n",
    "\n",
    "\n",
    "def similarity_function(doc1, doc2):\n",
    "    # Berechne die Cosinus-Ähnlichkeit zwischen den beiden Dokumenten\n",
    "    similarity = doc1.similarity(doc2)\n",
    "    return similarity\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compression_ratio(text, summary):\n",
    "    # Berechne das Verhältnis der Anzahl der Wörter in der Zusammenfassung zur Anzahl der Wörter im Ausgangstext\n",
    "    num_words_text = len(text.split())\n",
    "    num_words_summary = len(summary.split())\n",
    "    ratio = num_words_summary / num_words_text\n",
    "    return ratio\n",
    "\n",
    "\n",
    "\n",
    "def compression(text, compression_rate,split='\\. '):\n",
    "    max_iterations = 20\n",
    "    iterations = 0\n",
    "    #compression_rate -= 0.05\n",
    "    \n",
    "    extracted = textrank_extractive(text, compression_rate,split)\n",
    "    summary = '. '.join(extracted)\n",
    "    compression_rate_renwed = compression_rate\n",
    "\n",
    "\n",
    "    while compression_ratio(text, summary) < compression_rate and iterations < max_iterations:\n",
    "        iterations += 1\n",
    "        compression_rate_renwed += 0.05\n",
    "        if compression_rate_renwed > 1:\n",
    "            compression_rate_renwed = 1\n",
    "        extracted = textrank_extractive(text, compression_rate=compression_rate_renwed)\n",
    "        summary = '. '.join(extracted)\n",
    "    return summary\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_count(text):\n",
    "    tokens = text.split()\n",
    "    return len(tokens)\n",
    "\n",
    "\n",
    "def adjust_length(text):\n",
    "    length = token_count(text)\n",
    "    if length <20:\n",
    "        min_length = length + int(length * 0.05)\n",
    "        max_length = min_length +min_length\n",
    "    elif length <50:\n",
    "        min_length = length + int(length * 0.05)\n",
    "        max_length = min_length +min_length* 0.5\n",
    "    elif length <60:\n",
    "        min_length = length + int(length * 0.05)\n",
    "        max_length = min_length +min_length* 0.4\n",
    "    elif length < 80:\n",
    "        min_length = length + int(length * 0.05)\n",
    "        max_length = min_length + min_length* 0.25\n",
    "    elif length < 100:\n",
    "        min_length = length + int(length * 0.3)\n",
    "        max_length = min_length + 100\n",
    "    else:\n",
    "        min_length = math.ceil(length / 50) * 70\n",
    "        max_length = min_length + 100\n",
    "    return min_length, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_sent(sentenc,splitt=180,split='\\. '):\n",
    "    \n",
    "    sentences = re.split(fr'(?<!\\b\\w\\w){split}', sentenc.lower())\n",
    "\n",
    "    # Erstellen Sie Batches von Sätzen, die weniger als 1024 Tokens enthalten\n",
    "    batches = []\n",
    "    batch = []\n",
    "    batch_len = 0\n",
    "    for sentence in sentences:\n",
    "        \n",
    "        sentence_len = len(tokenizer.tokenize(sentence))\n",
    "        if sentence_len + batch_len > splitt:\n",
    "            if sentence_len < splitt:  # überspringen Sie Sätze, die länger als 256 Tokens sind\n",
    "                batches.append(batch)\n",
    "                batch = [sentence]\n",
    "                batch_len = sentence_len\n",
    "            # wenn ein Satz alleine 1024 Tokens überschreitet, wird er übersprungen\n",
    "        else:\n",
    "            batch.append(sentence)\n",
    "            batch_len += sentence_len\n",
    "    batches.append(batch)\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_rank_algo(dictionary,komp='compression',split='\\. ',random_T=True,column='text'):\n",
    "   \n",
    "    text = dictionary[column].replace(\"\\n\", \" \")\n",
    "    if random_T:\n",
    "        random_value = dictionary[komp]  # Zufälliger Wert zwischen 0.2 und 0.8 auf zwei Stellen nach dem Komma begrenzt\n",
    "    else:\n",
    "        if dictionary['reduction_multiplier']<0.8:\n",
    "            random_value=dictionary['desired_compression_rate']\n",
    "        elif dictionary['reduction_multiplier']<0.9:\n",
    "            random_value=dictionary['reduction_multiplier']\n",
    "        else:\n",
    "            random_value=1\n",
    "    text_rank_text= compression(text.replace(\"\\n\\n\", \" \"),random_value,split)\n",
    "    compression_ratio_value=compression_ratio(text, compression(text,random_value,split))\n",
    "    text=re.sub(' +', ' ', text.replace(\"\\n\", \" \").replace('-',' ').replace('_',' ').replace(\"\\'\", \"\").replace(\"!\", \".\").replace(\"?\", \".\").replace(\";\", \"\"))\n",
    "    text_rank_text=re.sub(' +', ' ', text_rank_text.replace(\"\\n\", \" \").replace('-',' ').replace('_',' ').replace(\"\\'\", \"\").replace(\"!\", \".\").replace(\"?\", \".\").replace(\";\", \"\"))\n",
    "    if random_T:\n",
    "        dictionary['text'] = text\n",
    "        dictionary['text_rank_text'] = text_rank_text\n",
    "        dictionary['tokens_gesamt'] = len(text.split(' '))\n",
    "        dictionary['token_text_rank'] = len(text_rank_text.split(' '))\n",
    "        dictionary['desired_compression_rate'] = random_value\n",
    "        dictionary['text_rank_compression_rate'] = compression_ratio_value\n",
    "    else:\n",
    "        dictionary['text_rank_text_2'] = text_rank_text\n",
    "        dictionary['tokens_gesamt_2'] = len(text.split(' '))\n",
    "        dictionary['token_text_rank_2'] = len(text_rank_text.split(' '))\n",
    "        dictionary['desired_compression_rate_2'] = random_value\n",
    "        dictionary['text_rank_compression_rate_2'] = compression_ratio_value\n",
    "                \n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_class_and_get_model_name(input_dict, class_key):\n",
    "    class_value = input_dict.get(class_key)\n",
    "\n",
    "    if class_value is None:\n",
    "        raise ValueError(f\"'{class_key}' nicht im Eingabedictionary gefunden\")\n",
    "\n",
    "    if class_value == 'Scientific':\n",
    "        model_name = 'NICFRU/bart-base-paraphrasing-science'\n",
    "    elif class_value == 'news':\n",
    "        model_name = 'NICFRU/bart-base-paraphrasing-news'\n",
    "    elif class_value == 'story':\n",
    "        model_name = 'NICFRU/bart-base-paraphrasing-story'\n",
    "    elif class_value == 'reviews':\n",
    "        model_name = 'NICFRU/bart-base-paraphrasing-review'\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "    return model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(dictionary):\n",
    "    model_name=check_class_and_get_model_name(dictionary, 'classification')\n",
    "    global summarizer\n",
    "    global tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    summarizer = pipeline(\"text2text-generation\", model=model_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase_of_text(dictionary,text_name='text',komp_name='reduction_multiplier',split='\\. '):\n",
    "    text_gesamt_list=[]\n",
    "    batch_text_list=[]\n",
    "    \n",
    "    output_text_list=[]\n",
    "    text=dictionary[text_name]\n",
    "    komp = dictionary[komp_name]\n",
    "    for batch in tqdm(batch_sent(text,split=split), desc='Verarbeite Batches'):\n",
    "            # Zusammenfügen der Sätze in einem Batch\n",
    "            #print(batch)\n",
    "            if len(batch):\n",
    "                #print('Läuft')\n",
    "                batch_text = '. '.join(batch)\n",
    "                batch_text += \".\"\n",
    "                batch_text_list.append(batch_text)\n",
    "                min_length_test, max_length_test = adjust_length(batch_text)\n",
    "                ext_summary=summarizer(batch_text, max_length=int(round(max_length_test*komp,0)), min_length=int(round(min_length_test*komp,0)),length_penalty=100,num_beams=2)\n",
    "            # Erstellen Sie einen read_csv für die aktuellen Ergebnisse\n",
    "\n",
    "            text_gesamt_list.append(ext_summary[0]['generated_text'])\n",
    "         \n",
    "    text_gesamt = '. '.join(text_gesamt_list)\n",
    "    actual_compression_rate = len(text_gesamt.split(' '))/len(text.split(' '))*100\n",
    "    dictionary['Zusammenfassung'] = text_gesamt\n",
    "    dictionary['Endgueltige_Kompressionsrate'] = actual_compression_rate\n",
    "    dictionary['länge Zusammenfassung'] = len(text_gesamt.split(' '))\n",
    "    dictionary['länge Ausgangstext'] = len(text.split(' '))\n",
    "    dictionary['batch_texts'] = batch_text_list\n",
    "    dictionary['batch_output'] = text_gesamt_list\n",
    "    return dictionary\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_compression(input_dict, total_tokens_col, current_tokens_col, desired_compression_rate):\n",
    "    input_dict['current_compression_rate'] = input_dict[current_tokens_col] / input_dict[total_tokens_col]\n",
    "    input_dict['compression_difference'] = input_dict[desired_compression_rate] - input_dict['current_compression_rate']\n",
    "    input_dict['reduction_multiplier'] = input_dict[desired_compression_rate] / input_dict['current_compression_rate']\n",
    "    return input_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_text_gen(dictionary,split='\\. ',seed=10):\n",
    "    dictionary_copy=text_rank_algo(dictionary,split=split)\n",
    "    dictionary_copy=calculate_compression(dictionary_copy, 'tokens_gesamt', 'token_text_rank', 'desired_compression_rate')\n",
    "    create_model(dictionary_copy)\n",
    "    dictionary_copy=paraphrase_of_text(dictionary_copy,text_name='text_rank_text',split=split)\n",
    "    dictionary_copy['ent_com_rate']=dictionary_copy['länge Zusammenfassung'] / dictionary_copy['tokens_gesamt']\n",
    "    dictionary_copy=calculate_compression(dictionary_copy, 'tokens_gesamt', 'länge Zusammenfassung', 'desired_compression_rate')\n",
    "    dictionary_copy=text_rank_algo(dictionary_copy,random_T=False,column = 'Zusammenfassung')\n",
    "    dictionary_copy['ent_com_rate']=dictionary_copy['länge Zusammenfassung'] / dictionary_copy['tokens_gesamt']\n",
    "    return dictionary_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=pd.read_csv('data/data_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict=df_test[df_test.classification=='news'].reset_index(drop=True)[['classification','text']].reset_index(drop=True)[['classification', 'text']][0:1].to_dict('records')[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict['compression'] = 0.54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classification': 'news',\n",
       " 'text': 'RJD Chief Lalu Prasad Yadav on Wednesday said that Bihar Chief Minister Nitish Kumar has not asked for his son Tejashwi Yadav\\'s resignation after the CBI raided him over corruption allegations. \"We will not tolerate any disrespect towards him (Nitish Kumar). We have formed the grand alliance, made Nitish CM. Why will we break the alliance,\" Lalu said.',\n",
       " 'compression': 0.54}"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k3/z_svrdgd6sb9lc9bqdfzp_k00000gn/T/ipykernel_52625/1576956366.py:38: DeprecationWarning: networkx.pagerank_numpy is deprecated and will be removed in NetworkX 3.0, use networkx.pagerank instead.\n",
      "  scores = nx.pagerank_numpy(graph)\n",
      "/Users/niclascramer/opt/miniconda3/envs/torch/lib/python3.9/site-packages/networkx/algorithms/link_analysis/pagerank_alg.py:354: FutureWarning: google_matrix will return an np.ndarray instead of a np.matrix in\n",
      "NetworkX version 3.0.\n",
      "  M = google_matrix(\n",
      "Verarbeite Batches: 100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classification': 'news',\n",
       " 'text': 'RJD Chief Lalu Prasad Yadav on Wednesday said that Bihar Chief Minister Nitish Kumar has not asked for his son Tejashwi Yadavs resignation after the CBI raided him over corruption allegations. \"We will not tolerate any disrespect towards him (Nitish Kumar). We have formed the grand alliance, made Nitish CM. Why will we break the alliance,\" Lalu said.',\n",
       " 'compression': 0.54,\n",
       " 'text_rank_text': 'RJD Chief Lalu Prasad Yadav on Wednesday said that Bihar Chief Minister Nitish Kumar has not asked for his son Tejashwi Yadavs resignation after the CBI raided him over corruption allegations. \"We will not tolerate any disrespect towards him (Nitish Kumar). We have formed the grand alliance, made Nitish CM. Why will we break the alliance,\" Lalu said.',\n",
       " 'tokens_gesamt': 58,\n",
       " 'token_text_rank': 58,\n",
       " 'desired_compression_rate': 0.54,\n",
       " 'text_rank_compression_rate': 1.0,\n",
       " 'current_compression_rate': 0.3793103448275862,\n",
       " 'compression_difference': 0.16068965517241385,\n",
       " 'reduction_multiplier': 1.4236363636363638,\n",
       " 'Zusammenfassung': 'rjd chief lalu prasad yadav on wednesday said that bihar chief minister nitish kumar has not asked for his son tejashwi yad',\n",
       " 'Endgueltige_Kompressionsrate': 37.93103448275862,\n",
       " 'länge Zusammenfassung': 22,\n",
       " 'länge Ausgangstext': 58,\n",
       " 'batch_texts': ['rjd chief lalu prasad yadav on wednesday said that bihar chief minister nitish kumar has not asked for his son tejashwi yadavs resignation after the cbi raided him over corruption allegations. \"we will not tolerate any disrespect towards him (nitish kumar). we have formed the grand alliance, made nitish cm. why will we break the alliance,\" lalu said..'],\n",
       " 'batch_output': ['rjd chief lalu prasad yadav on wednesday said that bihar chief minister nitish kumar has not asked for his son tejashwi yad'],\n",
       " 'ent_com_rate': 0.3793103448275862,\n",
       " 'text_rank_text_2': 'rjd chief lalu prasad yadav on wednesday said that bihar chief minister nitish kumar has not asked for his son tejashwi yad',\n",
       " 'tokens_gesamt_2': 22,\n",
       " 'token_text_rank_2': 22,\n",
       " 'desired_compression_rate_2': 1,\n",
       " 'text_rank_compression_rate_2': 1.0}"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=execute_text_gen(test_dict,split='\\. ',seed=10)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63be3639594356bc87fe58051c1d1c5221c23a964c31c0e05d208c4974bedf26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
