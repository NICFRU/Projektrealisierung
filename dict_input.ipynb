{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import networkx as nx\n",
    "import spacy\n",
    "from summa import keywords\n",
    "from summa.summarizer import summarize\n",
    "\n",
    "# Laden des Spacy-Modells\n",
    "import evaluate\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import AutoModelForSeq2SeqLM, PegasusForConditionalGeneration, PegasusTokenizer, AutoTokenizer\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_repetitions(text):\n",
    "    # Wir benutzen reguläre Ausdrücke (regex), um Wiederholungen von Zeichen zu reduzieren.\n",
    "    text = re.sub(r'\\.{2,}', '.', text)  # Ersetzt zwei oder mehr Punkte durch einen Punkt.\n",
    "    text = re.sub(r'\\!{2,}', '!', text)  # Ersetzt zwei oder mehr Ausrufezeichen durch ein Ausrufezeichen.\n",
    "    text = re.sub(r'\\,{2,}', ',', text)  # Ersetzt zwei oder mehr Kommas durch ein Komma.\n",
    "    text = re.sub(r'\\;{2,}', ';', text)  # Ersetzt zwei oder mehr Semikolons durch ein Semikolon.\n",
    "    return text  # Gibt den bereinigten Text zurück.\n",
    "\n",
    "def textrank_extractive(text, compression_rate=0.5,split='\\. '):\n",
    "    # Hier verwenden wir Spacy, um den Text in Sätze zu zerlegen und zu tokenisieren.\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    doc = re.split(fr'(?<!\\b\\w\\w){split}', reduce_repetitions(re.sub(' +', ' ', text.replace(\"\\n\", \" \").replace('-',' ').replace('_',' ').replace(\"\\'\", \"\").replace(\"!\", \".\").replace(\"?\", \".\").replace(\";\", \"\"))))\n",
    "    sentences = [sent for sent in doc if len(sent.replace(\"-\", \" \").split()) > 2]\n",
    "    sentence_docs = [nlp(sentence) for sentence in sentences]\n",
    "\n",
    "    # Hier verwenden wir TextRank, um wichtige Sätze zu extrahieren.\n",
    "    num_sentences = max(1, int(len(sentences) * compression_rate))\n",
    "    extracted_sentences = summarize(text, words=num_sentences, split=True)\n",
    "\n",
    "    # Wir bauen eine Matrix auf, die die Ähnlichkeit zwischen den Sätzen misst.\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "    for i, doc_i in enumerate(sentence_docs):\n",
    "        for j, doc_j in enumerate(sentence_docs):\n",
    "            similarity = similarity_function(doc_i, doc_j)\n",
    "            similarity_matrix[i, j] = similarity\n",
    "\n",
    "    # Hier erstellen wir einen Graphen, in dem die Sätze die Knoten und die Ähnlichkeiten die Kanten sind.\n",
    "    graph = nx.from_numpy_array(similarity_matrix)\n",
    "\n",
    "    # Nun berechnen wir den TextRank-Score für jeden Satz.\n",
    "    scores = nx.pagerank_numpy(graph)\n",
    "\n",
    "    # Wir wählen die besten Sätze basierend auf ihren TextRank-Scores aus.\n",
    "    top_sentences = sorted(scores, key=scores.get, reverse=True)[:num_sentences]\n",
    "\n",
    "    # Die ausgewählten Sätze werden nach ihrer Position im Text sortiert.\n",
    "    top_sentences = sorted(top_sentences)\n",
    "\n",
    "    # Schließlich geben wir die extrahierten Schlüsselsätze zurück.\n",
    "    extracted_sentences = [sentences[index] for index in top_sentences]\n",
    "    return extracted_sentences\n",
    "\n",
    "def similarity_function(doc1, doc2):\n",
    "    # Wir berechnen die Cosinus-Ähnlichkeit zwischen den beiden Dokumenten.\n",
    "    similarity = doc1.similarity(doc2)\n",
    "    return similarity\n",
    "\n",
    "def compression_ratio(text, summary):\n",
    "    # Wir berechnen das Verhältnis der Anzahl der Wörter in der Zusammenfassung zur Anzahl der Wörter im Ausgangstext.\n",
    "    num_words_text = len(text.split())\n",
    "    num_words_summary = len(summary.split())\n",
    "    ratio = num_words_summary / num_words_text\n",
    "    return ratio\n",
    "\n",
    "def compression(text, compression_rate,split='\\. '):\n",
    "    max_iterations = 20\n",
    "    iterations = 0\n",
    "    extracted = textrank_extractive(text, compression_rate,split)\n",
    "    summary = '. '.join(extracted)\n",
    "    compression_rate_renwed = compression_rate\n",
    "\n",
    "    # Wenn das Kompressionsverhältnis kleiner ist als die gewünschte Rate, versuchen wir, das Kompressionsverhältnis zu erhöhen.\n",
    "    while compression_ratio(text, summary) < compression_rate and iterations < max_iterations:\n",
    "        iterations += 1\n",
    "        compression_rate_renwed += 0.05\n",
    "        if compression_rate_renwed > 1:\n",
    "            compression_rate_renwed = 1\n",
    "        extracted = textrank_extractive(text, compression_rate=compression_rate_renwed)\n",
    "        summary = '. '.join(extracted)\n",
    "    return summary  # Gibt die komprimierte Zusammenfassung zurück.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_count(text):\n",
    "    # Wir zerlegen den Text in einzelne Worte (Token) und zählen diese.\n",
    "    tokens = text.split()\n",
    "    return len(tokens)  # Gibt die Anzahl der Wörter (Token) im Text zurück.\n",
    "\n",
    "def adjust_length(text):\n",
    "    # Zuerst zählen wir die Anzahl der Wörter (Token) im Text.\n",
    "    length = token_count(text)\n",
    "    \n",
    "    # Wir passen die Mindest- und Maxmimallänge des Textes basierend auf der aktuellen Länge an.\n",
    "    # Die genauen Werte sind hier variabel und hängen von der Länge des Textes ab.\n",
    "    \n",
    "    if length <20:\n",
    "        # Wenn der Text weniger als 20 Wörter hat, erhöhen wir die Mindestlänge um 5% der aktuellen Länge.\n",
    "        # Die maximale Länge wird dann auf das Doppelte der Mindestlänge gesetzt.\n",
    "        min_length = length + int(length * 0.05)\n",
    "        max_length = min_length +min_length\n",
    "    elif length <50:\n",
    "        # Wenn der Text zwischen 20 und 50 Wörtern hat, verwenden wir die gleichen Regeln wie zuvor, \n",
    "        # aber die maximale Länge wird auf das 1,5-fache der Mindestlänge gesetzt.\n",
    "        min_length = length + int(length * 0.05)\n",
    "        max_length = min_length +min_length* 0.5\n",
    "    elif length <60:\n",
    "        # Bei Texten zwischen 50 und 60 Wörtern erhöhen wir die Mindestlänge um 5% der aktuellen Länge.\n",
    "        # Die maximale Länge wird dann auf das 1,4-fache der Mindestlänge gesetzt.\n",
    "        min_length = length + int(length * 0.05)\n",
    "        max_length = min_length +min_length* 0.4\n",
    "    elif length < 80:\n",
    "        # Bei Texten zwischen 60 und 80 Wörtern erhöhen wir die Mindestlänge um 5% der aktuellen Länge.\n",
    "        # Die maximale Länge wird dann auf das 1,25-fache der Mindestlänge gesetzt.\n",
    "        min_length = length + int(length * 0.05)\n",
    "        max_length = min_length + min_length* 0.25\n",
    "    elif length < 100:\n",
    "        # Bei Texten zwischen 80 und 100 Wörtern erhöhen wir die Mindestlänge um 30% der aktuellen Länge.\n",
    "        # Die maximale Länge wird dann auf die Mindestlänge plus 100 Wörter gesetzt.\n",
    "        min_length = length + int(length * 0.3)\n",
    "        max_length = min_length + 100\n",
    "    else:\n",
    "        # Bei Texten mit 100 Wörtern oder mehr setzen wir die Mindestlänge auf das nächste Vielfache von 70,\n",
    "        # das größer als die aktuelle Länge geteilt durch 50 ist. \n",
    "        # Die maximale Länge wird dann auf die Mindestlänge plus 100 Wörter gesetzt.\n",
    "        min_length = math.ceil(length / 50) * 70\n",
    "        max_length = min_length + 100\n",
    "\n",
    "    # Wir geben die berechneten minimalen und maximalen Längen zurück.\n",
    "    return min_length, max_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_sent(sentenc,splitt=180,split='\\. '):\n",
    "    # Teile den eingegebenen Text in einzelne Sätze auf, wobei jeder Satz durch \". \" getrennt ist.\n",
    "    # Hierbei wird sichergestellt, dass das \". \" nicht auf ein einzelnes Wort folgt.\n",
    "    sentences = re.split(fr'(?<!\\b\\w\\w){split}', sentenc.lower())\n",
    "\n",
    "    # Initialisierung der Batches und der aktuellen Batch-Liste sowie der aktuellen Batch-Länge.\n",
    "    batches = []\n",
    "    batch = []\n",
    "    batch_len = 0\n",
    "    \n",
    "    # Durchlaufen Sie jeden Satz in den Sätzen.\n",
    "    for sentence in sentences:\n",
    "        # Berechnen Sie die Anzahl der Tokens im Satz.\n",
    "        sentence_len = len(tokenizer.tokenize(sentence))\n",
    "        \n",
    "        # Wenn die Hinzufügung des aktuellen Satzes die maximale Batch-Länge überschreitet...\n",
    "        if sentence_len + batch_len > splitt:\n",
    "            # ...und wenn der aktuelle Satz weniger als die maximale Batch-Länge hat...\n",
    "            if sentence_len < splitt:  \n",
    "                # ...füge die aktuelle Batch-Liste zu den Batches hinzu...\n",
    "                batches.append(batch)\n",
    "                # ...und beginne eine neue Batch-Liste mit dem aktuellen Satz.\n",
    "                batch = [sentence]\n",
    "                # Die aktuelle Batch-Länge wird auf die Länge des aktuellen Satzes gesetzt.\n",
    "                batch_len = sentence_len\n",
    "            # Sätze, die länger als die maximale Batch-Länge sind, werden übersprungen.\n",
    "        else:\n",
    "            # Wenn der aktuelle Satz zur aktuellen Batch-Liste hinzugefügt werden kann, ohne die maximale Batch-Länge zu überschreiten...\n",
    "            # ...füge den Satz zur aktuellen Batch-Liste hinzu...\n",
    "            batch.append(sentence)\n",
    "            # ...und erhöhe die aktuelle Batch-Länge um die Länge des aktuellen Satzes.\n",
    "            batch_len += sentence_len\n",
    "    \n",
    "    # Füge die letzte Batch-Liste zu den Batches hinzu.\n",
    "    batches.append(batch)\n",
    "\n",
    "    # Die Funktion gibt die erstellten Batches zurück.\n",
    "    return batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_rank_algo(dictionary, komp='compression', split='\\\\. ', random_T=True, column='text'):\n",
    "    # Text aus dem Wörterbuch extrahieren und bearbeiten\n",
    "    text = dictionary[column].replace(\"\\n\", \" \")\n",
    "    if random_T:\n",
    "        random_value = dictionary[komp]\n",
    "    else:\n",
    "        if dictionary['reduction_multiplier'] < 0.8:\n",
    "            random_value = dictionary['desired_compression_rate']\n",
    "        elif dictionary['reduction_multiplier'] < 0.9:\n",
    "            random_value = dictionary['reduction_multiplier']\n",
    "        else:\n",
    "            random_value = 1\n",
    "\n",
    "    # Durchführen der Textkompression\n",
    "    text_rank_text = compression(text.replace(\"\\n\\n\", \" \"), random_value, split)\n",
    "    compression_ratio_value = compression_ratio(text, compression(text, random_value, split))\n",
    "\n",
    "    # Weitere Textbearbeitung\n",
    "    text = re.sub(' +', ' ', text.replace(\"\\n\", \" \").replace('-',' ').replace('_',' ').replace(\"\\'\", \"\").replace(\"!\", \".\").replace(\"?\", \".\").replace(\";\", \"\"))\n",
    "    text_rank_text = re.sub(' +', ' ', text_rank_text.replace(\"\\n\", \" \").replace('-',' ').replace('_',' ').replace(\"\\'\", \"\").replace(\"!\", \".\").replace(\"?\", \".\").replace(\";\", \"\"))\n",
    "\n",
    "    # Hinzufügen von Ergebnissen zum Wörterbuch\n",
    "    if random_T:\n",
    "        dictionary['text'] = text\n",
    "        dictionary['text_rank_text'] = text_rank_text\n",
    "        dictionary['tokens_gesamt'] = len(text.split(' '))\n",
    "        dictionary['token_text_rank'] = len(text_rank_text.split(' '))\n",
    "        dictionary['desired_compression_rate'] = random_value\n",
    "        dictionary['text_rank_compression_rate'] = compression_ratio_value\n",
    "    else:\n",
    "        dictionary['text_rank_text_2'] = text_rank_text\n",
    "        dictionary['tokens_gesamt_2'] = len(text.split(' '))\n",
    "        dictionary['token_text_rank_2'] = len(text_rank_text.split(' '))\n",
    "        dictionary['desired_compression_rate_2'] = random_value\n",
    "        dictionary['text_rank_compression_rate_2'] = compression_ratio_value\n",
    "\n",
    "    # Rückgabe des aktualisierten Wörterbuchs\n",
    "    return dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_class_and_get_model_name(input_dict, class_key):\n",
    "    # Aus dem Wörterbuch den Wert des gegebenen Schlüssels abrufen\n",
    "    class_value = input_dict.get(class_key)\n",
    "\n",
    "    # Überprüfen, ob der Wert existiert\n",
    "    if class_value is None:\n",
    "        raise ValueError(f\"'{class_key}' nicht im Eingabedictionary gefunden\")\n",
    "\n",
    "    # Entsprechend dem Wert den Modellnamen zuweisen\n",
    "    if class_value == 'Scientific':\n",
    "        model_name = 'NICFRU/bart-base-paraphrasing-science'\n",
    "    elif class_value == 'news':\n",
    "        model_name = 'NICFRU/bart-base-paraphrasing-news'\n",
    "    elif class_value == 'story':\n",
    "        model_name = 'NICFRU/bart-base-paraphrasing-story'\n",
    "    elif class_value == 'reviews':\n",
    "        model_name = 'NICFRU/bart-base-paraphrasing-review'\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "    # Rückgabe des Modellnamens\n",
    "    return model_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(dictionary):\n",
    "    # Nutzen Sie die check_class_and_get_model_name Funktion, um den Namen des Modells zu ermitteln\n",
    "    model_name = check_class_and_get_model_name(dictionary, 'classification')\n",
    "    \n",
    "    # Definieren Sie die Tokenizer und Summarizer als globale Variablen, damit sie außerhalb der Funktion verwendet werden können\n",
    "    global tokenizer, summarizer\n",
    "\n",
    "    # Laden Sie den Tokenizer und das Modell von Huggingface\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    summarizer = pipeline(\"text2text-generation\", model=model_name)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase_of_text(dictionary, text_name='text', komp_name='reduction_multiplier', split='\\. '):\n",
    "    # Initialisierung der Listen\n",
    "    text_gesamt_list = []\n",
    "    batch_text_list = []\n",
    "\n",
    "    # Auslesen der Text- und Kompressionsinformationen aus dem Wörterbuch\n",
    "    text = dictionary[text_name]\n",
    "    komp = dictionary[komp_name]\n",
    "\n",
    "    # Iteration über die Batches des Texts\n",
    "    for batch in tqdm(batch_sent(text, split=split), desc='Verarbeite Batches'):\n",
    "        # Überprüfen Sie, ob der aktuelle Batch nicht leer ist\n",
    "        if len(batch):\n",
    "            # Zusammenfügen der Sätze in einem Batch\n",
    "            batch_text = '. '.join(batch)\n",
    "            batch_text += \".\"\n",
    "            batch_text_list.append(batch_text)\n",
    "            \n",
    "            # Anpassung der Länge und Generierung der Zusammenfassung\n",
    "            min_length_test, max_length_test = adjust_length(batch_text)\n",
    "            ext_summary = summarizer(batch_text, max_length=int(round(max_length_test * komp, 0)), min_length=int(round(min_length_test * komp, 0)), length_penalty=100, num_beams=2)\n",
    "\n",
    "            # Hinzufügen der generierten Zusammenfassung zur Gesamtliste\n",
    "            text_gesamt_list.append(ext_summary[0]['generated_text'])\n",
    "\n",
    "    # Erstellen der Gesamtzusammenfassung und Berechnung der endgültigen Kompressionsrate\n",
    "    text_gesamt = '. '.join(text_gesamt_list)\n",
    "    actual_compression_rate = len(text_gesamt.split(' ')) / len(text.split(' ')) * 100\n",
    "\n",
    "    # Aktualisierung des Wörterbuchs mit den neuen Informationen\n",
    "    dictionary['Zusammenfassung'] = text_gesamt\n",
    "    dictionary['Endgueltige_Kompressionsrate'] = actual_compression_rate\n",
    "    dictionary['länge Zusammenfassung'] = len(text_gesamt.split(' '))\n",
    "    dictionary['länge Ausgangstext'] = len(text.split(' '))\n",
    "    dictionary['batch_texts'] = batch_text_list\n",
    "    dictionary['batch_output'] = text_gesamt_list\n",
    "\n",
    "    return dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_compression(input_dict, total_tokens_col, current_tokens_col, desired_compression_rate):\n",
    "    # Berechne die aktuelle Kompressionsrate\n",
    "    input_dict['current_compression_rate'] = input_dict[current_tokens_col] / input_dict[total_tokens_col]\n",
    "    # Berechne die Differenz zur gewünschten Kompressionsrate\n",
    "    input_dict['compression_difference'] = input_dict[desired_compression_rate] - input_dict['current_compression_rate']\n",
    "    # Berechne den Reduktionsmultiplikator\n",
    "    input_dict['reduction_multiplier'] = input_dict[desired_compression_rate] / input_dict['current_compression_rate']\n",
    "    return input_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_text_gen(dictionary, split='\\. ', seed=10):\n",
    "    # Kopieren Sie das Wörterbuch für Manipulationen\n",
    "    dictionary_copy = dictionary.copy()\n",
    "\n",
    "    # Führe den Text-Rank-Algorithmus aus\n",
    "    dictionary_copy = text_rank_algo(dictionary_copy, split=split)\n",
    "\n",
    "    # Berechne die Kompression\n",
    "    dictionary_copy = calculate_compression(dictionary_copy, 'tokens_gesamt', 'token_text_rank', 'desired_compression_rate')\n",
    "\n",
    "    # Erstelle das Modell\n",
    "    create_model(dictionary_copy)\n",
    "\n",
    "    # Paraphrasiere den Text\n",
    "    dictionary_copy = paraphrase_of_text(dictionary_copy, text_name='text_rank_text', split=split)\n",
    "\n",
    "    # Berechne die endgültige Kompressionsrate\n",
    "    dictionary_copy['ent_com_rate'] = dictionary_copy['länge Zusammenfassung'] / dictionary_copy['tokens_gesamt']\n",
    "\n",
    "    # Berechne erneut die Kompression\n",
    "    dictionary_copy = calculate_compression(dictionary_copy, 'tokens_gesamt', 'länge Zusammenfassung', 'desired_compression_rate')\n",
    "\n",
    "    # Führe erneut den Text-Rank-Algorithmus aus\n",
    "    dictionary_copy = text_rank_algo(dictionary_copy, random_T=False, column='Zusammenfassung')\n",
    "\n",
    "    # Berechne die endgültige Kompressionsrate erneut\n",
    "    dictionary_copy['ent_com_rate'] = dictionary_copy['länge Zusammenfassung'] / dictionary_copy['tokens_gesamt']\n",
    "\n",
    "    return dictionary_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=pd.read_csv('data/data_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict=df_test[df_test.classification=='news'].reset_index(drop=True)[['classification','text']].reset_index(drop=True)[['classification', 'text']][0:1].to_dict('records')[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict['compression'] = 0.54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classification': 'news',\n",
       " 'text': 'RJD Chief Lalu Prasad Yadav on Wednesday said that Bihar Chief Minister Nitish Kumar has not asked for his son Tejashwi Yadav\\'s resignation after the CBI raided him over corruption allegations. \"We will not tolerate any disrespect towards him (Nitish Kumar). We have formed the grand alliance, made Nitish CM. Why will we break the alliance,\" Lalu said.',\n",
       " 'compression': 0.54}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k3/z_svrdgd6sb9lc9bqdfzp_k00000gn/T/ipykernel_52625/1576956366.py:38: DeprecationWarning: networkx.pagerank_numpy is deprecated and will be removed in NetworkX 3.0, use networkx.pagerank instead.\n",
      "  scores = nx.pagerank_numpy(graph)\n",
      "/Users/niclascramer/opt/miniconda3/envs/torch/lib/python3.9/site-packages/networkx/algorithms/link_analysis/pagerank_alg.py:354: FutureWarning: google_matrix will return an np.ndarray instead of a np.matrix in\n",
      "NetworkX version 3.0.\n",
      "  M = google_matrix(\n",
      "Verarbeite Batches: 100%|██████████| 1/1 [00:01<00:00,  1.25s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'classification': 'news',\n",
       " 'text': 'RJD Chief Lalu Prasad Yadav on Wednesday said that Bihar Chief Minister Nitish Kumar has not asked for his son Tejashwi Yadavs resignation after the CBI raided him over corruption allegations. \"We will not tolerate any disrespect towards him (Nitish Kumar). We have formed the grand alliance, made Nitish CM. Why will we break the alliance,\" Lalu said.',\n",
       " 'compression': 0.54,\n",
       " 'text_rank_text': 'RJD Chief Lalu Prasad Yadav on Wednesday said that Bihar Chief Minister Nitish Kumar has not asked for his son Tejashwi Yadavs resignation after the CBI raided him over corruption allegations. \"We will not tolerate any disrespect towards him (Nitish Kumar). We have formed the grand alliance, made Nitish CM. Why will we break the alliance,\" Lalu said.',\n",
       " 'tokens_gesamt': 58,\n",
       " 'token_text_rank': 58,\n",
       " 'desired_compression_rate': 0.54,\n",
       " 'text_rank_compression_rate': 1.0,\n",
       " 'current_compression_rate': 0.3793103448275862,\n",
       " 'compression_difference': 0.16068965517241385,\n",
       " 'reduction_multiplier': 1.4236363636363638,\n",
       " 'Zusammenfassung': 'rjd chief lalu prasad yadav on wednesday said that bihar chief minister nitish kumar has not asked for his son tejashwi yad',\n",
       " 'Endgueltige_Kompressionsrate': 37.93103448275862,\n",
       " 'länge Zusammenfassung': 22,\n",
       " 'länge Ausgangstext': 58,\n",
       " 'batch_texts': ['rjd chief lalu prasad yadav on wednesday said that bihar chief minister nitish kumar has not asked for his son tejashwi yadavs resignation after the cbi raided him over corruption allegations. \"we will not tolerate any disrespect towards him (nitish kumar). we have formed the grand alliance, made nitish cm. why will we break the alliance,\" lalu said..'],\n",
       " 'batch_output': ['rjd chief lalu prasad yadav on wednesday said that bihar chief minister nitish kumar has not asked for his son tejashwi yad'],\n",
       " 'ent_com_rate': 0.3793103448275862,\n",
       " 'text_rank_text_2': 'rjd chief lalu prasad yadav on wednesday said that bihar chief minister nitish kumar has not asked for his son tejashwi yad',\n",
       " 'tokens_gesamt_2': 22,\n",
       " 'token_text_rank_2': 22,\n",
       " 'desired_compression_rate_2': 1,\n",
       " 'text_rank_compression_rate_2': 1.0}"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test=execute_text_gen(test_dict,split='\\. ',seed=10)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63be3639594356bc87fe58051c1d1c5221c23a964c31c0e05d208c4974bedf26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
