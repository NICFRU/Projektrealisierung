{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en\n",
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install tensorflow\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import spacy\n",
    "from summa import keywords\n",
    "from summa.summarizer import summarize\n",
    "\n",
    "# Laden des Spacy-Modells\n",
    "import evaluate\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import AutoModelForSeq2SeqLM, PegasusForConditionalGeneration, PegasusTokenizer, AutoTokenizer\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classification</th>\n",
       "      <th>text</th>\n",
       "      <th>sentences</th>\n",
       "      <th>num_sentences</th>\n",
       "      <th>words</th>\n",
       "      <th>num_words</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>stops</th>\n",
       "      <th>num_stops</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Scientific</td>\n",
       "      <td>the interest in anchoring phenomena and phenom...</td>\n",
       "      <td>['the interest in anchoring phenomena and phen...</td>\n",
       "      <td>222</td>\n",
       "      <td>['interest', 'anchoring', 'phenomena', 'phenom...</td>\n",
       "      <td>3533</td>\n",
       "      <td>['interest', 'anchor', 'phenomenon', 'phenomen...</td>\n",
       "      <td>[the, in, and, in, has, been, by, their, in, a...</td>\n",
       "      <td>2426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Scientific</td>\n",
       "      <td>galaxy clusters , as the largest peaks in the ...</td>\n",
       "      <td>['galaxy clusters , as the largest peaks in th...</td>\n",
       "      <td>372</td>\n",
       "      <td>['galaxy', 'clusters', 'largest', 'peaks', 'co...</td>\n",
       "      <td>8982</td>\n",
       "      <td>['galaxy', 'cluster', 'large', 'peak', 'cosmic...</td>\n",
       "      <td>[as, the, in, the, an, in, and, in, for, and, ...</td>\n",
       "      <td>5768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Scientific</td>\n",
       "      <td>quantum correlations between components of a s...</td>\n",
       "      <td>['quantum correlations between components of a...</td>\n",
       "      <td>138</td>\n",
       "      <td>['quantum', 'correlations', 'components', 'sys...</td>\n",
       "      <td>2451</td>\n",
       "      <td>['quantum', 'correlation', 'component', 'syste...</td>\n",
       "      <td>[between, of, a, or, are, on, the, of, whole, ...</td>\n",
       "      <td>1780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Scientific</td>\n",
       "      <td>methanol masers are often found in star - form...</td>\n",
       "      <td>['methanol masers are often found in star - fo...</td>\n",
       "      <td>141</td>\n",
       "      <td>['methanol', 'masers', 'found', 'star', 'formi...</td>\n",
       "      <td>3232</td>\n",
       "      <td>['methanol', 'maser', 'find', 'star', 'form', ...</td>\n",
       "      <td>[are, often, in, are, two, of, to, i, most, th...</td>\n",
       "      <td>1940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Scientific</td>\n",
       "      <td>interdisciplinary research has recently gained...</td>\n",
       "      <td>['interdisciplinary research has recently gain...</td>\n",
       "      <td>88</td>\n",
       "      <td>['interdisciplinary', 'research', 'recently', ...</td>\n",
       "      <td>2695</td>\n",
       "      <td>['interdisciplinary', 'research', 'recently', ...</td>\n",
       "      <td>[has, a, in, the, of, to, the, in, was, to, tw...</td>\n",
       "      <td>1997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  classification                                               text  \\\n",
       "0     Scientific  the interest in anchoring phenomena and phenom...   \n",
       "1     Scientific  galaxy clusters , as the largest peaks in the ...   \n",
       "2     Scientific  quantum correlations between components of a s...   \n",
       "3     Scientific  methanol masers are often found in star - form...   \n",
       "4     Scientific  interdisciplinary research has recently gained...   \n",
       "\n",
       "                                           sentences  num_sentences  \\\n",
       "0  ['the interest in anchoring phenomena and phen...            222   \n",
       "1  ['galaxy clusters , as the largest peaks in th...            372   \n",
       "2  ['quantum correlations between components of a...            138   \n",
       "3  ['methanol masers are often found in star - fo...            141   \n",
       "4  ['interdisciplinary research has recently gain...             88   \n",
       "\n",
       "                                               words  num_words  \\\n",
       "0  ['interest', 'anchoring', 'phenomena', 'phenom...       3533   \n",
       "1  ['galaxy', 'clusters', 'largest', 'peaks', 'co...       8982   \n",
       "2  ['quantum', 'correlations', 'components', 'sys...       2451   \n",
       "3  ['methanol', 'masers', 'found', 'star', 'formi...       3232   \n",
       "4  ['interdisciplinary', 'research', 'recently', ...       2695   \n",
       "\n",
       "                                              lemmas  \\\n",
       "0  ['interest', 'anchor', 'phenomenon', 'phenomen...   \n",
       "1  ['galaxy', 'cluster', 'large', 'peak', 'cosmic...   \n",
       "2  ['quantum', 'correlation', 'component', 'syste...   \n",
       "3  ['methanol', 'maser', 'find', 'star', 'form', ...   \n",
       "4  ['interdisciplinary', 'research', 'recently', ...   \n",
       "\n",
       "                                               stops  num_stops  \n",
       "0  [the, in, and, in, has, been, by, their, in, a...       2426  \n",
       "1  [as, the, in, the, an, in, and, in, for, and, ...       5768  \n",
       "2  [between, of, a, or, are, on, the, of, whole, ...       1780  \n",
       "3  [are, often, in, are, two, of, to, i, most, th...       1940  \n",
       "4  [has, a, in, the, of, to, the, in, was, to, tw...       1997  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train=pd.read_csv('data/data_train_with_features.csv')\n",
    "df_train = df_train.drop('Unnamed: 0', axis=1)\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def textrank_extractive(text, compression_rate=0.5):\n",
    "    # Tokenisierung\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "    # Speichern der Spacy-Dokumente der Sätze für spätere Verwendung\n",
    "    sentence_docs = [nlp(sentence) for sentence in sentences]\n",
    "\n",
    "    # Extrahiere Schlüsselsätze mit TextRank\n",
    "    num_sentences = max(1, int(len(sentences) * compression_rate))\n",
    "    extracted_sentences = summarize(text, words=num_sentences, split=True)\n",
    "\n",
    "    # Erzeuge eine Matrix mit den Ähnlichkeiten zwischen den Sätzen\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "    for i, doc_i in enumerate(sentence_docs):\n",
    "        for j, doc_j in enumerate(sentence_docs):\n",
    "            similarity = similarity_function(doc_i, doc_j)\n",
    "            similarity_matrix[i, j] = similarity\n",
    "\n",
    "    # Konstruiere einen Graphen mit den Sätzen als Knoten und den Ähnlichkeiten als Kanten\n",
    "    graph = nx.from_numpy_array(similarity_matrix)\n",
    "\n",
    "    # Berechne den TextRank-Score für jeden Satz\n",
    "    scores = nx.pagerank(graph)\n",
    "\n",
    "    # Wähle die besten Sätze basierend auf ihren TextRank-Scores aus\n",
    "    top_sentences = sorted(scores, key=scores.get, reverse=True)[:num_sentences]\n",
    "\n",
    "    # Sortiere die ausgewählten Sätze nach ihrer Position im Text\n",
    "    top_sentences = sorted(top_sentences)\n",
    "\n",
    "    # Gebe die extrahierten Schlüsselsätze zurück\n",
    "    extracted_sentences = [sentences[index] for index in top_sentences]\n",
    "    return extracted_sentences\n",
    "\n",
    "\n",
    "def similarity_function(doc1, doc2):\n",
    "    # Berechne die Cosinus-Ähnlichkeit zwischen den beiden Dokumenten\n",
    "    similarity = doc1.similarity(doc2)\n",
    "    return similarity\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compression_ratio(text, summary):\n",
    "    # Berechne das Verhältnis der Anzahl der Wörter in der Zusammenfassung zur Anzahl der Wörter im Ausgangstext\n",
    "    num_words_text = len(text.split())\n",
    "    num_words_summary = len(summary.split())\n",
    "    ratio = num_words_summary / num_words_text\n",
    "    return ratio\n",
    "\n",
    "\n",
    "\n",
    "def compression(text, compression_rate):\n",
    "    max_iterations = 20\n",
    "    iterations = 0\n",
    "    #compression_rate -= 0.05\n",
    "    \n",
    "    extracted = textrank_extractive(text, compression_rate)\n",
    "    summary = ' '.join(extracted)\n",
    "    compression_rate_renwed = compression_rate\n",
    "\n",
    "\n",
    "    while compression_ratio(text, summary) < compression_rate and iterations < max_iterations:\n",
    "        iterations += 1\n",
    "        compression_rate_renwed += 0.05\n",
    "        if compression_rate_renwed > 1:\n",
    "            compression_rate_renwed = 1\n",
    "        extracted = textrank_extractive(text, compression_rate=compression_rate_renwed)\n",
    "        summary = ' '.join(extracted)\n",
    "    return summary\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='facebook/bart-large-cnn'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "summarizer = pipeline(\"text2text-generation\", model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_count(text):\n",
    "    tokens = text.split()\n",
    "    return len(tokens)\n",
    "\n",
    "\n",
    "def adjust_length(text):\n",
    "    length = token_count(text)\n",
    "    if length < 80:\n",
    "        min_length = length + int(length * 0.05)\n",
    "        max_length = min_length + 50\n",
    "    elif length < 100:\n",
    "        min_length = length + int(length * 0.3)\n",
    "        max_length = min_length + 100\n",
    "    else:\n",
    "        min_length = math.ceil(length / 50) * 70\n",
    "        max_length = min_length + 100\n",
    "    return min_length, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_sent(sentenc,splitt=180,split='. '):\n",
    "    sentences = sentenc.split(split)\n",
    "    # Erstellen Sie Batches von Sätzen, die weniger als 1024 Tokens enthalten\n",
    "    batches = []\n",
    "    batch = []\n",
    "    batch_len = 0\n",
    "    for sentence in sentences:\n",
    "        \n",
    "        sentence_len = len(tokenizer.tokenize(sentence))\n",
    "        if sentence_len + batch_len > splitt:\n",
    "            if sentence_len < splitt:  # überspringen Sie Sätze, die länger als 256 Tokens sind\n",
    "                batches.append(batch)\n",
    "                batch = [sentence]\n",
    "                batch_len = sentence_len\n",
    "            # wenn ein Satz alleine 1024 Tokens überschreitet, wird er übersprungen\n",
    "        else:\n",
    "            batch.append(sentence)\n",
    "            batch_len += sentence_len\n",
    "    batches.append(batch)\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase_of_text(df_s,text_name='text',komp_name='reduction_multiplier',split='. '):\n",
    "\n",
    "    df_summary_testing = pd.DataFrame(columns=['Zusammenfassung','Min_Kompressionsrate', 'Max_Kompressionsrate', 'Endgueltige_Kompressionsrate','länge Zusammenfassung','länge Ausgangstext'])\n",
    "    # Teilen Sie den Text in Sätze --> für tatsächliche umsetzubng\n",
    "    soplitting=True\n",
    "    for _, row in tqdm(df_s.iterrows(), total=df_s.shape[0]):\n",
    "     \n",
    "        text = row[text_name]\n",
    "        komp = row[komp_name]\n",
    "        text_gesamt_list=[]\n",
    "\n",
    "        for batch in tqdm(batch_sent(text,split=split), desc='Verarbeite Batches'):\n",
    "            # Zusammenfügen der Sätze in einem Batch\n",
    "            batch_text = '. '.join(batch)\n",
    "            min_length_test, max_length_test = adjust_length(batch_text)\n",
    "            ext_summary=summarizer(batch_text, max_length=int(round(max_length_test*komp,0)), min_length=int(round(min_length_test*komp,0)),length_penalty=100,num_beams=2)\n",
    "            # Erstellen Sie einen DataFrame für die aktuellen Ergebnisse\n",
    "           \n",
    "            text_gesamt_list.append(ext_summary[0]['generated_text'])\n",
    "        text_gesamt = '. '.join(text_gesamt_list)\n",
    "        actual_compression_rate = len(text_gesamt.split(' '))/len(text.split(' '))*100\n",
    "      \n",
    "        df_current = pd.DataFrame({\n",
    "            'Zusammenfassung': [text_gesamt],\n",
    "            'Min_Kompressionsrate': [min_length_test],\n",
    "            'Max_Kompressionsrate': [max_length_test],\n",
    "            'Endgueltige_Kompressionsrate': [actual_compression_rate],\n",
    "            'länge Zusammenfassung': [len(text_gesamt.split(' '))],\n",
    "            'länge Ausgangstext': [len(text.split(' '))]\n",
    "        \n",
    "        })\n",
    "       \n",
    "        # Fügen Sie die Daten zum DataFrame hinzu\n",
    "        df_summary_testing = pd.concat([df_summary_testing, df_current], ignore_index=True)\n",
    "      \n",
    "    return df_summary_testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_compression(df, total_tokens_col, current_tokens_col, desired_compression_rate):\n",
    "    df['current_compression_rate'] = df[current_tokens_col] / df[total_tokens_col]\n",
    "    df['compression_difference'] = df[desired_compression_rate] - df['current_compression_rate']\n",
    "    df['reduction_multiplier'] = df[desired_compression_rate] / df['current_compression_rate']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_rank_algo(df):\n",
    "    df_return = pd.DataFrame(columns=['text','text_rank_text','tokens_gesamt', 'token_text_rank','desired_compression_rate','text_rank_compression_rate'])\n",
    "    # Schleife zur Generierung der zufälligen Werte\n",
    "    random.seed(10)\n",
    "    for index, row in tqdm(df.iterrows()):\n",
    "        text = row[\"text\"]\n",
    "        \n",
    "        random_value = round(random.uniform(0.2, 0.8), 2)  # Zufälliger Wert zwischen 0.2 und 0.8 auf zwei Stellen nach dem Komma begrenzt\n",
    "        text_rank_text= compression(text,random_value)\n",
    "        compression_ratio_value=compression_ratio(text, compression(text,random_value))\n",
    "        df_current = pd.DataFrame({\n",
    "                'text':[text],\n",
    "                'text_rank_text': [text_rank_text],\n",
    "                'tokens_gesamt': [len(text.split(' '))],\n",
    "                'token_text_rank': [len(text_rank_text.split(' '))],\n",
    "                'desired_compression_rate': [random_value],\n",
    "                'text_rank_compression_rate': [compression_ratio_value]\n",
    "            })\n",
    "            # Fügen Sie die Daten zum DataFrame hinzu\n",
    "        df_return = pd.concat([df_return, df_current], ignore_index=True)\n",
    "    return df_return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_text_gen(df,split='. '):\n",
    "    rank_df=text_rank_algo(df)\n",
    "    df_zwischen=calculate_compression(rank_df, 'tokens_gesamt', 'token_text_rank', 'desired_compression_rate')\n",
    "    df_sum=paraphrase_of_text(df_zwischen[['text_rank_text','reduction_multiplier']],text_name='text_rank_text',split=split)\n",
    "    merged_df = pd.concat([df_sum, df_zwischen], axis=1)\n",
    "    merged_df['ent_com_rate']=merged_df['länge Zusammenfassung'] / merged_df['tokens_gesamt']\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Scientific', 'news', 'reviews', 'story'], dtype=object)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.classification.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training=df_train[df_train.classification=='news'].reset_index(drop=True)[['classification','text']][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [01:00,  6.04s/it]\n",
      "Verarbeite Batches: 100%|██████████| 1/1 [00:03<00:00,  3.52s/it]\n",
      "Verarbeite Batches: 100%|██████████| 1/1 [00:04<00:00,  4.98s/it]\n",
      "Verarbeite Batches: 100%|██████████| 1/1 [00:03<00:00,  3.44s/it]\n",
      "Verarbeite Batches: 100%|██████████| 1/1 [00:03<00:00,  3.66s/it]\n",
      "Verarbeite Batches: 100%|██████████| 1/1 [00:03<00:00,  3.86s/it]\n",
      "Verarbeite Batches: 100%|██████████| 1/1 [00:04<00:00,  4.25s/it]\n",
      "Verarbeite Batches: 100%|██████████| 1/1 [00:03<00:00,  3.52s/it]\n",
      "Verarbeite Batches: 100%|██████████| 1/1 [00:02<00:00,  2.75s/it]\n",
      "Verarbeite Batches: 100%|██████████| 1/1 [00:04<00:00,  4.30s/it]\n",
      "Verarbeite Batches: 100%|██████████| 1/1 [00:03<00:00,  3.18s/it]\n",
      "100%|██████████| 10/10 [00:37<00:00,  3.75s/it]\n"
     ]
    }
   ],
   "source": [
    "df_sum=execute_text_gen(df_training,split='. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Zusammenfassung</th>\n",
       "      <th>Min_Kompressionsrate</th>\n",
       "      <th>Max_Kompressionsrate</th>\n",
       "      <th>länge Zusammenfassung</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens_gesamt</th>\n",
       "      <th>token_text_rank</th>\n",
       "      <th>desired_compression_rate</th>\n",
       "      <th>text_rank_compression_rate</th>\n",
       "      <th>current_compression_rate</th>\n",
       "      <th>reduction_multiplier</th>\n",
       "      <th>ent_com_rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Administration of Union Territory Daman an...</td>\n",
       "      <td>34</td>\n",
       "      <td>84</td>\n",
       "      <td>33</td>\n",
       "      <td>The Administration of Union Territory Daman an...</td>\n",
       "      <td>60</td>\n",
       "      <td>33</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.981818</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Malaika Arora slammed an Instagram user who tr...</td>\n",
       "      <td>40</td>\n",
       "      <td>90</td>\n",
       "      <td>39</td>\n",
       "      <td>Malaika Arora slammed an Instagram user who tr...</td>\n",
       "      <td>60</td>\n",
       "      <td>39</td>\n",
       "      <td>0.46</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.707692</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bihar Health Minister defined virgin as being ...</td>\n",
       "      <td>36</td>\n",
       "      <td>86</td>\n",
       "      <td>34</td>\n",
       "      <td>The Indira Gandhi Institute of Medical Science...</td>\n",
       "      <td>60</td>\n",
       "      <td>35</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.942857</td>\n",
       "      <td>0.566667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n",
       "      <td>42</td>\n",
       "      <td>92</td>\n",
       "      <td>22</td>\n",
       "      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n",
       "      <td>60</td>\n",
       "      <td>40</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.366667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hotels in Maharashtra will train their staff t...</td>\n",
       "      <td>53</td>\n",
       "      <td>103</td>\n",
       "      <td>38</td>\n",
       "      <td>Hotels in Maharashtra will train their staff t...</td>\n",
       "      <td>60</td>\n",
       "      <td>51</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.850000</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.811765</td>\n",
       "      <td>0.633333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Man, 32, was found hanging inside the washroom...</td>\n",
       "      <td>50</td>\n",
       "      <td>100</td>\n",
       "      <td>42</td>\n",
       "      <td>A 32-year-old man on Wednesday was found hangi...</td>\n",
       "      <td>60</td>\n",
       "      <td>48</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8625</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Delhi High Court reduced the compensation awar...</td>\n",
       "      <td>37</td>\n",
       "      <td>87</td>\n",
       "      <td>35</td>\n",
       "      <td>The Delhi High Court reduced the compensation ...</td>\n",
       "      <td>61</td>\n",
       "      <td>36</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.590164</td>\n",
       "      <td>0.999722</td>\n",
       "      <td>0.57377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>60-year-old Dalit woman was allegedly lynched ...</td>\n",
       "      <td>27</td>\n",
       "      <td>77</td>\n",
       "      <td>21</td>\n",
       "      <td>A 60-year old Dalit woman was allegedly lynche...</td>\n",
       "      <td>60</td>\n",
       "      <td>26</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Pawan Hans helicopter was flying at a crit...</td>\n",
       "      <td>63</td>\n",
       "      <td>113</td>\n",
       "      <td>40</td>\n",
       "      <td>An inquiry by the Aircraft Accident Investigat...</td>\n",
       "      <td>61</td>\n",
       "      <td>60</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.983607</td>\n",
       "      <td>0.5185</td>\n",
       "      <td>0.655738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Congress party has opened a bank called 'State...</td>\n",
       "      <td>43</td>\n",
       "      <td>93</td>\n",
       "      <td>30</td>\n",
       "      <td>The Congress party has opened a bank called 'S...</td>\n",
       "      <td>57</td>\n",
       "      <td>41</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.719298</td>\n",
       "      <td>0.719298</td>\n",
       "      <td>0.556098</td>\n",
       "      <td>0.526316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Zusammenfassung Min_Kompressionsrate  \\\n",
       "0  The Administration of Union Territory Daman an...                   34   \n",
       "1  Malaika Arora slammed an Instagram user who tr...                   40   \n",
       "2  Bihar Health Minister defined virgin as being ...                   36   \n",
       "3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...                   42   \n",
       "4  Hotels in Maharashtra will train their staff t...                   53   \n",
       "5  Man, 32, was found hanging inside the washroom...                   50   \n",
       "6  Delhi High Court reduced the compensation awar...                   37   \n",
       "7  60-year-old Dalit woman was allegedly lynched ...                   27   \n",
       "8  The Pawan Hans helicopter was flying at a crit...                   63   \n",
       "9  Congress party has opened a bank called 'State...                   43   \n",
       "\n",
       "  Max_Kompressionsrate länge Zusammenfassung  \\\n",
       "0                   84                    33   \n",
       "1                   90                    39   \n",
       "2                   86                    34   \n",
       "3                   92                    22   \n",
       "4                  103                    38   \n",
       "5                  100                    42   \n",
       "6                   87                    35   \n",
       "7                   77                    21   \n",
       "8                  113                    40   \n",
       "9                   93                    30   \n",
       "\n",
       "                                                text tokens_gesamt  \\\n",
       "0  The Administration of Union Territory Daman an...            60   \n",
       "1  Malaika Arora slammed an Instagram user who tr...            60   \n",
       "2  The Indira Gandhi Institute of Medical Science...            60   \n",
       "3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...            60   \n",
       "4  Hotels in Maharashtra will train their staff t...            60   \n",
       "5  A 32-year-old man on Wednesday was found hangi...            60   \n",
       "6  The Delhi High Court reduced the compensation ...            61   \n",
       "7  A 60-year old Dalit woman was allegedly lynche...            60   \n",
       "8  An inquiry by the Aircraft Accident Investigat...            61   \n",
       "9  The Congress party has opened a bank called 'S...            57   \n",
       "\n",
       "  token_text_rank  desired_compression_rate  text_rank_compression_rate  \\\n",
       "0              33                      0.54                    0.550000   \n",
       "1              39                      0.46                    0.650000   \n",
       "2              35                      0.55                    0.583333   \n",
       "3              40                      0.32                    0.666667   \n",
       "4              51                      0.69                    0.850000   \n",
       "5              48                      0.69                    0.800000   \n",
       "6              36                      0.59                    0.600000   \n",
       "7              26                      0.30                    0.433333   \n",
       "8              60                      0.51                    1.000000   \n",
       "9              41                      0.40                    0.719298   \n",
       "\n",
       "  current_compression_rate reduction_multiplier ent_com_rate  \n",
       "0                     0.55             0.981818         0.55  \n",
       "1                     0.65             0.707692         0.65  \n",
       "2                 0.583333             0.942857     0.566667  \n",
       "3                 0.666667                 0.48     0.366667  \n",
       "4                     0.85             0.811765     0.633333  \n",
       "5                      0.8               0.8625          0.7  \n",
       "6                 0.590164             0.999722      0.57377  \n",
       "7                 0.433333             0.692308         0.35  \n",
       "8                 0.983607               0.5185     0.655738  \n",
       "9                 0.719298             0.556098     0.526316  "
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sum[['Zusammenfassung', 'Min_Kompressionsrate', 'Max_Kompressionsrate',\n",
    "       'länge Zusammenfassung',\n",
    "       'text','tokens_gesamt',\n",
    "       'token_text_rank', 'desired_compression_rate',\n",
    "       'text_rank_compression_rate', 'current_compression_rate',\n",
    "      'reduction_multiplier', 'ent_com_rate']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63be3639594356bc87fe58051c1d1c5221c23a964c31c0e05d208c4974bedf26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
