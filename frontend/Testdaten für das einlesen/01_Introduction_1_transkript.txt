So, dann herzlich willkommen zur ersten Vorlesung in Reinforcement Learning. Dies soll einfach als Grundlage dafür dienen, das Thema in Machine Learning Kontext einzuordnen, auch Bezug herzustellen zu aktuellen Themen aus der Data Science Welt und auch schon erste teils mathematische Konzepte zu vermitteln. In die Tiefe gehen wir dann in der nächsten Vorlesung. Aber genau, erstmal so als groben Überblick über Reinforcement Learning. Ich würde gerne mal ganz klassisch mit einem Zitat von John McCarthy starten. Der gilt als Gründervater der künstlichen Intelligenz, hat auch zum Beispiel den Turing Award und die, glaube ich, National Medal of Science gewonnen. Also recht wichtige Persönlichkeit in der Welt der AI und im Grunde besagt sein Statement, dass das Ziel der AI ist, Programm des Lernen von vergangenen Erfahrungen beizubringen und so effektiv wie bei menschlichen Denken zu sein. Vielleicht als kleiner Denkanstoss für euch, einmal die Frage, ob wir schon soweit sind, wenn ihr auch an die verschiedenen Arten des Machine Learning denkt, supervised, unsupervised, ob ihr das da schon seht, dass Programme wirklich wie Menschen lernen können, auch in dem Hinblick, dass wir in Reinforcement Learning ja wirklich wahrscheinlich am nächsten am menschlichen Lernen dran sind. Ich hatte es ja gerade kurz angesprochen, wir haben die drei Felder supervised, unsupervised und Reinforcement Learning. Klar, es gibt auch noch semi-supervised, aber das sind die drei Grundbausteine der AI beziehungsweise des Machine Learning. Ich würde sagen, dass künstliche Intelligenz noch weit weg ist, um davon menschenähnliches Denken oder menschenähnlicher Intelligenz zu imitieren, also auch Strong AI genannt, aber für bestimmte fest definierte Aufgaben würde ich schon sagen, dass künstliche Intelligenz so gut oder sogar besser als Menschen sein kann. Noch mal zur Einordnung. AI ist das gesamte Feld, es gibt regelbasierte AI, es gibt Machine Learning, es gibt starke, es gibt schwache AI und Teil davon ist eben das Machine Learning und davon Teil Deep Learning mit Neuronennetzen. Ich denke, ihr werdet die Einordnung in gefühlt jeder Vorlesung, die mit Data Science zu tun hat, schon gesehen oder gehört haben. Trotzdem noch mal als Recap an supervised Learning, da haben wir Daten ohne Label, Daten, wo wir nicht wissen, wie sie klassifiziert werden können, heißt da geht es wirklich darum, einen Datensatz in gewisser Weise zu beschreiben beziehungsweise auch die Dimensionalität zu reduzieren. Also durch Clustering versuchen wir ja Informationen über diesen Datensatz zu aggregieren. Genau das gleiche mit Association, Warenkorbanalysen und eben Principal Component Analysis sind alles Dinge, die in irgendeiner Art und Weise versuchen, Informationen über einen Datensatz zu generieren und damit dessen Dimensionalität zu reduzieren. Bei supervised Learning hingegen haben wir die Labels für die Daten, wir haben zum Beispiel Informationen, ja der Kunde hat bei uns gekauft wieder oder der Kunde hat nicht mehr bei uns gekauft, haben die verschiedenen Algorithmen wie Rekression, Klassifikation, Anomalieerkennung etc., wobei Anomalieerkennung auch ein Teil des supervised Learning sein kann, je nachdem wie man das Problem definiert, heißt da haben wir recht straightforward durch das Label schon die Aufgabe auch vorgegeben bekommen. Jetzt sind wir hier im Reinforcement Learning, etwas andere Struktur als die vorigen beiden Methoden, weil hier lernt ein Agent über Trial and Error, also durch Interaktion mit der Umgebung lernt ein Agent das Modell oder lernt eine Aufgabe besser gesagt zu erfüllen und dadurch unterscheidet sich das Reinforcement Learning schon sehr stark von supervised und unsupervised Learning. Nicht erschrecken, ich habe kurz mein Streaming-Mikrofon rausgeholt, weil mein anderes eine zu schlechte Qualität hat, wie ich bemerkt habe. Ich würde jetzt gerne einmal kurz über verschiedene Beispiele von Reinforcement Learning, also vor allem aktuelle Beispiele von Reinforcement Learning drüber gehen. Einige davon dürften euch auch bekannt vorkommen, zum Beispiel ChatGPT. Ich gehe mal davon aus, dass ihr das sicher schon mal verwendet habt. Ein Natural Language Processing Algorithmus, den man auch als Reinforcement Learning Algorithmus definieren kann. Also verschiedenste Probleme kann man im Grunde genommen immer als Reinforcement Learning darstellen, als Reinforcement Learning Problem darstellen und das hat eben auch OpenAI gemacht. In dem ersten Schritt hat das Modell erst mal Sequenzen gelernt, also Sequence-to-Sequence-Modelle, als NLP-Modell gelernt, um überhaupt erst mal diese Ausgabesätze generieren zu können und dann in einem zweiten Schritt, also darum ging, wie verbessern wir jetzt das Modell, wie sagen wir was Gutes und was Schlechte Sätze sind, hat es eben das Ganze in eine Reinforcement Learning Umgebung quasi umformuliert und gesagt, für gute Sätze geben wir dem Modell Pluspunkte, geben wir Belohnungen und für schlechte Sätze keine Belohnung bzw. gegen den Fall sogar Strafpunkte und so hat sich das über den Lauf der Zeit verbessert. Jetzt auch mit der neuen Version, abgesehen davon, dass man Bilder hochladen kann etc., hat es eben anhand der Reinforcement Learning Logik sich verbessert. Also je mehr Leute es nutzen, natürlich desto mehr Daten hat man und desto besser kann man dann auch den RL-Part, sage ich mal, trainieren. Es gibt zum Beispiel auch noch im Gesundheitswesen ein Reinforcement Learning Algorithmus, der dürfte jetzt nicht ganz hoh bekannt sein, aber KenSci zum Beispiel sagt vorher, welche Krankheiten ein Mensch haben könnte, mit welcher Wahrscheinlichkeit, hört sich erstmal auch supervised learning mäßig an, ist aber als Reinforcement Learning Algorithmus umgesetzt worden. Auch sehr bekannt ist OpenAI5. Reinforcement Learning wird sehr, sehr gerne in der Gaming-Welt angewendet und OpenAI5 hat schon viele professionelle Spieler im Spiel Dota 2 geschlagen. Das hat so ein bisschen den Stil von League of Legends. Es ist ein 5 gegen 5 Spiel und wer League of Legends kennt, der weiß, dass es super viele verschiedene Charaktere gibt. Man kann super viele verschiedene Aktionen ausführen und deswegen kann man sich schon mal vorstellen, wie komplex auch so ein Reinforcement Learning Algorithmus werden kann, beziehungsweise wie komplexe Umgebungen, also wie gut es damit umgehen kann, dass es sogar Menschen besiegt in einem Computerspiel. Ein sehr, schon ein bisschen älteres Beispiel, aber auch ein Beispiel, das einen Mensch besiegt hat, ist Ivargo, in dem Brettspiel Go trainiert. Als Reinforcement Learning Algorithmus hat, glaube ich, den Weltmeister sogar besiegt. Das war, glaube ich, auch so der erste Durchbruch, der mit Reinforcement Learning geschafft wurde, beziehungsweise nicht der erste, aber ich denke, der wohl bekannteste. Wer Boston Dynamics kennt, kennt bestimmt auch den Roboter, den sie entwickelt haben. Ich hatte eingangs erwähnt, dass Reinforcement Learning wahrscheinlich so am nächsten an dem menschlichen Lernen dran ist und Roboter sollen ja immer Menschen imitieren oder menschenähnliches Verhalten aufzeigen. Entsprechend ist auch naheliegend, dass Reinforcement Learning in der Robotik verwendet wird. Und genau, dieser Roboter von Boston Dynamics kann springen, laufen, verschiedene Hindernisse bewältigen, kann Dinge tragen. In diesem Beispielvideo, wo ich den Screenshot her habe, zum Beispiel erreicht er eine Holzplatte einem Bauarbeiter, der oben auf einem Gerüst steht, genau quasi so als kleiner Helferlein, als kleines Helferlein entwickelt. Und ich glaube, inzwischen hat es auch noch mal ein ganz anderes Niveau erreicht. Ist ja auch logisch, mit steigender Kapazität an Berechnungsressourcen etc. kann man natürlich auch solche Algorithmen noch mal verbessern und weiterentwickeln. Vor dem Hintergrund, jetzt auch, wo wir es eingeordnet haben, mit Beispielen, jetzt habt ihr schon ein bisschen was gehört, möchte ich gerne eine Allrounder-Definition von Reinforcement Learning euch vorstellen, euch geben. Die wird uns im Laufe der Vorlesungen immer wieder begegnen. Ich werde immer wieder darauf eingehen, deswegen auch keine Panik, wenn ihr jetzt noch gar nicht versteht, was die einzelnen Dinge bedeuten. Ich möchte nur kurz einmal drüber gehen und grundsätzlich mal erklären, was ist Reinforcement Learning. Es handelt sich da um einen Machine Learning Algorithmus, der mithilfe von Trial and Error lernt. Also Trial and Error, auf Deutsch Versuch und Irrtum, glaube ich, ist im Grunde, wenn man so lange zulässige Lösungsmöglichkeiten für ein Problem probiert, bis man eine hat, die funktioniert und dann immer noch weiter probiert, bis man eine hat, die noch besser funktioniert. Das ist Trial and Error Learning. Und das Ziel von Reinforcement Learning ist also der Agent, der diese Vorgänge mit der Umgebung wiederholt, dessen Performance zu maximieren über eine gewisse Zeitspanne. Also man hat hier auf jeden Fall mehrere Wiederholungen drin, man hat hier eine Zeitspanne und nicht nur jetzt zum Beispiel bei Supervised Learning eine einzelne Vorhersage, die man sich da rausholt. Und ja, das Feedback, das der Agent von der Umgebung bekommt, in der er sich befindet, zum Beispiel jetzt in dem Dota 2 Spiel, das war ja eine Umgebung, ist einmal sequenziell. Also da haben wir auch wieder diesen Zeitaspekt bewertet, weil wir haben in dem Sinne keine Labels, sondern wir haben nur so ungefähr eine Indikation, wie gut eine Aktion war, die der Agent gemacht hat und gesampelt, weil wir natürlich zum Beispiel auch in diesem Dota 2 Spiel nicht immer alles über die Umgebung wissen und alles bekommen an Info, sondern man lernt so peu à peu die Rückmeldung und die Informationen zusammenzusetzen, sag ich mal. Und das funktioniert alles im Deep Reinforcement Learning über Nicht-Lineare, in Reinforcement Learning generell über allgemeine Funktionsapproximation. Wie gesagt, keine Angst, keine Panik, wenn ihr jetzt noch nicht so ganz versteht, was das alles bedeutet. Ich werde nochmal darauf eingehen in zukünftigen Vorlesungen und ich denke auch schon nach der Vorlesung bzw. nach dem Video hier, werdet ihr eine gute Idee davon haben, was das genau bedeutet. Genauso wie es in dieser Gesamtübersicht eben wie Deep Learning ein Teil von Machine Learning ist, gibt es eben auch Deep Reinforcement Learning, auch vorhin kurz angesprochen bei den Beispielen, heißt wir benutzen neuronale Netze, um Reinforcement Learning Aufgaben zu lösen und diese Aufgaben sind im Grunde genommen sehr, sehr komplexe sequentielle Entscheidungsfindungsprozesse, die eben in einer Umgebung stattfinden, über die man nicht alle Informationen hat, über die alle Informationen vorliegen und damit man auch ein gewisses Grad an Unsicherheit hat. Also wir gehen da auch in Richtung Statistik später bzw. Wahrscheinlichkeiten, Stochastik, genau das schon vorweg. Reinforcement Learning oder vor allem Deep Reinforcement Learning hat sehr, sehr viele Schlittmengen mit anderen Disziplinen, vor allem in der angewandten Mathematik. Dazu gehört zum Beispiel einmal die Kontrolltheorie, wird auch Regelungstheorie genannt, das ist ein Teilgebiet der angewandten Mathematik und die betrachtet dynamische Systeme, deren Verhalten durch verschiedene Einflussgrößen von außen beeinflusst werden können und das ist ja im Grunde von der Idee her genau das gleiche wie bei Reinforcement Learning, bei dem der Agent mit seinen Aktionen seine Umgebung beeinflussen kann bzw. eben auch seine Belohnung, die er davon bekommt. Ich gehe mal davon aus, dass ihr Operations Research, wie ich damals auch als Kurs schon hatte, dementsprechend wisst ihr wahrscheinlich schon, was Operations Research ist. Da geht es um die Entwicklung von quantitativen Modellen und Methoden, die eben generell bei Entscheidungen unterstützen sollen. Also auch hier wieder dieser Entscheidungsfindungsaspekt in einer gewissen Umgebung und gerade in dem Fall eben auch mit sehr, sehr vielen möglichen Handlungsmöglichkeiten. Genau das ist so da der Schnittpunkt. Theoretisch kann man auch sagen, Reinforcement Learning kann ja genauso gut mit Psychologie ein Stück weit verglichen werden, weil Entscheidungsfindung bei Menschen hat ja auch sehr viele psychologische Aspekte etc. Also dafür sind wir uns echt in einem sehr, sehr interdisziplinären Bereich und jetzt nicht rein in diesem Machine Learning Ding, wie man vielleicht denken könnte. Es ist jetzt schon mehrfach der Begriff Agent und Umgebung gefallen und deswegen einmal hier die Übersicht über den Lernzyklus bei Reinforcement Learning. Also wenn wir später auch programmieren, was wir programmieren, ist der Agent, also dieses Computerprogramm, das am Ende die Entscheidung trifft, unser Modell in dem Sinne. Agenten befinden sich immer in einer Umgebung. Im Grunde ist die Umgebung alles außer der Agent, die in dem Sinne nicht wirklich kontrollierbar bzw. änderbar ist. Könnt ihr euch vorstellen, zum Beispiel beim Schachspielen, ihr könnt jetzt nicht einfach die Regeln ändern. Die sind fix und genau das ist in dem Fall die Umgebung. Agenten können die Umgebung ein Stück weit beeinflussen, also die Zustände der Umgebung, in denen sie sich befinden, mithilfe seiner Aktionen. Also eine Umgebung, ich nehme jetzt mal wirklich das Schachbrett als Beispiel noch mal, hat einen bestimmten Zustand, zum Beispiel der Startzustand, bei dem alle Figuren auf der jeweiligen Seite auf dem Spielbrett stehen. Der Agent, wenn wir es jetzt programmieren würden, würde diesen Startzustand entgegennehmen und sich überlegen, welche Aktion führe ich jetzt als erste Aktion aus. Über diese Aktion wird dann der Zustand wieder geändert, weil jetzt ist jetzt zum Beispiel der Bauer einen Schritt vorgegangen und damit hat sich ja die Zusammensetzung auf dem Spielfeld, wie die Figuren stehen, geändert. Entsprechend hat sich der Zustand auf dem Spielfeld geändert. Und je nachdem, bei Beispielen, bei dem Beispiel jetzt von Schach ist es ein bisschen schwierigeres Beispiel, weil die Belohnung kommt ja erst ganz am Ende, wenn man gewinnt oder verliert. Aber es gibt auch Beispiele, bei denen man dann direkt eine Belohnung an den Agenten geht. Ein gutes Beispiel ist Pac-Man zum Beispiel. Da kann man ja auch so diese Pluspunkte einsammeln, indem man das isst. Und das ist ja auch mal ein Beispiel, dass eben eine Aktion, dass sie sich zum Beispiel nach links bewegt, direkt eine Belohnung gibt. Und um es ein bisschen in Richtung Mathe zu bewegen, es gibt einmal eine Beziehung zwischen Aktion und Belohnung. Also man kann das als Funktion darstellen. Das nennt man auch Belohnungsfunktion oder Reward Function. Sorry dafür auch schon, weil ich so viel Denglisch rede, aber Machine Learning generell ist halt eigentlich eher auf Englisch, wird auf Englisch abgehalten. Deswegen entschuldige, dass es ein bisschen hier Denglisch wird. Aber diese Reward Function beschreibt am Ende, also es nimmt eine Aktion entgegen in einem gewissen Zustand und sagt dann, hey, die Aktion in dem Zustand gibt mir den Reward. Und genauso kann man die Beziehung zwischen Aktion und Zustand darstellen, ohne jetzt die Belohnung mit einzubeziehen. Heißt, hey, wenn ich die Aktion in diesem Zustand, in diesem State ausführe, komme ich in den und den nächsten State. Also wenn ich jetzt den Bauer einen Schritt nach vorne mache, dann ändert sich die Spielumgebung, dass jetzt der Bauer einen Schritt nach vorne geht und entsprechend das Spielfeld anders aussieht. Das kann auch mathematisch dargestellt werden über die Transition Function. Und nimmt man alle Transition Functions, die es für diese Umgebung gibt und alle Reward Functions zusammen, hat man ein Modell der Umgebung. Heißt, hat man das Modell vollumfänglich beschrieben. Da das wirklich die Grundlage vom Reinforcement Learning ist, möchte ich noch ein paar mehr Beispiele euch zeigen beziehungsweise mit euch durchsprechen. Einmal eher so vom echten Leben rausgeholt, da sieht man auch wieder, dass wirklich auch reale Entscheidungsprozesse irgendwie so dargestellt werden können. Und einmal noch aus dem Gaming-Umfeld ein Beispiel. Stellt euch vor, ihr hättet eine Katze und die Katze liegt irgendwo in eurem Haus. Das ist der Zustand. Katze befindet sich, keine Ahnung, im Schlafzimmer oder so. Und ihr versucht jetzt, die Katze herzulocken, indem ihr mit Leckerli oder Futterdosen raschelt. Und die Katze ist in unserem Fall der Agent. Und dieser Agent oder die Katze nimmt eben diesen neuen Zustand in der Umgebung wahr. In dem Fall eine Observation, was genau der Unterschied ist, werden wir noch besprechen. Und fährt dann die Aktion, zu euch zu kommen, also in die Küche zu kommen. Und damit ist dann die Katze in der Küche. Heißt, der Zustand hat sich wieder geändert. Katze befindet sich jetzt in der Küche und ihr wartet eben da und dann kriegt die Katze ein Leckerli oder Futter zur Belohnung, was in dem Fall dann plus 1 als Reward zählen würde. Und der Zustand eben, dass die Katze in der Küche ist. Genau. Und würde man das jetzt immer wieder über einen längeren Zeitraum wiederholen, würde die Katze eine Strategie lernen, bei dem es quasi die Leckerlis maximieren kann, indem sie immer herkommt, wenn ihr mit der Futterdose raschelt. Also so ein Stück weit auch konditionieren könnte man sich vorstellen. Und diese Strategie wird im Reinforcement Learning Policy genannt. Und dieser Tweet eben Reward. Und ja, so kann man im Grunde alle Entscheidungsfindungsprozesse modellieren. Ihr werdet jetzt gleich noch ein Beispiel sehen, ein recht bekanntes aus der Spielewelt von Atari. Aber das jetzt erstmal so als realer alltäglicher Prozess. Genau, dann zu dem Spiel von Atari. Das nennt sich Pong, dürfte relativ bekannt sein. Da spielen sich zwei Spieler einen Ball hin und her mit einer Plattform, die sie bewegen können. Und Ziel ist einmal, dass man verhindert, dass der Ball neben der Plattform quasi in die jeweilige Bande versenkt wird und gleichzeitig zu versuchen, mit verschiedenen Spieltechniken dann den Ball beim Gegner zu versenken. Und wenn wir das einmal als Lernzyklus durchspielen, kann man als Ausgang Zustand mal sagen, dass der Spieler auf den Ball von dem Gegner wartet und dann beobachtet der Agent, okay, der Gegner hat jetzt den Ball berührt und der Ball kommt auf die Seite des Spielers. Und er hat jetzt zwei mögliche Aktionen, die er ausführen kann, vereinfacht gesagt rechts oder links. Natürlich kann man auch sagen, so und so viele Zentimeter nach rechts oder links, aber jetzt einfach mal vereinfacht dargestellt. Somit kommt dann der Ball auf die Spielerseite und in unserem Fall hat er die falsche Seite gewählt, ist in die falsche Richtung gegangen und entsprechend wird der Ball versenkt und der Gegner bekommt dann einen Punkt, beziehungsweise in Reinforcement Learning würde entsprechend der Agent dann einen Minuspunkt bekommen, beziehungsweise eine Strafe als Belohnung. Wichtig auch hier ist zu sagen, man sagt immer Belohnung, auch wenn es eine negative Belohnung ist, also eine Strafe ist. In Reinforcement Learning ist ein Reward einfach, kann positiv oder negativ sein, beinhaltet eben aber auch damit Strafungen. Und da haben wir dann wieder die Policy, die ein Spieler lernen kann, die Plattform so zu bewegen, dass er gewinnt. Dazu gehört nicht nur, dass er die Bälle des Gegners abwehrt, aber auch dann bestimmte Winkel zum Beispiel zu spielen, in denen dann der Gegner Schwierigkeiten hat, mit der Plattform hinterher zu kommen und eben dann, dass der Spieler den Ball versenken kann und so Punkte bekommt. Damit haben wir jetzt eigentlich schon alle Dinge kennengelernt, die ein Agent lernen kann in Reinforcement Learning, nämlich einmal Policies, Value Functions und Models. Die Policies sind eben diese Strategien, also zu lernen, wie handle ich am besten, wenn ich einen bestimmten Zustand beobachte. Dann die Value Functions einmal vorher sagen zu können, welche Belohnung kann ich erwarten, wenn ich mich mit der Policy verhalte. Und dann noch das Modell, also vorhin haben wir gesagt, das ist die Menge an allen Transition und Reward Functions, also zu lernen, wie ist es denn die Dynamik der Umgebung, in der ich mich befinde. Das sind alles die drei Dinge, um die es in den Folgevideos gehen wird, heißt die zu lernen, die zu approximieren. Und dabei verfolgt der Agent immer so ein Drei-Schritte-Prozess im Grunde. Einmal, dass der Agent die Umgebung beobachtet und damit interagiert. Dann evaluiert er sein Verhalten, evaluieren und basieren eben auf der Belohnung, die er bekommen hat und final dann dieses Verhalten, die Strategie zu verbessern und damit wiederum mehr Belohnung einzuholen. Und das tut er eben auf Basis dieser Interaktion, damit sammelt der Agent Erfahrung. Heißt, dieser Drei-Schritte-Prozess, den wir gerade gesehen haben, der geht über mehrere Zeitschritte, sag ich mal. Die werden mathematisch im Artikel definiert und dabei muss man aber unterscheiden, weil es gibt zum Beispiel bei Schach, weiß man, das Spiel ist vorbei, wenn Schach matt ist. Also das ist so ein sogenannter episodischer, episodische Aufgabe, heißt ja ein definiertes Ende. Generell bei Spielen eigentlich gibt es immer eine Bedingung, die erfüllt sein muss, damit das Spiel endet. Also wenn man siegt oder verloren. Auf der anderen Seite gibt es auch kontinuierliche Aufgaben, bei denen es kein natürliches definiertes Ende gibt. Also ein gutes Beispiel ist da bei Robotern, die Bewegungen lernen. Bei wann sagst du wirklich, dass ein Roboter die Bewegung kann? Es geht immer noch ein bisschen besser. Gerade wenn man Genauigkeit zum Beispiel, kann man ja unendlich weit nach dem Komma optimieren, heißt das nennt man dann kontinuierliche Aufgabe. Und die Erfahrung, die der Agenda im Laufe dieser verschiedenen Aufgaben sammelt, werden in sogenannten Erfahrungstupeln gespeichert, auf englisch experience tuples, immer pro Zeitschritt. Also diese Erfahrungstupel beinhaltet dann einmal den Zustand, in dem der Agent war, die Aktion, die er da getätigt hat, die Belohnung, die er dann bekommen hat und der neue Zustand, in dem er sich danach befunden hat, nachdem er diese Aktion in dem Zustand eben vorrücken getätigt hat. Und das ist recht ganz gut dargestellt, eben das passiert, dieser Lernzyklus passiert immer pro Zeitschritt einmal. Also der Zeitschritt erhöht sich dann, sobald einmal interagiert wurde. Und das kann man je nachdem auch, was für eine Art von Task es ist, x Zeitschritte wiederholen oder unendlich viele Zeitschritte, bis man eben zu einem gewissen Punkt konvergiert. Ich habe es vorhin schon vor dem Hintergrund des Schachbeispiels mal erwähnt, dass es teilweise natürlich braucht, bis sich Belohnungen über gewisse Aktionen manifestieren. Wenn ihr euch mal vorstellt, dass ihr Fußball spielt und habt heute ein Spiel und nach 90 Minuten habt ihr ja keine Ahnung, x Mal den Ball weiter gespielt, ihr habt getribbelt, ihr seid gerannt, habt geschossen, vielleicht sogar gefoult und am Ende verliert euer Team. Und danach möchte euer Trainer dann mal besprechen, woran hat es gelegen, wer war jetzt schuld, heißt, wer muss am meisten noch mal ein bisschen daran arbeiten und wie würdet ihr da vorangehen, um genau identifizieren zu können, woran es jetzt gelegen hat, dass ihr verloren habt. Und dieses Vorhaben zeigt im Grunde das sogenannte Temporal Credit Assignment Problem in Reinforcement Learning. Einfach, dass ihr es schon mal gehört habt, es ist einfach nicht möglich, alle Aktionen von allen Spielern, die getagged wurden, jede Sekunde von dem Spiel, sei es jetzt einmal ein Schritt nach rechts zu gehen, einen Schritt nach links zu gehen, man kann das einfach nicht auf das finale Ergebnis mappen. Also man kann nicht sagen, so und so viel Prozent hat die Aktion dazu beigetragen, dass wir verloren haben. Und das liegt einfach an der Tatsache, dass Aktionen auch spätere Konsequenzen haben. Die sind so ein bisschen zeitlich verschoben, weil es kann jetzt sein, hey, an sich war es jetzt nicht schlecht, dass er den Ball in die Richtung gepasst hat, aber dafür drei Spielzüge später haben wir dann den Ball verloren und das wurde alles durch den ersten Pass verursacht. Heißt, diese Belohnung, beziehungsweise auch negative Belohnung in unserem Fall, ist nicht direkt immer klar nach Aktionen. Und das ist eben das Problem, auch dass wir sequentielles Feedback haben, also dass wir eben diese mehreren Pässe erst mal haben, bevor wir eine Belohnung generieren. Und da ist eben die Frage, wie dann der Agent diese Belohnung gewichten soll, beziehungsweise die Aktion gewichten soll, bewerten soll, wenn er eh erst in x Zeitschritten später die zugehörige Belohnung bekommt. Damit ist, denke ich, jetzt ganz gut klar geworden, was genau dieses Sequenzielle bedeutet und eben auch, was es für Probleme mit sich bringt. Nichtsdestotrotz müssen wir noch einmal über die anderen zwei Themen drüber gehen, damit ihr auch da ein besseres Verständnis bekommt, um so die grundlegende Definition mal zu verstehen. Ich hatte es vorhin kurz schon angerissen. Im Supervised Learning haben wir eindeutige Labels. Das sind ja auch Supervised, keine Ahnung, im Computer Vision zum Beispiel, da bekommen wir ein Bild und da ist eindeutig eine Katze drauf, ein Hund, ein Auto, whatever. Heißt, wir können lernen anhand der Information, ob etwas richtig oder falsch vorhergesagt wurde. Ich meine, darauf basiert ja auch Backpropagation, gerade diese Differenz zwischen habe ich richtig vorhergesagt, habe ich falsch vorhergesagt und das haben wir im Reinforcement Learning in dem Sinne nicht, weil Aktionen sind nicht richtig oder falsch, sondern sie können eher gut oder weniger gut sein. Und das meint quasi dieses bewertete Feedback, dass Agenten auch gar nicht einschätzen können, wie sich Belohnungen strukturieren. Das sieht man eigentlich ganz gut in dem Beispiel, das habe ich auch zum Buch genommen, von, also dieses Crocking, Deep Reinforcement Learning. Da interagiert der Agent über mehrere Zeitschritte mit der Umgebung und bekommt am Anfang zum Beispiel in der ersten Interaktion 50 als Belohnung, was ja erstmal nicht schlecht ist, vor allem wenn man sich die zweite Aktion anschaut, 20 weniger, okay, heißt 50 war schon ein recht guter Start, bis er dann nach mehreren Zeitschritten merkt, okay, ich hätte genauso gut 1000 bekommen können. Und das ist eben, man hat nie so richtig am Anfang eine Idee davon, wie Aktionen wirklich zu bewerten sind, das entwickelt sich über die Zeit. Und um eben diese Information einzusammeln, muss der Agent sich eben auch von seiner Safe Zone auch rausbewegen und so ein bisschen die Umgebung erkunden und nicht nur die Aktionen nehmen, die davor schon gut funktioniert haben, weil wer weiß, er hat zwar am Anfang 50 bekommen, aber er hätte auch die Aussicht auf 1000 gehabt können. Und da muss man das so ein bisschen, so ein Trade-off nennt man Exploration, Exploitation, Trade-off, Explore. Man möchte natürlich neue Informationen einholen, aber auf der anderen Seite möchte man auch die Informationen, die der Agent bereits besitzt, auf Basis der Erfahrung, auch manchmal einfach ausschöpfen. Also da muss man so ein bisschen schauen, dass man einen guten Mittelweg befindet, dass man wirklich auf lange Frist, auf lange Sicht das Optimum finden kann. Um die gesammelte Eigenschaft des Feedbacks in reinforcement Learning zu verstehen, müssen wir noch mal auf das Umgebungsmodell schauen. Umgebungsmodell war die Menge aller Transitionen und Belohnungsfunktionen, heißt Menge aller Transition and Reward Functions auf Englisch. Transitionen beschreiben, was passiert, wenn ich in diesem Zustand diese Aktion ausführe, in welchem Zustand lande ich dann am Ende. Das ist die Transition. Und die Belohnungsfunktion sagt, welche Belohnung kann ich erwarten, wenn ich genau diese Transition ausführe. Das beschreibt die Belohnungsfunktion. Und man kann sich vorstellen, um noch mal zum Beispiel auf das Schachspiel zu schauen, die Kombinationen, in der Spielfiguren stehen können, 16 Spielfiguren insgesamt auf einem 8x8 Spielfeld, alle möglichen Kombinationen, die sich in einem Schachspiel ergeben könnten. Das sind unglaublich viele mögliche Spielkonfigurationen, sage ich mal. Und dann könnte man ja noch in jedem Zustand eine bestimmte Menge an Aktionen ausführen, also an Spielzügen ausführen. Und da merkt man schon, dass man sich alleine in einem Schachspiel schon in einer sehr, sehr komplexen Umgebung befindet. Und Sampled Feedback in dem Zusammenhang meint dann, dass man mit jedem Spielzug nur so ganz, ganz langsam und nur ganz fragmental so den Überblick über diese Transition and Reward Functions bekommt. Also mit jedem Mal, mit dem ein Agent interagiert, bekommt er so einen Bruchteil der Umgebung, offenbart sich dadurch quasi. Und selbst wenn man x-tausend Mal interagiert hat, ist es jetzt bei dem Beispiel Schach, kann man immer noch nicht wirklich hundertprozentig die Transition and Reward Functions herausfinden, sondern man muss dann eben generalisieren. Und das ist eben auch diese Funktionsapproximation. Deswegen brauchen wir die, weil wir ja nicht unendlich oft diese Schachspiele spielen wollen. Das wäre auch sehr, wie soll ich sagen, sehr teuer in dem Sinne, dass wir sehr viele Ressourcen verbrauchen würden. Heißt, wir wollen also schnell wie möglich Informationen darüber generieren anhand des Sampled Feedbacks. Damit habe ich jetzt hoffentlich ein bisschen Licht ins Dunkel gebracht, was dieses sequenzielle bewertete und gesamplete Feedback bedeutet. Nach wie vor müssen wir uns noch anschauen, was genau es mit der Funktionsapproximation dann auf sich hat. Also nochmal als Recap. Wir haben drei Dinge, die ein Agent lernen kann. Einmal Policies, also Spielstrategien, Wertefunktion, Value Functions, also die Belohnungsfunktion und ein Modell bzw. Modelle der Umgebung, also diese Menge an Belohnungs- und Transitionsfunktionen. Und je nachdem, mit welchem Ziel ein Agent lernt, also je nachdem, was er von diesen Dingen lernt, hat er dann einen anderen Namen. Man unterscheidet hier zwischen Policy, Value und Model-based Agents, respektiv dann eben zu den jeweiligen Lernzielen. Und es gibt aber auch eine Kombination zum Beispiel von Policy und Value-based Agents, das nennt sich dann Actor-Critic Agents. Und wir werden im Laufe der Videos das meiste anschneiden, wobei wir Model-based Agents ein bisschen außen vor lassen, weil da nochmal eine mathematische Einführung zu geben und vor allem die Algorithmen mathematisch darzustellen, ist nochmal ein ganz anderes Thema, da Policy und Value-based so schon sehr viele Schnittmengen haben. Entsprechend werden wir uns dann auf die zwei Arten konzentrieren. Aus dieser gesammelten Eigenschaft des Feedbacks im Reinforcement Learning ging ja vor, dass ein Agent generalisieren muss über verschiedene Samples hinweg. Und rein mathematisch ausgedrückt ist eine Generalisierung ja eine Funktionsapproximation, besonders im Hinblick, dass wir Wertefunktionen lernen wollen, Policies lernen wollen oder eben Modelle. Das hat alles quasi diese Funktionsapproximation mathematisch als Hintergrund. Und ihr dürftet schon einige verschiedene Methoden im Machine Learning kennengelernt haben, Support Vector Machine, Entscheidungsräume etc. Und natürlich im Deep Learning dann die Neuronalnetze für nicht-geniale Approximation. Auch im Reinforcement Learning ist da zwischen verschiedenen Use Cases zu unterscheiden, zwischen verschiedenen Umgebungen zu unterscheiden, weil manchmal ist es auch einfach overkill, direkt einen Deep Reinforcement Learning Algorithmus anzuwenden, wenn es ein einfacherer auch getan hätte. Die Übersicht soll so ein bisschen als Hilfe dienen, wann denn Deep Reinforcement Learning geeignet ist und wann eher nicht. Es ging ja auch schon aus den Beispielen, aus den aktuellen Beispielen hervor, dass Reinforcement Learning Agenten sehr gut in genau definierten Aufgaben sind und Spiele sind ebenso genau definierte Aufgaben, weil wir haben Regeln, wir haben ein Spielfeld, heißt die Umgebung ist da recht klar, während aber im Gegensatz zu Supervised Learning Algorithmen diese Agenten nicht über verschiedene Umgebungen hinweg irgendwie generalisieren können. Also Transfer Learning funktioniert bei Reinforcement Learning nicht wirklich, während man oft ja zum Beispiel bei der Bildverarbeitung, Convolutional Neural Network, auch für andere ähnliche Tasks verwenden kann und ebenso Transfer Learning betreiben kann. Das ist eben nicht der Fall für Reinforcement Learning. Außerdem kann ein Reinforcement Learning Agent handeln. Also das ist natürlich besonders. Anders als ein Supervised Learning Programm, das einfach nur sagt, Katze, Hund, kann ein Reinforcement Learning Agent interagieren, handeln und das macht eben diese Agenten auch so anders als unsupervised oder supervised learning Modelle. Leider braucht man natürlich unglaublich viele Interaktionen, um diese Generalisierung, diese Funktionsapproximation zu ermöglichen, weil die Umgebungen auch so komplex sind, heißt Reinforcement Learning Algorithmen sind sehr ineffizient hinsichtlich der Samples und entsprechend braucht man auch sehr viel Computer Power, um die trainieren zu können, also um wirklich komplex trainieren zu können. Was auch interessant ist, ist, dass man Reinforcement Learning eben auch mit Computer Vision zum Beispiel verbinden kann. Also die neuronalen Netze können zum Beispiel auch Convolutional Neural Networks sein, heißt Roboter zum Beispiel sind dann auch in der Lage, über visuellen Input zu handeln, also wenn man Bilder als Eingabedaten hat. Ein bisschen ein Nachteil ist, dass oft gerade bei der Robotik auch man nicht genau definieren kann, wann jetzt eine Aktion gut, wann sie schlecht war und in welcher Range man sich befinden sollte, weil gerade die Bewegung, ja es ist sehr schwer zu identifizieren, was jetzt genau die beste Bewegung ist. Da braucht man eben oft auch Input von Experten in dem Feld und die definieren dann die Belohnung manuell. Zum Ende dann nochmal eine ganz kurze Zusammenfassung des Videos. Also Reinforcement Learning haben wir Agenten, die mit der Umgebung interagieren und das Ziel ist, die Aktionen zu optimieren in den jeweiligen Zuständen der Umgebung basierend darauf, welche Belohnungen Agenten bekommen. Und man hat eben diesen drei-Schritte-Prozess, den wir durchgesprochen haben. Einmal die Interaktion, dann die Bewertung des Feedbacks und dann am Ende die Verbesserung des Verhaltens. Und auch angerissen haben wir, dass das Feedback bewertet ist, heißt wir haben keine richtigen Labels, sondern nur so einen ungefähren Indikator, wie gut eine Aktion ist. Das Feedback ist sequentiell, da sich dieser drei-Schritte-Prozess ja über mehrere Zyklen wiederholt und das Feedback ist geserfelt, weil man eben nur sehr spärliche Informationen über diese sehr hochdimensionalen Umgebungen erhält. Heißt man hat keinen Zugriff auf Belohnungsfunktion, Transitionsfunktion etc., sondern der Agent muss durch seine Erfahrungen oder muss basierend auf seinen Erfahrungen generalisieren mit Funktionsapproximation. Und je nachdem, was das Ziel des Agenten ist, das Lernziel des Agenten ist, ist er Policy-Value-Model-Based oder ein Actor-Critic-Agent. Und im Deep Reinforcement Learning haben wir dann eben die Kombination mit neuronalen Netzen, heißt diese Funktionsapproximation ist nicht linear und da kann man zum Beispiel auch Computer Vision neuronale Netze, also CNNs mit verbinden, um Bilddaten verarbeiten zu können.