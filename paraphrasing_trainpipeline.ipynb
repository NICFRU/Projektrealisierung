{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Smr0ok1lbXQJ"
      },
      "source": [
        "# Training Paraphrasing withÂ T5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "quellen\n",
        "https://medium.com/nlplanet/a-full-guide-to-finetuning-t5-for-text2text-and-building-a-demo-with-streamlit-c72009631887\n",
        "https://github.com/hetpandya/paraphrase-datasets-pretrained-models/blob/main/examples/t5_paraphrase_model_training_example.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LDHJ9e3bgQf"
      },
      "source": [
        "## Install libraries and download the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import evaluate\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from transformers import AutoModelForSeq2SeqLM, PegasusForConditionalGeneration, PegasusTokenizer, AutoTokenizer\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "from transformers import pipeline\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "model_name='facebook/bart-large-cnn'\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "summarizer = pipeline(\"text2text-generation\", model=model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVH49J6Dbd7l"
      },
      "source": [
        "## Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "02d4b6b871324b9f92cd549284594225",
            "6291ccc136df4a1b9d011ec738709da2",
            "dde56e8076ae44b28c8583da3ff14113",
            "59f8e3e626e247bba94146336cdea02f",
            "f4fd042a35994f0e9b4e8b8a5a35277c",
            "8200cf9645fd4a7181facbebcb6daef4",
            "6b980b90511d4d869a2bbff97286b53e",
            "327c2b682d2b44be8fb0e61021a4ac48",
            "2613f346dc434001979271278b2b55fe",
            "c7ed4d41f2524651af15054fc601b93b",
            "181ba258d92e49ba8eb2d8fb763f0bdf",
            "c48c5645edc94dfd9ac7c9348af06f01",
            "2ed1cceaa15b4f55b890c483b3029709",
            "246fd9e9c26b474ab8adeb20bdf26e14",
            "b7ec9d1cf5304be6b2d9d856c5250869",
            "b71a79e330c2431cbaf5e13732549b6a",
            "97c990dd1f7f4fd0b66de34898d6fb6d",
            "1f71d5ff86d549da8f5e2adf53b72095",
            "6c712f6809e448b3a310c27a3541e58e",
            "39963f9cb9ee4f9ba80ea253ff74bc4f",
            "c208d387027d4ad99384a1b9efe90ee7",
            "0b20da24f3bc46fa814e26cffea42573"
          ]
        },
        "id": "qo9DNKuHdsl-",
        "outputId": "0c0a4c5f-90dd-4889-ad56-4d37a3c4e943"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>classification</th>\n",
              "      <th>text</th>\n",
              "      <th>sentences</th>\n",
              "      <th>num_sentences</th>\n",
              "      <th>words</th>\n",
              "      <th>num_words</th>\n",
              "      <th>lemmas</th>\n",
              "      <th>stops</th>\n",
              "      <th>num_stops</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Scientific</td>\n",
              "      <td>the interest in anchoring phenomena and phenom...</td>\n",
              "      <td>['the interest in anchoring phenomena and phen...</td>\n",
              "      <td>222</td>\n",
              "      <td>['interest', 'anchoring', 'phenomena', 'phenom...</td>\n",
              "      <td>3533</td>\n",
              "      <td>['interest', 'anchor', 'phenomenon', 'phenomen...</td>\n",
              "      <td>[the, in, and, in, has, been, by, their, in, a...</td>\n",
              "      <td>2426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>Scientific</td>\n",
              "      <td>galaxy clusters , as the largest peaks in the ...</td>\n",
              "      <td>['galaxy clusters , as the largest peaks in th...</td>\n",
              "      <td>372</td>\n",
              "      <td>['galaxy', 'clusters', 'largest', 'peaks', 'co...</td>\n",
              "      <td>8982</td>\n",
              "      <td>['galaxy', 'cluster', 'large', 'peak', 'cosmic...</td>\n",
              "      <td>[as, the, in, the, an, in, and, in, for, and, ...</td>\n",
              "      <td>5768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Scientific</td>\n",
              "      <td>quantum correlations between components of a s...</td>\n",
              "      <td>['quantum correlations between components of a...</td>\n",
              "      <td>138</td>\n",
              "      <td>['quantum', 'correlations', 'components', 'sys...</td>\n",
              "      <td>2451</td>\n",
              "      <td>['quantum', 'correlation', 'component', 'syste...</td>\n",
              "      <td>[between, of, a, or, are, on, the, of, whole, ...</td>\n",
              "      <td>1780</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Scientific</td>\n",
              "      <td>methanol masers are often found in star - form...</td>\n",
              "      <td>['methanol masers are often found in star - fo...</td>\n",
              "      <td>141</td>\n",
              "      <td>['methanol', 'masers', 'found', 'star', 'formi...</td>\n",
              "      <td>3232</td>\n",
              "      <td>['methanol', 'maser', 'find', 'star', 'form', ...</td>\n",
              "      <td>[are, often, in, are, two, of, to, i, most, th...</td>\n",
              "      <td>1940</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>Scientific</td>\n",
              "      <td>interdisciplinary research has recently gained...</td>\n",
              "      <td>['interdisciplinary research has recently gain...</td>\n",
              "      <td>88</td>\n",
              "      <td>['interdisciplinary', 'research', 'recently', ...</td>\n",
              "      <td>2695</td>\n",
              "      <td>['interdisciplinary', 'research', 'recently', ...</td>\n",
              "      <td>[has, a, in, the, of, to, the, in, was, to, tw...</td>\n",
              "      <td>1997</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0 classification  \\\n",
              "0           0     Scientific   \n",
              "1           1     Scientific   \n",
              "2           2     Scientific   \n",
              "3           3     Scientific   \n",
              "4           4     Scientific   \n",
              "\n",
              "                                                text  \\\n",
              "0  the interest in anchoring phenomena and phenom...   \n",
              "1  galaxy clusters , as the largest peaks in the ...   \n",
              "2  quantum correlations between components of a s...   \n",
              "3  methanol masers are often found in star - form...   \n",
              "4  interdisciplinary research has recently gained...   \n",
              "\n",
              "                                           sentences  num_sentences  \\\n",
              "0  ['the interest in anchoring phenomena and phen...            222   \n",
              "1  ['galaxy clusters , as the largest peaks in th...            372   \n",
              "2  ['quantum correlations between components of a...            138   \n",
              "3  ['methanol masers are often found in star - fo...            141   \n",
              "4  ['interdisciplinary research has recently gain...             88   \n",
              "\n",
              "                                               words  num_words  \\\n",
              "0  ['interest', 'anchoring', 'phenomena', 'phenom...       3533   \n",
              "1  ['galaxy', 'clusters', 'largest', 'peaks', 'co...       8982   \n",
              "2  ['quantum', 'correlations', 'components', 'sys...       2451   \n",
              "3  ['methanol', 'masers', 'found', 'star', 'formi...       3232   \n",
              "4  ['interdisciplinary', 'research', 'recently', ...       2695   \n",
              "\n",
              "                                              lemmas  \\\n",
              "0  ['interest', 'anchor', 'phenomenon', 'phenomen...   \n",
              "1  ['galaxy', 'cluster', 'large', 'peak', 'cosmic...   \n",
              "2  ['quantum', 'correlation', 'component', 'syste...   \n",
              "3  ['methanol', 'maser', 'find', 'star', 'form', ...   \n",
              "4  ['interdisciplinary', 'research', 'recently', ...   \n",
              "\n",
              "                                               stops  num_stops  \n",
              "0  [the, in, and, in, has, been, by, their, in, a...       2426  \n",
              "1  [as, the, in, the, an, in, and, in, for, and, ...       5768  \n",
              "2  [between, of, a, or, are, on, the, of, whole, ...       1780  \n",
              "3  [are, often, in, are, two, of, to, i, most, th...       1940  \n",
              "4  [has, a, in, the, of, to, the, in, was, to, tw...       1997  "
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train=pd.read_csv('data/data_train_with_features.csv')\n",
        "df_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create Test und Train Data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def token_count(text):\n",
        "    tokens = text.split()\n",
        "    return len(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def adjust_length(text):\n",
        "    length = token_count(text)\n",
        "    if length < 80:\n",
        "        min_length = length + int(length * 0.05)\n",
        "        max_length = min_length + 50\n",
        "    elif length < 100:\n",
        "        min_length = length + int(length * 0.3)\n",
        "        max_length = min_length + 100\n",
        "    else:\n",
        "        min_length = math.ceil(length / 50) * 70\n",
        "        max_length = min_length + 100\n",
        "    return min_length, max_length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def batch_sent(sentenc,splitt=180,split='. '):\n",
        "    sentences = sentenc.split(split)\n",
        "    # Erstellen Sie Batches von SÃ¤tzen, die weniger als 1024 Tokens enthalten\n",
        "    batches = []\n",
        "    batch = []\n",
        "    batch_len = 0\n",
        "    for sentence in sentences:\n",
        "        \n",
        "        sentence_len = len(tokenizer.tokenize(sentence))\n",
        "        if sentence_len + batch_len > splitt:\n",
        "            if sentence_len < splitt:  # Ã¼berspringen Sie SÃ¤tze, die lÃ¤nger als 256 Tokens sind\n",
        "                batches.append(batch)\n",
        "                batch = [sentence]\n",
        "                batch_len = sentence_len\n",
        "            # wenn ein Satz alleine 1024 Tokens Ã¼berschreitet, wird er Ã¼bersprungen\n",
        "        else:\n",
        "            batch.append(sentence)\n",
        "            batch_len += sentence_len\n",
        "    batches.append(batch)\n",
        "    return batches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def paraphrase_of_text(df_s,text_name='text',komp_name='reduction_multiplier',split='. '):\n",
        "    counter = 0\n",
        "    df_summary_testing = pd.DataFrame(columns=['text', 'Zusammenfassung','Min_Kompressionsrate', 'Max_Kompressionsrate', 'Endgueltige_Kompressionsrate','lÃ¤nge Zusammenfassung','lÃ¤nge Ausgangstext'])\n",
        "    # Teilen Sie den Text in SÃ¤tze --> fÃ¼r tatsÃ¤chliche umsetzubng\n",
        "    soplitting=True\n",
        "    for _, row in tqdm(df_s.iterrows(), total=df_s.shape[0]):\n",
        "     \n",
        "        text = row[text_name]\n",
        "        komp = row[komp_name]\n",
        "        text_gesamt_list=[]\n",
        "\n",
        "        for batch in tqdm(batch_sent(text,split=split), desc='Verarbeite Batches'):\n",
        "            # ZusammenfÃ¼gen der SÃ¤tze in einem Batch\n",
        "            batch_text = '. '.join(batch)\n",
        "            min_length_test, max_length_test = adjust_length(batch_text)\n",
        "            ext_summary=summarizer(batch_text, max_length=int(round(max_length_test*komp,0)), min_length=int(round(min_length_test*komp,0)),length_penalty=100,num_beams=2)\n",
        "            # Erstellen Sie einen DataFrame fÃ¼r die aktuellen Ergebnisse\n",
        "           \n",
        "            text_gesamt_list.append(ext_summary[0]['generated_text'])\n",
        "        text_gesamt = '. '.join(text_gesamt_list)\n",
        "        actual_compression_rate = len(text_gesamt.split(' '))/len(text.split(' '))*100\n",
        "      \n",
        "        df_current = pd.DataFrame({\n",
        "            'text':[text],\n",
        "            'Zusammenfassung': [text_gesamt],\n",
        "            'Min_Kompressionsrate': [min_length_test],\n",
        "            'Max_Kompressionsrate': [max_length_test],\n",
        "            'Endgueltige_Kompressionsrate': [actual_compression_rate],\n",
        "            'lÃ¤nge Zusammenfassung': [len(text_gesamt.split(' '))],\n",
        "            'lÃ¤nge Ausgangstext': [len(text.split(' '))]\n",
        "        \n",
        "        })\n",
        "       \n",
        "        # FÃ¼gen Sie die Daten zum DataFrame hinzu\n",
        "        df_summary_testing = pd.concat([df_summary_testing, df_current], ignore_index=True)\n",
        "        counter += 1\n",
        "        soplitting=False\n",
        "    return df_summary_testing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_compression(df, total_tokens_col, current_tokens_col, desired_compression_rate):\n",
        "    df['current_compression_rate'] = df[current_tokens_col] / df[total_tokens_col]\n",
        "    df['compression_difference'] = desired_compression_rate - df['current_compression_rate']\n",
        "    df['reduction_multiplier'] = desired_compression_rate / df['current_compression_rate']\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_test=calculate_compression(df_test, 'total_tokens', 'token_count', desired_compression_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_summary=paraphrase_of_text(df_test[['text','reduction_multiplier']])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "meFMYGPrbkwp"
      },
      "source": [
        "## Dataset train/validation/test split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FPsvfzq-uAKc"
      },
      "outputs": [],
      "source": [
        "# datasets_train_test = \n",
        "# datasets_train_validation = datasets_train_test[\"train\"].train_test_split(test_size=3000)\n",
        "\n",
        "# datasets[\"train\"] = datasets_train_validation[\"train\"]\n",
        "# datasets[\"validation\"] = datasets_train_validation[\"test\"]\n",
        "# datasets[\"test\"] = datasets_train_test[\"test\"]\n",
        "\n",
        "# datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4R2UP-FovZBe"
      },
      "outputs": [],
      "source": [
        "n_samples_train = len(datasets[\"train\"])\n",
        "n_samples_validation = len(datasets[\"validation\"])\n",
        "n_samples_test = len(datasets[\"test\"])\n",
        "n_samples_total = n_samples_train + n_samples_validation + n_samples_test\n",
        "\n",
        "print(f\"- Training set: {n_samples_train*100/n_samples_total:.2f}%\")\n",
        "print(f\"- Validation set: {n_samples_validation*100/n_samples_total:.2f}%\")\n",
        "print(f\"- Test set: {n_samples_test*100/n_samples_total:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYW2j-_l4FwR"
      },
      "outputs": [],
      "source": [
        "# keep only a subsample of the datasets\n",
        "datasets[\"train\"] = datasets[\"train\"].shuffle().select(range(100000))\n",
        "datasets[\"validation\"] = datasets[\"validation\"].shuffle().select(range(1000))\n",
        "datasets[\"test\"] = datasets[\"test\"].shuffle().select(range(1000))\n",
        "\n",
        "datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMlba5F2bqTQ"
      },
      "source": [
        "## Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4w4QyMc2wF4z"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import string\n",
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXE4liMzxJVu"
      },
      "outputs": [],
      "source": [
        "model_checkpoint = \"facebook/bart-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AksRL_QyyYZj"
      },
      "outputs": [],
      "source": [
        "prefix = \"text2text-generation: \"\n",
        "\n",
        "max_input_length = 512\n",
        "max_target_length = 64\n",
        "\n",
        "def clean_text(text):\n",
        "  sentences = nltk.sent_tokenize(text.strip())\n",
        "  sentences_cleaned = [s for sent in sentences for s in sent.split(\"\\n\")]\n",
        "  sentences_cleaned_no_titles = [sent for sent in sentences_cleaned\n",
        "                                 if len(sent) > 0 and\n",
        "                                 sent[-1] in string.punctuation]\n",
        "  text_cleaned = \"\\n\".join(sentences_cleaned_no_titles)\n",
        "  return text_cleaned\n",
        "\n",
        "def preprocess_data(examples):\n",
        "  texts_cleaned = [clean_text(text) for text in examples[\"text\"]]\n",
        "  inputs = [prefix + text for text in texts_cleaned]\n",
        "  model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
        "\n",
        "  # Setup the tokenizer for targets\n",
        "  with tokenizer.as_target_tokenizer():\n",
        "    labels = tokenizer(examples[\"title\"], max_length=max_target_length,\n",
        "                       truncation=True)\n",
        "\n",
        "  model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "  return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsEaq2Um3HcD"
      },
      "outputs": [],
      "source": [
        "datasets_cleaned = datasets.filter(lambda example: (len(example['text']) >= 500) and (len(example['title']) >= 20))\n",
        "tokenized_datasets = datasets_cleaned.map(preprocess_data, batched=True)\n",
        "tokenized_datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Xc_wx8WbtJ4"
      },
      "source": [
        "## Fine-tune bart"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0SW4HFx4wzG"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73iDTHHYgVTN"
      },
      "outputs": [],
      "source": [
        "!rm -r {model_dir}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjLfz7b6cVHJ"
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "model_name = \"bart-base-paraphrasing\"\n",
        "model_dir = f\"drive/MyDrive/Models/{model_name}\"\n",
        "args = Seq2SeqTrainingArguments(\n",
        "    model_dir,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=100,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=100,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=200,\n",
        "    learning_rate=4e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=1,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"rouge1\",\n",
        "    report_to=\"tensorboard\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joWGLoZ8dOIS"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorForSeq2Seq(tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qYq_4DWjdXYa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "metric = load_metric(\"rouge\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100 in the labels as we can't decode them.\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # Rouge expects a newline after each sentence\n",
        "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip()))\n",
        "                      for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip()))\n",
        "                      for label in decoded_labels]\n",
        "\n",
        "    # Compute ROUGE scores\n",
        "    result = metric.compute(predictions=decoded_preds, references=decoded_labels,\n",
        "                            use_stemmer=True)\n",
        "\n",
        "    # Extract ROUGE f1 scores\n",
        "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
        "\n",
        "    # Add mean generated length to metrics\n",
        "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id)\n",
        "                      for pred in predictions]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "    return {k: round(v, 4) for k, v in result.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODp1f0ZkdadY"
      },
      "outputs": [],
      "source": [
        "# Function that returns an untrained model to be trained\n",
        "def model_init():\n",
        "    return AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model_init=model_init,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tq8R43Ls8-EU"
      },
      "outputs": [],
      "source": [
        "# Start TensorBoard before training to monitor it in progress\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir '{model_dir}'/runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOeZoPmudwiv"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4PIcpIeoDW7"
      },
      "outputs": [],
      "source": [
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOuRWgO1F0Xc"
      },
      "source": [
        "## Load the model from GDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LNiSsHt1oRmN"
      },
      "outputs": [],
      "source": [
        "model_name = \"t5-base-medium-title-generation/checkpoint-2000\"\n",
        "model_dir = f\"drive/MyDrive/Models/{model_name}\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_dir)\n",
        "\n",
        "max_input_length = 512"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COdutSc1Oel9"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "We define access to a Streamlit app in a browser tab as a session.\n",
        "For each browser tab that connects to the Streamlit server, a new session is created.\n",
        "Streamlit reruns your script from top to bottom every time you interact with your app.\n",
        "Each reruns takes place in a blank slate: no variables are shared between runs.\n",
        "Session State is a way to share variables between reruns, for each user session.\n",
        "In addition to the ability to store and persist state, Streamlit also exposes the\n",
        "ability to manipulate state using Callbacks. In this guide, we will illustrate the\n",
        "usage of Session State and Callbacks as we build a stateful Counter app.\n",
        "For details on the Session State and Callbacks API, please refer to our Session\n",
        "State API Reference Guide. Also, check out this Session State basics tutorial\n",
        "video by Streamlit Developer Advocate Dr. Marisa Smith to get started:\n",
        "\"\"\"\n",
        "\n",
        "inputs = [\"summarize: \" + text]\n",
        "\n",
        "inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, return_tensors=\"pt\")\n",
        "output = model.generate(**inputs, num_beams=8, do_sample=True, min_length=10, max_length=64)\n",
        "decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
        "predicted_title = nltk.sent_tokenize(decoded_output.strip())[0]\n",
        "\n",
        "print(predicted_title)\n",
        "# Session State and Callbacks in Streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqH3hlZg_3HE"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "Many financial institutions started building conversational AI, prior to the Covid19\n",
        "pandemic, as part of a digital transformation initiative. These initial solutions\n",
        "were high profile, highly personalized virtual assistants â like the Erica chatbot\n",
        "from Bank of America. As the pandemic hit, the need changed as contact centers were\n",
        "under increased pressures. As Cathal McGloin of ServisBOT explains in âhow it started,\n",
        "and how it is going,â financial institutions were looking for ways to automate\n",
        "solutions to help get back to ânormalâ levels of customer service. This resulted\n",
        "in a change from the âfuture of conversational AIâ to a real tactical assistant\n",
        "that can help in customer service. Haritha Dev of Wells Fargo, saw a similar trend.\n",
        "Banks were originally looking to conversational AI as part of digital transformation\n",
        "to keep up with the times. However, with the pandemic, it has been more about\n",
        "customer retention and customer satisfaction. In addition, new use cases came about\n",
        "as a result of Covid-19 that accelerated adoption of conversational AI. As Vinita\n",
        "Kumar of Deloitte points out, banks were dealing with an influx of calls about new\n",
        "concerns, like questions around the Paycheck Protection Program (PPP) loans. This\n",
        "resulted in an increase in volume, without enough agents to assist customers, and\n",
        "tipped the scale to incorporate conversational AI. When choosing initial use cases\n",
        "to support, financial institutions often start with high volume, low complexity\n",
        "tasks. For example, password resets, checking account balances, or checking the\n",
        "status of a transaction, as Vinita points out. From there, the use cases can evolve\n",
        "as the banks get more mature in developing conversational AI, and as the customers\n",
        "become more engaged with the solutions. Cathal indicates another good way for banks\n",
        "to start is looking at use cases that are a pain point, and also do not require a\n",
        "lot of IT support. Some financial institutions may have a multi-year technology\n",
        "roadmap, which can make it harder to get a new service started. A simple chatbot\n",
        "for document collection in an onboarding process can result in high engagement,\n",
        "and a high return on investment. For example, Cathal has a banking customer that\n",
        "implemented a chatbot to capture a driverâs license to be used in the verification\n",
        "process of adding an additional user to an account â it has over 85% engagement\n",
        "with high satisfaction. An interesting use case Haritha discovered involved\n",
        "educating customers on financial matters. People feel more comfortable asking a\n",
        "chatbot what might be considered a âdumbâ question, as the chatbot is less judgmental.\n",
        "Users can be more ambiguous with their questions as well, not knowing the right\n",
        "words to use, as chatbot can help narrow things down.\n",
        "\"\"\"\n",
        "\n",
        "inputs = [\"summarize: \" + text]\n",
        "\n",
        "inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, return_tensors=\"pt\")\n",
        "output = model.generate(**inputs, num_beams=8, do_sample=True, min_length=10, max_length=64)\n",
        "decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
        "predicted_title = nltk.sent_tokenize(decoded_output.strip())[0]\n",
        "\n",
        "print(predicted_title)\n",
        "# Conversational AI: The Future of Customer Service"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvilrwiEezhD"
      },
      "source": [
        "## Upload the model to the Hugging Space Hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DdcAcVojw6z"
      },
      "source": [
        "https://huggingface.co/docs/transformers/model_sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DnuA0tggwhW"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade jax jaxlib # CPU-only\n",
        "!pip install flax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVj3RpFieol9"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyZQIZSHfpPI"
      },
      "outputs": [],
      "source": [
        "from transformers import T5ForConditionalGeneration, TFT5ForConditionalGeneration, FlaxT5ForConditionalGeneration\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
        "pt_model = T5ForConditionalGeneration.from_pretrained(model_dir)\n",
        "tf_model = TFT5ForConditionalGeneration.from_pretrained(model_dir, from_pt=True)\n",
        "flax_model = FlaxT5ForConditionalGeneration.from_pretrained(model_dir, from_pt=True)\n",
        "\n",
        "tokenizer.push_to_hub(model_name)\n",
        "pt_model.push_to_hub(model_name)\n",
        "tf_model.push_to_hub(model_name)\n",
        "flax_model.push_to_hub(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqDj8iX_kQD1"
      },
      "source": [
        "## Load the model from the Hugging Face Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQHVTHJTkYyR"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"fabiochiu/t5-small-medium-title-generation\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"fabiochiu/t5-small-medium-title-generation\")\n",
        "\n",
        "text = \"\"\"\n",
        "Many financial institutions started building conversational AI, prior to the Covid19\n",
        "pandemic, as part of a digital transformation initiative. These initial solutions\n",
        "were high profile, highly personalized virtual assistants â like the Erica chatbot\n",
        "from Bank of America. As the pandemic hit, the need changed as contact centers were\n",
        "under increased pressures. As Cathal McGloin of ServisBOT explains in âhow it started,\n",
        "and how it is going,â financial institutions were looking for ways to automate\n",
        "solutions to help get back to ânormalâ levels of customer service. This resulted\n",
        "in a change from the âfuture of conversational AIâ to a real tactical assistant\n",
        "that can help in customer service. Haritha Dev of Wells Fargo, saw a similar trend.\n",
        "Banks were originally looking to conversational AI as part of digital transformation\n",
        "to keep up with the times. However, with the pandemic, it has been more about\n",
        "customer retention and customer satisfaction. In addition, new use cases came about\n",
        "as a result of Covid-19 that accelerated adoption of conversational AI. As Vinita\n",
        "Kumar of Deloitte points out, banks were dealing with an influx of calls about new\n",
        "concerns, like questions around the Paycheck Protection Program (PPP) loans. This\n",
        "resulted in an increase in volume, without enough agents to assist customers, and\n",
        "tipped the scale to incorporate conversational AI. When choosing initial use cases\n",
        "to support, financial institutions often start with high volume, low complexity\n",
        "tasks. For example, password resets, checking account balances, or checking the\n",
        "status of a transaction, as Vinita points out. From there, the use cases can evolve\n",
        "as the banks get more mature in developing conversational AI, and as the customers\n",
        "become more engaged with the solutions. Cathal indicates another good way for banks\n",
        "to start is looking at use cases that are a pain point, and also do not require a\n",
        "lot of IT support. Some financial institutions may have a multi-year technology\n",
        "roadmap, which can make it harder to get a new service started. A simple chatbot\n",
        "for document collection in an onboarding process can result in high engagement,\n",
        "and a high return on investment. For example, Cathal has a banking customer that\n",
        "implemented a chatbot to capture a driverâs license to be used in the verification\n",
        "process of adding an additional user to an account â it has over 85% engagement\n",
        "with high satisfaction. An interesting use case Haritha discovered involved\n",
        "educating customers on financial matters. People feel more comfortable asking a\n",
        "chatbot what might be considered a âdumbâ question, as the chatbot is less judgmental.\n",
        "Users can be more ambiguous with their questions as well, not knowing the right\n",
        "words to use, as chatbot can help narrow things down.\n",
        "\"\"\"\n",
        "\n",
        "inputs = [\"summarize: \" + text]\n",
        "\n",
        "inputs = tokenizer(inputs, max_length=max_input_length, truncation=True, return_tensors=\"pt\")\n",
        "output = model.generate(**inputs, num_beams=8, do_sample=True, min_length=10, max_length=64)\n",
        "decoded_output = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n",
        "predicted_title = nltk.sent_tokenize(decoded_output.strip())[0]\n",
        "\n",
        "print(predicted_title)\n",
        "# Conversational AI: The Future of Customer Service"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_WaVGKccca9"
      },
      "source": [
        "## Evaluate the model on the test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LsPh50j4qdFd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# get test split\n",
        "test_tokenized_dataset = tokenized_datasets[\"test\"]\n",
        "\n",
        "# pad texts to the same length\n",
        "def preprocess_test(examples):\n",
        "  inputs = [prefix + text for text in examples[\"text\"]]\n",
        "  model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True,\n",
        "                           padding=\"max_length\")\n",
        "  return model_inputs\n",
        "\n",
        "test_tokenized_dataset = test_tokenized_dataset.map(preprocess_test, batched=True)\n",
        "\n",
        "# prepare dataloader\n",
        "test_tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
        "dataloader = torch.utils.data.DataLoader(test_tokenized_dataset, batch_size=32)\n",
        "\n",
        "# generate text for each batch\n",
        "all_predictions = []\n",
        "for i,batch in enumerate(dataloader):\n",
        "  predictions = model.generate(**batch)\n",
        "  all_predictions.append(predictions)\n",
        "\n",
        "# flatten predictions\n",
        "all_predictions_flattened = [pred for preds in all_predictions for pred in preds]\n",
        "\n",
        "# tokenize and pad titles\n",
        "all_titles = tokenizer(test_tokenized_dataset[\"title\"], max_length=max_target_length,\n",
        "                       truncation=True, padding=\"max_length\")[\"input_ids\"]\n",
        "\n",
        "# compute metrics\n",
        "predictions_labels = [all_predictions_flattened, all_titles]\n",
        "compute_metrics(predictions_labels)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.15 ('torch')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "63be3639594356bc87fe58051c1d1c5221c23a964c31c0e05d208c4974bedf26"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "02d4b6b871324b9f92cd549284594225": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6291ccc136df4a1b9d011ec738709da2",
              "IPY_MODEL_dde56e8076ae44b28c8583da3ff14113",
              "IPY_MODEL_59f8e3e626e247bba94146336cdea02f"
            ],
            "layout": "IPY_MODEL_f4fd042a35994f0e9b4e8b8a5a35277c"
          }
        },
        "0b20da24f3bc46fa814e26cffea42573": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "181ba258d92e49ba8eb2d8fb763f0bdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f71d5ff86d549da8f5e2adf53b72095": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "246fd9e9c26b474ab8adeb20bdf26e14": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c712f6809e448b3a310c27a3541e58e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_39963f9cb9ee4f9ba80ea253ff74bc4f",
            "value": 0
          }
        },
        "2613f346dc434001979271278b2b55fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2ed1cceaa15b4f55b890c483b3029709": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97c990dd1f7f4fd0b66de34898d6fb6d",
            "placeholder": "â",
            "style": "IPY_MODEL_1f71d5ff86d549da8f5e2adf53b72095",
            "value": "Extracting data files:   0%"
          }
        },
        "327c2b682d2b44be8fb0e61021a4ac48": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39963f9cb9ee4f9ba80ea253ff74bc4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "59f8e3e626e247bba94146336cdea02f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7ed4d41f2524651af15054fc601b93b",
            "placeholder": "â",
            "style": "IPY_MODEL_181ba258d92e49ba8eb2d8fb763f0bdf",
            "value": " 1/1 [00:00&lt;00:00, 11.74it/s]"
          }
        },
        "6291ccc136df4a1b9d011ec738709da2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8200cf9645fd4a7181facbebcb6daef4",
            "placeholder": "â",
            "style": "IPY_MODEL_6b980b90511d4d869a2bbff97286b53e",
            "value": "Downloading data files: 100%"
          }
        },
        "6b980b90511d4d869a2bbff97286b53e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6c712f6809e448b3a310c27a3541e58e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8200cf9645fd4a7181facbebcb6daef4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97c990dd1f7f4fd0b66de34898d6fb6d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b71a79e330c2431cbaf5e13732549b6a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7ec9d1cf5304be6b2d9d856c5250869": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c208d387027d4ad99384a1b9efe90ee7",
            "placeholder": "â",
            "style": "IPY_MODEL_0b20da24f3bc46fa814e26cffea42573",
            "value": " 0/1 [00:00&lt;?, ?it/s]"
          }
        },
        "c208d387027d4ad99384a1b9efe90ee7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c48c5645edc94dfd9ac7c9348af06f01": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2ed1cceaa15b4f55b890c483b3029709",
              "IPY_MODEL_246fd9e9c26b474ab8adeb20bdf26e14",
              "IPY_MODEL_b7ec9d1cf5304be6b2d9d856c5250869"
            ],
            "layout": "IPY_MODEL_b71a79e330c2431cbaf5e13732549b6a"
          }
        },
        "c7ed4d41f2524651af15054fc601b93b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dde56e8076ae44b28c8583da3ff14113": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_327c2b682d2b44be8fb0e61021a4ac48",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2613f346dc434001979271278b2b55fe",
            "value": 1
          }
        },
        "f4fd042a35994f0e9b4e8b8a5a35277c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
