{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en\n",
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install tensorflow\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import networkx as nx\n",
    "import spacy\n",
    "from summa import keywords\n",
    "from summa.summarizer import summarize\n",
    "\n",
    "# Laden des Spacy-Modells\n",
    "import evaluate\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import AutoModelForSeq2SeqLM, PegasusForConditionalGeneration, PegasusTokenizer, AutoTokenizer\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print(x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name='facebook/bart-large-cnn'\n",
    "model_name='facebook/bart-large'\n",
    "#model_name='NICFRU/bart-base-paraphrasing'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "summarizer = pipeline(\"text2text-generation\", model=model_name,device='mps:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reduce_repetitions(text):\n",
    "    text = re.sub(r'\\.{2,}', '.', text)\n",
    "    text = re.sub(r'\\!{2,}', '!', text)\n",
    "    text = re.sub(r'\\,{2,}', ',', text)\n",
    "    text = re.sub(r'\\;{2,}', ';', text)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def textrank_extractive(text, compression_rate=0.5,split='\\. '):\n",
    "    # Tokenisierung\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    #doc = nlp(text.replace(\"\\n\\n\", \" \"))\n",
    "\n",
    "    # Split the text at each \". \" that is not followed by a single letter\n",
    "    doc = re.split(fr'(?<!\\b\\w\\w){split}', reduce_repetitions(re.sub(' +', ' ', text.replace(\"\\n\", \" \").replace('-',' ').replace('_',' ').replace(\"\\'\", \"\").replace(\"!\", \".\").replace(\"?\", \".\").replace(\";\", \"\"))))\n",
    "    sentences = [sent for sent in doc if len(sent.replace(\"-\", \" \").split()) > 2]\n",
    "\n",
    "    # Speichern der Spacy-Dokumente der Sätze für spätere Verwendung\n",
    "    sentence_docs = [nlp(sentence) for sentence in sentences]\n",
    "\n",
    "    # Extrahiere Schlüsselsätze mit TextRank\n",
    "    num_sentences = max(1, int(len(sentences) * compression_rate))\n",
    "    extracted_sentences = summarize(text, words=num_sentences, split=True)\n",
    "\n",
    "    # Erzeuge eine Matrix mit den Ähnlichkeiten zwischen den Sätzen\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "    for i, doc_i in enumerate(sentence_docs):\n",
    "        for j, doc_j in enumerate(sentence_docs):\n",
    "            similarity = similarity_function(doc_i, doc_j)\n",
    "            similarity_matrix[i, j] = similarity\n",
    "    \n",
    "    # Konstruiere einen Graphen mit den Sätzen als Knoten und den Ähnlichkeiten als Kanten\n",
    "    graph = nx.from_numpy_array(similarity_matrix)\n",
    "\n",
    "    # Berechne den TextRank-Score für jeden Satz\n",
    "    scores = nx.pagerank_numpy(graph)\n",
    "\n",
    "    # Wähle die besten Sätze basierend auf ihren TextRank-Scores aus\n",
    "    top_sentences = sorted(scores, key=scores.get, reverse=True)[:num_sentences]\n",
    "\n",
    "    # Sortiere die ausgewählten Sätze nach ihrer Position im Text\n",
    "    top_sentences = sorted(top_sentences)\n",
    "\n",
    "    # Gebe die extrahierten Schlüsselsätze zurück\n",
    "    extracted_sentences = [sentences[index] for index in top_sentences]\n",
    "    return extracted_sentences\n",
    "\n",
    "\n",
    "def similarity_function(doc1, doc2):\n",
    "    # Berechne die Cosinus-Ähnlichkeit zwischen den beiden Dokumenten\n",
    "    similarity = doc1.similarity(doc2)\n",
    "    return similarity\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compression_ratio(text, summary):\n",
    "    # Berechne das Verhältnis der Anzahl der Wörter in der Zusammenfassung zur Anzahl der Wörter im Ausgangstext\n",
    "    num_words_text = len(text.split())\n",
    "    num_words_summary = len(summary.split())\n",
    "    ratio = num_words_summary / num_words_text\n",
    "    return ratio\n",
    "\n",
    "\n",
    "\n",
    "def compression(text, compression_rate,split='\\. '):\n",
    "    max_iterations = 20\n",
    "    iterations = 0\n",
    "    #compression_rate -= 0.05\n",
    "    \n",
    "    extracted = textrank_extractive(text, compression_rate,split)\n",
    "    summary = '. '.join(extracted)\n",
    "    compression_rate_renwed = compression_rate\n",
    "\n",
    "\n",
    "    while compression_ratio(text, summary) < compression_rate and iterations < max_iterations:\n",
    "        iterations += 1\n",
    "        compression_rate_renwed += 0.05\n",
    "        if compression_rate_renwed > 1:\n",
    "            compression_rate_renwed = 1\n",
    "        extracted = textrank_extractive(text, compression_rate=compression_rate_renwed)\n",
    "        summary = '. '.join(extracted)\n",
    "    return summary\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_count(text):\n",
    "    tokens = text.split()\n",
    "    return len(tokens)\n",
    "\n",
    "\n",
    "def adjust_length(text):\n",
    "    length = token_count(text)\n",
    "    if length <20:\n",
    "        min_length = length + int(length * 0.05)\n",
    "        max_length = min_length +min_length\n",
    "    elif length <50:\n",
    "        min_length = length + int(length * 0.05)\n",
    "        max_length = min_length +min_length* 0.5\n",
    "    elif length <60:\n",
    "        min_length = length + int(length * 0.05)\n",
    "        max_length = min_length +min_length* 0.4\n",
    "    elif length < 80:\n",
    "        min_length = length + int(length * 0.05)\n",
    "        max_length = min_length + min_length* 0.25\n",
    "    elif length < 100:\n",
    "        min_length = length + int(length * 0.3)\n",
    "        max_length = min_length + 100\n",
    "    else:\n",
    "        min_length = math.ceil(length / 50) * 70\n",
    "        max_length = min_length + 100\n",
    "    return min_length, max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_sent(sentenc,splitt=180,split='\\. '):\n",
    "    \n",
    "    sentences = re.split(fr'(?<!\\b\\w\\w){split}', sentenc.lower())\n",
    "\n",
    "    # Erstellen Sie Batches von Sätzen, die weniger als 1024 Tokens enthalten\n",
    "    batches = []\n",
    "    batch = []\n",
    "    batch_len = 0\n",
    "    for sentence in sentences:\n",
    "        \n",
    "        sentence_len = len(tokenizer.tokenize(sentence))\n",
    "        if sentence_len + batch_len > splitt:\n",
    "            if sentence_len < splitt:  # überspringen Sie Sätze, die länger als 256 Tokens sind\n",
    "                batches.append(batch)\n",
    "                batch = [sentence]\n",
    "                batch_len = sentence_len\n",
    "            # wenn ein Satz alleine 1024 Tokens überschreitet, wird er übersprungen\n",
    "        else:\n",
    "            batch.append(sentence)\n",
    "            batch_len += sentence_len\n",
    "    batches.append(batch)\n",
    "    return batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase_of_text(df_s,text_name='text',komp_name='reduction_multiplier',split='\\. '):\n",
    "\n",
    "    df_summary_testing = pd.DataFrame(columns=['Zusammenfassung','Min_Kompressionsrate', 'Max_Kompressionsrate', 'Endgueltige_Kompressionsrate','länge Zusammenfassung','länge Ausgangstext','batch_texts','batch_output'])\n",
    "    # Teilen Sie den Text in Sätze --> für tatsächliche umsetzubng\n",
    "    soplitting=True\n",
    "    \n",
    "    for _, row in tqdm(df_s.iterrows(), total=df_s.shape[0]):\n",
    "        batch_text_list=[]\n",
    "\n",
    "        output_text_list=[]\n",
    "        text = row[text_name]\n",
    "        komp = row[komp_name]\n",
    "        text_gesamt_list=[]\n",
    "\n",
    "        for batch in tqdm(batch_sent(text,split=split), desc='Verarbeite Batches'):\n",
    "            # Zusammenfügen der Sätze in einem Batch\n",
    "            #print(batch)\n",
    "            if len(batch):\n",
    "                #print('Läuft')\n",
    "                batch_text = '. '.join(batch)\n",
    "                batch_text += \".\"\n",
    "                batch_text_list.append(batch_text)\n",
    "                min_length_test, max_length_test = adjust_length(batch_text)\n",
    "                ext_summary=summarizer(batch_text, max_length=int(round(max_length_test*komp,0)), min_length=int(round(min_length_test*komp,0)),length_penalty=100,num_beams=2)\n",
    "            # Erstellen Sie einen read_csv für die aktuellen Ergebnisse\n",
    "\n",
    "            text_gesamt_list.append(ext_summary[0]['generated_text'])\n",
    "         \n",
    "        text_gesamt = '. '.join(text_gesamt_list)\n",
    "        actual_compression_rate = len(text_gesamt.split(' '))/len(text.split(' '))*100\n",
    "      \n",
    "        df_current = pd.DataFrame({\n",
    "            'Zusammenfassung': [text_gesamt],\n",
    "            'Min_Kompressionsrate': [min_length_test],\n",
    "            'Max_Kompressionsrate': [max_length_test],\n",
    "            'Endgueltige_Kompressionsrate': [actual_compression_rate],\n",
    "            'länge Zusammenfassung': [len(text_gesamt.split(' '))],\n",
    "            'länge Ausgangstext': [len(text.split(' '))],\n",
    "            'batch_texts': [batch_text_list],\n",
    "            'batch_output': [text_gesamt_list]\n",
    "        \n",
    "        })\n",
    "       \n",
    "        # Fügen Sie die Daten zum DataFrame hinzu\n",
    "        df_summary_testing = pd.concat([df_summary_testing, df_current], ignore_index=True)\n",
    "      \n",
    "    return df_summary_testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_compression(df, total_tokens_col, current_tokens_col, desired_compression_rate):\n",
    "    df['current_compression_rate'] = df[current_tokens_col] / df[total_tokens_col]\n",
    "    df['compression_difference'] = df[desired_compression_rate] - df['current_compression_rate']\n",
    "    df['reduction_multiplier'] = df[desired_compression_rate] / df['current_compression_rate']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_rank_algo(df,seed=10,split='\\. ',random_T=True,column='text'):\n",
    "    df_return = pd.DataFrame(columns=['text','text_rank_text','tokens_gesamt', 'token_text_rank','desired_compression_rate','text_rank_compression_rate'])\n",
    "    # Schleife zur Generierung der zufälligen Werte\n",
    "    random.seed(seed)\n",
    "    for index, row in tqdm(df.iterrows()):\n",
    "        text = row[column].replace(\"\\n\", \" \")\n",
    "        if random_T:\n",
    "            random_value = round(random.uniform(0.2, 0.8), 2)  # Zufälliger Wert zwischen 0.2 und 0.8 auf zwei Stellen nach dem Komma begrenzt\n",
    "        else:\n",
    "            if row['reduction_multiplier']<0.8:\n",
    "                random_value=row['desired_compression_rate']\n",
    "            elif row['reduction_multiplier']<0.9:\n",
    "                random_value=row['reduction_multiplier']\n",
    "            else:\n",
    "                random_value=1\n",
    "        text_rank_text= compression(text.replace(\"\\n\\n\", \" \"),random_value,split)\n",
    "        compression_ratio_value=compression_ratio(text, compression(text,random_value,split))\n",
    "        text=re.sub(' +', ' ', text.replace(\"\\n\", \" \").replace('-',' ').replace('_',' ').replace(\"\\'\", \"\").replace(\"!\", \".\").replace(\"?\", \".\").replace(\";\", \"\"))\n",
    "        text_rank_text=re.sub(' +', ' ', text_rank_text.replace(\"\\n\", \" \").replace('-',' ').replace('_',' ').replace(\"\\'\", \"\").replace(\"!\", \".\").replace(\"?\", \".\").replace(\";\", \"\"))\n",
    "        df_current = pd.DataFrame({\n",
    "                'text':[text],\n",
    "                'text_rank_text': [text_rank_text],\n",
    "                'tokens_gesamt': [len(text.split(' '))],\n",
    "                'token_text_rank': [len(text_rank_text.split(' '))],\n",
    "                'desired_compression_rate': [random_value],\n",
    "                'text_rank_compression_rate': [compression_ratio_value]\n",
    "            })\n",
    "            # Fügen Sie die Daten zum DataFrame hinzu\n",
    "        df_return = pd.concat([df_return, df_current], ignore_index=True)\n",
    "    return df_return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_text_gen(df,split='\\. ',seed=10):\n",
    "    rank_df=text_rank_algo(df,seed=seed,split=split)\n",
    "    df_zwischen=calculate_compression(rank_df, 'tokens_gesamt', 'token_text_rank', 'desired_compression_rate')\n",
    "    df_sum=paraphrase_of_text(df_zwischen[['text_rank_text','reduction_multiplier']],text_name='text_rank_text',split=split)\n",
    "    merged_df = pd.concat([df_sum, df_zwischen], axis=1)\n",
    "    merged_df['ent_com_rate']=merged_df['länge Zusammenfassung'] / merged_df['tokens_gesamt']\n",
    "    new_df=calculate_compression(merged_df[['Zusammenfassung','länge Zusammenfassung','text','tokens_gesamt','desired_compression_rate','ent_com_rate']], 'tokens_gesamt', 'länge Zusammenfassung', 'desired_compression_rate')\n",
    "    df_testen=text_rank_algo(new_df,random_T=False,column = 'Zusammenfassung')\n",
    "    a=pd.concat([df_testen[['text_rank_text','token_text_rank',]], new_df[['Zusammenfassung','länge Zusammenfassung','text','tokens_gesamt','desired_compression_rate','ent_com_rate'\t]]], axis=1)\n",
    "    a['ent_com_rate']=a['länge Zusammenfassung'] / a['tokens_gesamt']\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 64-bit ('anaconda3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
