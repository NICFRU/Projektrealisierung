{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en\n",
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install tensorflow\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import networkx as nx\n",
    "import spacy\n",
    "from summa import keywords\n",
    "from summa.summarizer import summarize\n",
    "\n",
    "# Laden des Spacy-Modells\n",
    "import evaluate\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from transformers import AutoModelForSeq2SeqLM, PegasusForConditionalGeneration, PegasusTokenizer, AutoTokenizer\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from transformers import pipeline\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print(x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4735542ec6914492a1dfe362278f8bbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/558M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#model_name='facebook/bart-large-cnn'\n",
    "#model_name='facebook/bart-large'\n",
    "#model_name='NICFRU/bart-base-paraphrasing'\n",
    "#model_name='NICFRU/bart-base-paraphrasing-science'\n",
    "#model_name='NICFRU/bart-base-paraphrasing-news'\n",
    "#model_name='NICFRU/bart-base-paraphrasing-story'\n",
    "model_name='NICFRU/bart-base-paraphrasing-review'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "summarizer = pipeline(\"text2text-generation\", model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_repetitions(text):\n",
    "    # Wir benutzen reguläre Ausdrücke (regex), um Wiederholungen von Zeichen zu reduzieren.\n",
    "    text = re.sub(r'\\.{2,}', '.', text)  # Ersetzt zwei oder mehr Punkte durch einen Punkt.\n",
    "    text = re.sub(r'\\!{2,}', '!', text)  # Ersetzt zwei oder mehr Ausrufezeichen durch ein Ausrufezeichen.\n",
    "    text = re.sub(r'\\,{2,}', ',', text)  # Ersetzt zwei oder mehr Kommas durch ein Komma.\n",
    "    text = re.sub(r'\\;{2,}', ';', text)  # Ersetzt zwei oder mehr Semikolons durch ein Semikolon.\n",
    "    return text  # Gibt den bereinigten Text zurück.\n",
    "\n",
    "def textrank_extractive(text, compression_rate=0.5,split='\\. '):\n",
    "    # Hier verwenden wir Spacy, um den Text in Sätze zu zerlegen und zu tokenisieren.\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    doc = re.split(fr'(?<!\\b\\w\\w){split}', reduce_repetitions(re.sub(' +', ' ', text.replace(\"\\n\", \" \").replace('-',' ').replace('_',' ').replace(\"\\'\", \"\").replace(\"!\", \".\").replace(\"?\", \".\").replace(\";\", \"\"))))\n",
    "    sentences = [sent for sent in doc if len(sent.replace(\"-\", \" \").split()) > 2]\n",
    "    sentence_docs = [nlp(sentence) for sentence in sentences]\n",
    "\n",
    "    # Hier verwenden wir TextRank, um wichtige Sätze zu extrahieren.\n",
    "    num_sentences = max(1, int(len(sentences) * compression_rate))\n",
    "    extracted_sentences = summarize(text, words=num_sentences, split=True)\n",
    "\n",
    "    # Wir bauen eine Matrix auf, die die Ähnlichkeit zwischen den Sätzen misst.\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "    for i, doc_i in enumerate(sentence_docs):\n",
    "        for j, doc_j in enumerate(sentence_docs):\n",
    "            similarity = similarity_function(doc_i, doc_j)\n",
    "            similarity_matrix[i, j] = similarity\n",
    "\n",
    "    # Hier erstellen wir einen Graphen, in dem die Sätze die Knoten und die Ähnlichkeiten die Kanten sind.\n",
    "    graph = nx.from_numpy_array(similarity_matrix)\n",
    "\n",
    "    # Nun berechnen wir den TextRank-Score für jeden Satz.\n",
    "    scores = nx.pagerank_numpy(graph)\n",
    "\n",
    "    # Wir wählen die besten Sätze basierend auf ihren TextRank-Scores aus.\n",
    "    top_sentences = sorted(scores, key=scores.get, reverse=True)[:num_sentences]\n",
    "\n",
    "    # Die ausgewählten Sätze werden nach ihrer Position im Text sortiert.\n",
    "    top_sentences = sorted(top_sentences)\n",
    "\n",
    "    # Schließlich geben wir die extrahierten Schlüsselsätze zurück.\n",
    "    extracted_sentences = [sentences[index] for index in top_sentences]\n",
    "    return extracted_sentences\n",
    "\n",
    "def similarity_function(doc1, doc2):\n",
    "    # Wir berechnen die Cosinus-Ähnlichkeit zwischen den beiden Dokumenten.\n",
    "    similarity = doc1.similarity(doc2)\n",
    "    return similarity\n",
    "\n",
    "def compression_ratio(text, summary):\n",
    "    # Wir berechnen das Verhältnis der Anzahl der Wörter in der Zusammenfassung zur Anzahl der Wörter im Ausgangstext.\n",
    "    num_words_text = len(text.split())\n",
    "    num_words_summary = len(summary.split())\n",
    "    ratio = num_words_summary / num_words_text\n",
    "    return ratio\n",
    "\n",
    "def compression(text, compression_rate,split='\\. '):\n",
    "    max_iterations = 20\n",
    "    iterations = 0\n",
    "    extracted = textrank_extractive(text, compression_rate,split)\n",
    "    summary = '. '.join(extracted)\n",
    "    compression_rate_renwed = compression_rate\n",
    "\n",
    "    # Wenn das Kompressionsverhältnis kleiner ist als die gewünschte Rate, versuchen wir, das Kompressionsverhältnis zu erhöhen.\n",
    "    while compression_ratio(text, summary) < compression_rate and iterations < max_iterations:\n",
    "        iterations += 1\n",
    "        compression_rate_renwed += 0.05\n",
    "        if compression_rate_renwed > 1:\n",
    "            compression_rate_renwed = 1\n",
    "        extracted = textrank_extractive(text, compression_rate=compression_rate_renwed)\n",
    "        summary = '. '.join(extracted)\n",
    "    return summary  # Gibt die komprimierte Zusammenfassung zurück.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_count(text):\n",
    "    # Wir zerlegen den Text in einzelne Worte (Token) und zählen diese.\n",
    "    tokens = text.split()\n",
    "    return len(tokens)  # Gibt die Anzahl der Wörter (Token) im Text zurück.\n",
    "\n",
    "def adjust_length(text):\n",
    "    # Zuerst zählen wir die Anzahl der Wörter (Token) im Text.\n",
    "    length = token_count(text)\n",
    "    \n",
    "    # Wir passen die Mindest- und Maxmimallänge des Textes basierend auf der aktuellen Länge an.\n",
    "    # Die genauen Werte sind hier variabel und hängen von der Länge des Textes ab.\n",
    "    \n",
    "    if length <20:\n",
    "        # Wenn der Text weniger als 20 Wörter hat, erhöhen wir die Mindestlänge um 5% der aktuellen Länge.\n",
    "        # Die maximale Länge wird dann auf das Doppelte der Mindestlänge gesetzt.\n",
    "        min_length = length + int(length * 0.05)\n",
    "        max_length = min_length +min_length\n",
    "    elif length <50:\n",
    "        # Wenn der Text zwischen 20 und 50 Wörtern hat, verwenden wir die gleichen Regeln wie zuvor, \n",
    "        # aber die maximale Länge wird auf das 1,5-fache der Mindestlänge gesetzt.\n",
    "        min_length = length + int(length * 0.05)\n",
    "        max_length = min_length +min_length* 0.5\n",
    "    elif length <60:\n",
    "        # Bei Texten zwischen 50 und 60 Wörtern erhöhen wir die Mindestlänge um 5% der aktuellen Länge.\n",
    "        # Die maximale Länge wird dann auf das 1,4-fache der Mindestlänge gesetzt.\n",
    "        min_length = length + int(length * 0.05)\n",
    "        max_length = min_length +min_length* 0.4\n",
    "    elif length < 80:\n",
    "        # Bei Texten zwischen 60 und 80 Wörtern erhöhen wir die Mindestlänge um 5% der aktuellen Länge.\n",
    "        # Die maximale Länge wird dann auf das 1,25-fache der Mindestlänge gesetzt.\n",
    "        min_length = length + int(length * 0.05)\n",
    "        max_length = min_length + min_length* 0.25\n",
    "    elif length < 100:\n",
    "        # Bei Texten zwischen 80 und 100 Wörtern erhöhen wir die Mindestlänge um 30% der aktuellen Länge.\n",
    "        # Die maximale Länge wird dann auf die Mindestlänge plus 100 Wörter gesetzt.\n",
    "        min_length = length + int(length * 0.3)\n",
    "        max_length = min_length + 100\n",
    "    else:\n",
    "        # Bei Texten mit 100 Wörtern oder mehr setzen wir die Mindestlänge auf das nächste Vielfache von 70,\n",
    "        # das größer als die aktuelle Länge geteilt durch 50 ist. \n",
    "        # Die maximale Länge wird dann auf die Mindestlänge plus 100 Wörter gesetzt.\n",
    "        min_length = math.ceil(length / 50) * 70\n",
    "        max_length = min_length + 100\n",
    "\n",
    "    # Wir geben die berechneten minimalen und maximalen Längen zurück.\n",
    "    return min_length, max_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_sent(sentenc,splitt=180,split='\\. '):\n",
    "    # Teile den eingegebenen Text in einzelne Sätze auf, wobei jeder Satz durch \". \" getrennt ist.\n",
    "    # Hierbei wird sichergestellt, dass das \". \" nicht auf ein einzelnes Wort folgt.\n",
    "    sentences = re.split(fr'(?<!\\b\\w\\w){split}', sentenc.lower())\n",
    "\n",
    "    # Initialisierung der Batches und der aktuellen Batch-Liste sowie der aktuellen Batch-Länge.\n",
    "    batches = []\n",
    "    batch = []\n",
    "    batch_len = 0\n",
    "    \n",
    "    # Durchlaufen Sie jeden Satz in den Sätzen.\n",
    "    for sentence in sentences:\n",
    "        # Berechnen Sie die Anzahl der Tokens im Satz.\n",
    "        sentence_len = len(tokenizer.tokenize(sentence))\n",
    "        \n",
    "        # Wenn die Hinzufügung des aktuellen Satzes die maximale Batch-Länge überschreitet...\n",
    "        if sentence_len + batch_len > splitt:\n",
    "            # ...und wenn der aktuelle Satz weniger als die maximale Batch-Länge hat...\n",
    "            if sentence_len < splitt:  \n",
    "                # ...füge die aktuelle Batch-Liste zu den Batches hinzu...\n",
    "                batches.append(batch)\n",
    "                # ...und beginne eine neue Batch-Liste mit dem aktuellen Satz.\n",
    "                batch = [sentence]\n",
    "                # Die aktuelle Batch-Länge wird auf die Länge des aktuellen Satzes gesetzt.\n",
    "                batch_len = sentence_len\n",
    "            # Sätze, die länger als die maximale Batch-Länge sind, werden übersprungen.\n",
    "        else:\n",
    "            # Wenn der aktuelle Satz zur aktuellen Batch-Liste hinzugefügt werden kann, ohne die maximale Batch-Länge zu überschreiten...\n",
    "            # ...füge den Satz zur aktuellen Batch-Liste hinzu...\n",
    "            batch.append(sentence)\n",
    "            # ...und erhöhe die aktuelle Batch-Länge um die Länge des aktuellen Satzes.\n",
    "            batch_len += sentence_len\n",
    "    \n",
    "    # Füge die letzte Batch-Liste zu den Batches hinzu.\n",
    "    batches.append(batch)\n",
    "\n",
    "    # Die Funktion gibt die erstellten Batches zurück.\n",
    "    return batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paraphrase_of_text(df_s, text_name='text', komp_name='reduction_multiplier', split='\\. '):\n",
    "    # Erstellen eines leeren DataFrame, in das die generierten Zusammenfassungen und deren Details eingefügt werden.\n",
    "    df_summary_testing = pd.DataFrame(columns=['Zusammenfassung','Min_Kompressionsrate', 'Max_Kompressionsrate', 'Endgueltige_Kompressionsrate','länge Zusammenfassung','länge Ausgangstext','batch_texts','batch_output'])\n",
    "\n",
    "    # Durchlaufen Sie jede Zeile im gegebenen DataFrame.\n",
    "    for _, row in tqdm(df_s.iterrows(), total=df_s.shape[0]):\n",
    "        batch_text_list=[]\n",
    "        output_text_list=[]\n",
    "        text = row[text_name]\n",
    "        komp = row[komp_name]\n",
    "        text_gesamt_list=[]\n",
    "\n",
    "        # Teilen Sie den Text in kleinere Batches auf.\n",
    "        for batch in tqdm(batch_sent(text,split=split), desc='Verarbeite Batches'):\n",
    "            if len(batch):\n",
    "                # Fügen Sie die Sätze in einem Batch zusammen.\n",
    "                batch_text = '. '.join(batch)\n",
    "                batch_text += \".\"\n",
    "                batch_text_list.append(batch_text)\n",
    "                # Passen Sie die Länge des Textes an.\n",
    "                min_length_test, max_length_test = adjust_length(batch_text)\n",
    "                # Paraphrasieren Sie den Text.\n",
    "                ext_summary=summarizer(batch_text, max_length=int(round(max_length_test*komp,0)), min_length=int(round(min_length_test*komp,0)),length_penalty=100,num_beams=2)\n",
    "\n",
    "                # Speichern Sie die generierte Zusammenfassung.\n",
    "                text_gesamt_list.append(ext_summary[0]['generated_text'])\n",
    "\n",
    "        # Fügen Sie alle generierten Zusammenfassungen zusammen.\n",
    "        text_gesamt = '. '.join(text_gesamt_list)\n",
    "\n",
    "        # Berechnen Sie die tatsächliche Kompressionsrate.\n",
    "        actual_compression_rate = len(text_gesamt.split(' '))/len(text.split(' '))*100\n",
    "\n",
    "        # Erstellen Sie einen neuen DataFrame für die aktuellen Ergebnisse.\n",
    "        df_current = pd.DataFrame({\n",
    "            'Zusammenfassung': [text_gesamt],\n",
    "            'Min_Kompressionsrate': [min_length_test],\n",
    "            'Max_Kompressionsrate': [max_length_test],\n",
    "            'Endgueltige_Kompressionsrate': [actual_compression_rate],\n",
    "            'länge Zusammenfassung': [len(text_gesamt.split(' '))],\n",
    "            'länge Ausgangstext': [len(text.split(' '))],\n",
    "            'batch_texts': [batch_text_list],\n",
    "            'batch_output': [text_gesamt_list]\n",
    "        })\n",
    "\n",
    "        # Fügen Sie die Daten zum DataFrame hinzu.\n",
    "        df_summary_testing = pd.concat([df_summary_testing, df_current], ignore_index=True)\n",
    "\n",
    "    return df_summary_testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_compression(df, total_tokens_col, current_tokens_col, desired_compression_rate):\n",
    "    # Berechnung der aktuellen Kompressionsrate als Verhältnis von aktueller Tokenanzahl zu Gesamt-Tokenanzahl.\n",
    "    df['current_compression_rate'] = df[current_tokens_col] / df[total_tokens_col]\n",
    "    # Berechnung der Differenz zwischen der gewünschten und der aktuellen Kompressionsrate.\n",
    "    df['compression_difference'] = df[desired_compression_rate] - df['current_compression_rate']\n",
    "    # Berechnung des Reduktionsmultiplikators als Verhältnis der gewünschten zur aktuellen Kompressionsrate.\n",
    "    df['reduction_multiplier'] = df[desired_compression_rate] / df['current_compression_rate']\n",
    "    # Rückgabe des DataFrame mit den berechneten Kompressionsinformationen.\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_rank_algo(df, seed=10, split='\\\\. ', random_T=True, column='text'):\n",
    "    # Initialisierung des Rückgabe-DataFrames mit den angegebenen Spalten\n",
    "    df_return = pd.DataFrame(columns=['text', 'text_rank_text', 'tokens_gesamt', 'token_text_rank', 'desired_compression_rate', 'text_rank_compression_rate'])\n",
    "    \n",
    "    # Festlegen des Seeds für den Zufallsgenerator\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Iteration über die Zeilen des gegebenen DataFrame\n",
    "    for index, row in tqdm(df.iterrows()):\n",
    "        text = row[column].replace(\"\\n\", \" \")\n",
    "        \n",
    "        # Entscheidung über den zu verwendenden Kompressionsfaktor\n",
    "        if random_T:\n",
    "            random_value = round(random.uniform(0.2, 0.8), 2)  # Generieren eines Zufallswertes zwischen 0.2 und 0.8\n",
    "        else:\n",
    "            if row['reduction_multiplier'] < 0.8:\n",
    "                random_value = row['desired_compression_rate']\n",
    "            elif row['reduction_multiplier'] < 0.9:\n",
    "                random_value = row['reduction_multiplier']\n",
    "            else:\n",
    "                random_value = 1\n",
    "\n",
    "        # Anwendung der Text Rank Kompression\n",
    "        text_rank_text = compression(text.replace(\"\\n\\n\", \" \"), random_value, split)\n",
    "        \n",
    "        # Berechnung des Kompressionsverhältnisses\n",
    "        compression_ratio_value = compression_ratio(text, compression(text, random_value, split))\n",
    "        \n",
    "        # Bereinigung des Textes und des text_rank_text von mehrfachen Leerzeichen und bestimmten Zeichen\n",
    "        text = re.sub(' +', ' ', text.replace(\"\\n\", \" \").replace('-', ' ').replace('_', ' ').replace(\"\\'\", \"\").replace(\"!\", \".\").replace(\"?\", \".\").replace(\";\", \"\"))\n",
    "        text_rank_text = re.sub(' +', ' ', text_rank_text.replace(\"\\n\", \" \").replace('-', ' ').replace('_', ' ').replace(\"\\'\", \"\").replace(\"!\", \".\").replace(\"?\", \".\").replace(\";\", \"\"))\n",
    "        \n",
    "        # Erstellung eines DataFrame mit den aktuellen Ergebnissen\n",
    "        df_current = pd.DataFrame({\n",
    "            'text': [text],\n",
    "            'text_rank_text': [text_rank_text],\n",
    "            'tokens_gesamt': [len(text.split(' '))],\n",
    "            'token_text_rank': [len(text_rank_text.split(' '))],\n",
    "            'desired_compression_rate': [random_value],\n",
    "            'text_rank_compression_rate': [compression_ratio_value]\n",
    "        })\n",
    "\n",
    "        # Hinzufügen der aktuellen Ergebnisse zum Rückgabe-DataFrame\n",
    "        df_return = pd.concat([df_return, df_current], ignore_index=True)\n",
    "\n",
    "    # Rückgabe des erstellten DataFrames mit den Kompressionsinformationen\n",
    "    return df_return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_text_gen(df, split='\\\\. ', seed=10):\n",
    "    # Anwendung des Text Rank Algorithmus auf das gegebene DataFrame\n",
    "    rank_df = text_rank_algo(df, seed=seed, split=split)\n",
    "    \n",
    "    # Berechnung der Kompressionsrate basierend auf dem resultierenden DataFrame des Text Rank Algorithmus\n",
    "    df_zwischen = calculate_compression(rank_df, 'tokens_gesamt', 'token_text_rank', 'desired_compression_rate')\n",
    "    \n",
    "    # Erzeugen der Paraphrasen des Texts basierend auf dem DataFrame, das durch die Berechnung der Kompressionsrate erhalten wurde\n",
    "    df_sum = paraphrase_of_text(df_zwischen[['text_rank_text','reduction_multiplier']], text_name='text_rank_text', split=split)\n",
    "    \n",
    "    # Zusammenführen der beiden DataFrames zu einem einzigen DataFrame\n",
    "    merged_df = pd.concat([df_sum, df_zwischen], axis=1)\n",
    "    \n",
    "    # Berechnung der endgültigen Kompressionsrate\n",
    "    merged_df['ent_com_rate'] = merged_df['länge Zusammenfassung'] / merged_df['tokens_gesamt']\n",
    "    \n",
    "    # Erneute Berechnung der Kompressionsrate basierend auf dem zusammengeführten DataFrame\n",
    "    new_df = calculate_compression(merged_df[['Zusammenfassung','länge Zusammenfassung','text','tokens_gesamt','desired_compression_rate','ent_com_rate']], 'tokens_gesamt', 'länge Zusammenfassung', 'desired_compression_rate')\n",
    "    \n",
    "    # Erneute Anwendung des Text Rank Algorithmus auf das aktualisierte DataFrame, um verlängerte texte zu reduzieren\n",
    "    df_testen = text_rank_algo(new_df, random_T=False, column='Zusammenfassung')\n",
    "    \n",
    "    # Zusammenführen der Daten und Berechnung der endgültigen Kompressionsrate\n",
    "    a = pd.concat([df_testen[['text_rank_text','token_text_rank',]], new_df[['Zusammenfassung','länge Zusammenfassung','text','tokens_gesamt','desired_compression_rate','ent_com_rate']]], axis=1)\n",
    "    a['ent_com_rate'] = a['länge Zusammenfassung'] / a['tokens_gesamt']\n",
    "    \n",
    "    # Rückgabe des finalen DataFrame\n",
    "    return a\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=pd.read_csv('data/data_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Scientific', 'news', 'reviews', 'story'], dtype=object)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.classification.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training=df_test[df_test.classification=='story'].reset_index(drop=True)[['classification','text']].reset_index(drop=True).copy()\n",
    "rank_df=text_rank_algo(df_training,seed=10,split='\\. ',random_T=True,column='text')\n",
    "df_zwischen=calculate_compression(rank_df, 'tokens_gesamt', 'token_text_rank', 'desired_compression_rate')\n",
    "df_zwischen.to_csv(f'data/ergbnisse/test_story_text_rank.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execute_text_gen(df_test[df_test.classification=='reviews'].reset_index(drop=True)[['classification','text']].reset_index(drop=True).copy(),split='\\. ',seed=10).to_csv(f'data/ergbnisse/test_review_text.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict=df_test[['classification', 'text']][0:1].to_dict('records')[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('torch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63be3639594356bc87fe58051c1d1c5221c23a964c31c0e05d208c4974bedf26"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
